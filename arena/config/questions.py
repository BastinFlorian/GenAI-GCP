QUESTIONS = [
    ('What is the fundamental difference between a traditional LLM and a RAG architecture?', 'A traditional LLM generates responses solely from its internal parameters, whereas a RAG architecture accesses external information sources to enhance its responses.'),
    ('Explain how RAG mitigates the problem of hallucinations in language models.', 'RAG mitigates hallucinations by providing the language model with factual context from external documents, helping it generate more accurate and verified responses.'),
    ('Describe the two main steps involved in the RAG architecture.', 'The two main steps of RAG are: (1) retrieving relevant documents from a database and (2) generating a response using the retrieved context.'),
    ('Why and how are documents split into chunks in RAG?', 'Documents are divided into chunks to manage the context size limitations of language models. This allows the model to process larger documents by splitting them into smaller segments.'),
    ('Name two commonly used methods for information retrieval in RAG.', 'Two commonly used methods are TF-IDF and BM25. These methods rank documents based on their relevance to a given query.'),
    ('What is the difference between sparse search and dense search in RAG?', 'Sparse search uses keywords to find similar documents, whereas dense search uses semantic representations of documents and queries for a more nuanced search.'),
    ('Describe the purpose of the "Reciprocal Rank Fusion (RRF)" technique.', 'RRF allows combining results from different retrievers, thereby improving the precision and diversity of retrieved documents.'),
    ('List two advantages of using a reranker in a RAG architecture.', '1) Increased precision: a reranker can improve the likelihood that the most relevant information is used to generate the response. 2) Better contextual understanding: it helps the system grasp the nuances of the query.'),
    ('Explain the concept of "Query Augmentation" in the context of RAG.', '"Query Augmentation" involves enriching or expanding the user initial query to improve the retrieval of relevant information from the knowledge base.'),
    ('How can the performance of a retriever be evaluated in a RAG system?', 'The performance of a retriever can be evaluated using metrics like precision and recall, which measure the number of relevant documents retrieved compared to the total number of documents retrieved and the total number of relevant documents available.'),
    ('What is the definition of LLM (Large Language Model)?', 'LLM (Large Language Model): A deep learning model trained on a vast set of textual data, capable of generating text and performing various linguistic tasks.'),
    ('What is the definition of RAG (Retrieval Augmented Generation)?', 'RAG (Retrieval Augmented Generation): An architecture that combines information retrieval from external sources with text generation by a LLM.'),
    ('What is the definition of hallucinations in the context of language models?', 'Hallucinations: Responses generated by a language model that seem plausible but are not based on factual information.'),
    ('What is the definition of tokenization?', 'Tokenization: The process of dividing text into individual units called "tokens".'),
    ('What is the definition of a chunk in RAG?', 'Chunk: A segment of a document, generally used to manage the context size limitations of LLMs.'),
    ('What is the definition of information retrieval?', 'Information retrieval: The process of searching for documents relevant to a given query.'),
    ('What is the definition of TF-IDF (Term Frequency-Inverse Document Frequency)?', 'TF-IDF (Term Frequency-Inverse Document Frequency): A statistical method that measures the importance of a word in a document relative to a collection of documents.'),
    ('What is the definition of BM25 (Best Match 25)?', 'BM25 (Best Match 25): A ranking function that measures the relevance of a document to a given query.'),
    ('What is the definition of sparse search?', 'Sparse search: Search methods based on keywords to find similar documents.'),
    ('What is the definition of dense search?', 'Dense search: Search methods using semantic representations of documents and queries.'),
    ('What is an embedding vector?', 'Embedding vector: A numerical representation of a piece of text, capturing its semantic meaning.'),
    ('What is the definition of a vector database?', 'Vector database: A database specialized in storing, managing, and querying embedding vectors.'),
    ('What is the definition of a reranker?', 'Reranker: A model that reorders retrieved documents after an initial similarity search.'),
    ('What is Query Augmentation?', 'Query Augmentation: The process of enriching or expanding the user initial query to improve information retrieval.'),
    ('What is the definition of precision in information systems evaluation?', 'Precision: A measure of the number of relevant documents retrieved compared to the total number of documents retrieved.'),
    ('What is the definition of recall in information systems evaluation?', 'Recall: A measure of the number of relevant documents retrieved compared to the total number of relevant documents available.'),
    ('What is NDCG (Normalized Discounted Cumulative Gain)?', 'NDCG (Normalized Discounted Cumulative Gain): A measure of the quality of search result rankings.'),
    ('What is the definition of multimodal RAG?', 'Multimodal RAG: An extension of RAG that allows processing and understanding different types of data, such as text, images, and videos.'),
    ('What is M-ROPE (Multimodal Rotary Position Embedding)?', 'M-ROPE (Multimodal Rotary Position Embedding): A positional embedding technique used in multimodal RAG to capture spatial and temporal information from different modalities.'),
    ('What is the definition of Self-RAG?', 'Self-RAG: A RAG architecture that generates and evaluates multiple possible responses in parallel, using retrieved documents as context.'),
    ('What is RAPTOR in the context of RAG?', 'RAPTOR: A RAG architecture that recursively summarizes retrieved documents to create a hierarchy of summaries, thereby reducing the amount of information to process while preserving context.'),
    ('What is the definition of Corrective RAG?', 'Corrective RAG: A RAG architecture that uses a search evaluator to identify incorrect or ambiguous information sources and corrects them by performing additional searches.'),
    ('What is GraphRAG?', 'GraphRAG: A RAG architecture that uses an LLM-generated knowledge graph to organize data hierarchically, allowing for a holistic understanding of the dataset and more efficient information retrieval.');
    ('What is Retrieval Augmented Generation (RAG)?', 'Retrieval Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by giving them access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.'),
    ('What are the advantages of using RAG?', 'RAG offers several advantages:\n\n- **More informed and factual responses**: By accessing external information, RAG allows language models to provide more complete and accurate answers.\n- **Handling queries about recent events**: RAG can manage queries related to events not present in the model  training data, as it can retrieve up-to-date information from external sources.\n- **Reduction of hallucinations**: Language models tend to generate incorrect or nonsensical information, known as "hallucinations." RAG helps reduce these hallucinations by grounding responses in factual information.'),
    ('How does RAG work?', 'RAG follows a three-step process:\n\n1. **Document ingestion**: Relevant documents are loaded and processed, then converted into numerical representations called embeddings.\n2. **Document retrieval**: When a query is received, the system retrieves the most relevant documents for the query from the knowledge base using similarity algorithms.\n3. **Contextualized response**: The retrieved documents are provided as context to the language model, which then generates a response based on the information from the query and the documents.'),
    ('What are common methods for information retrieval in RAG?', 'Several methods can be used to retrieve information in RAG:\n\n- **TF-IDF**: A statistical method that evaluates the importance of words in a document relative to a collection of documents.\n- **BM25**: A ranking algorithm that considers term frequency and document length.\n- **Cosine similarity**: Measures the similarity between two vectors by calculating the cosine of the angle between them.\n- **Euclidean distance**: Measures the distance between two points in a multidimensional space.\n- **Maximum Marginal Relevance (MMR)**: Aims to retrieve documents that are both similar and diverse.'),
    ('What is a vector database and why is it used in RAG?', 'A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings, which are numerical representations of data such as text, images, or other content types. In RAG, vector databases are used to efficiently store and search document embeddings, enabling quick retrieval of relevant documents.'),
    ('How does RAG handle the context size limitations of LLMs?', 'Language models have a limit on the number of tokens they can process in a single input sequence. To overcome this limitation, RAG uses a technique called segmentation:\n\n- **Segmentation by document**: If the document is small, it can be processed as a single block.\n- **Segmentation by title or header**: If the document is large, it is divided into smaller blocks based on titles or headers, ensuring each block fits within the LLM\ context size.'),
    ('What are advanced RAG architectures?', 'Several advanced RAG architectures have been developed to improve the performance and capabilities of RAG systems:\n\n- **Self-RAG**: Generates multiple possible response segments in parallel and uses a critic model to select the most accurate and relevant segment as the final output.\n- **RAPTOR**: Recursively summarizes retrieved documents, creating a hierarchy of summaries that preserves context and essential facts while reducing the amount of information to process.\n- **Corrective RAG (CRAG)**: Adds a retrieval evaluator to assess the quality of retrieved sources and, if necessary, corrects or augments the context by performing a web search query.\n- **GraphRAG**: Processes the entire dataset and creates a knowledge graph used to organize data hierarchically, providing a more holistic context for answering queries.'),
    ('How are RAG systems evaluated?', 'Evaluating RAG systems involves assessing both the quality of retrieval and the accuracy of generated responses:\n\n- **Retriever evaluation**: Common metrics include precision at k, recall at k, and Normalized Discounted Cumulative Gain (NDCG), which measure the relevance of retrieved documents.\n- **Response evaluation**: Assessing the quality of generated responses is more challenging. One approach is to use an LLM as a judge to evaluate whether responses are factually correct, use context information, and provide sufficient facts. Metrics such as faithfulness, response relevance, context precision, and context recall can be used.'),
    ('What are the future trends of RAG?', 'RAG is an active research area, and several future trends are promising:\n\n- **Multimodal RAG**: Extending RAG to support data modalities other than text, such as images, audio, and video.\n- **Retrieval with feedback**: Using user feedback to refine retrieval and ranking models.\n- **Adaptive RAG**: Classifying queries into different categories and using customized retrieval strategies for each category.\n- **Iterative RAG**: Iteratively refining retrieval results and generating follow-ups to fill gaps or clarify information.')
    ('What is the fundamental difference between a traditional LLM and a RAG architecture?', 'A traditional LLM generates responses solely from its internal parameters, whereas a RAG architecture accesses external information sources to enhance its responses.'),
    ('Explain how RAG mitigates the problem of hallucinations in language models.', 'RAG mitigates hallucinations by providing the language model with factual context from external documents, helping it generate more accurate and verified responses.'),
    ('Describe the two main steps involved in the RAG architecture.', 'The two main steps of RAG are: (1) retrieving relevant documents from a database and (2) generating a response using the retrieved context.'),
    ('Why and how are documents split into chunks in RAG?', 'Documents are divided into chunks to manage the context size limitations of language models. This allows the model to process larger documents by splitting them into smaller segments.'),
    ('Name two commonly used methods for information retrieval in RAG.', 'Two commonly used methods are TF-IDF and BM25. These methods rank documents based on their relevance to a given query.'),
    ('What is the difference between sparse search and dense search in RAG?', 'Sparse search uses keywords to find similar documents, whereas dense search uses semantic representations of documents and queries for a more nuanced search.'),
    ('Describe the purpose of the "Reciprocal Rank Fusion (RRF)" technique.', 'RRF allows combining results from different retrievers, thereby improving the precision and diversity of retrieved documents.'),
    ('List two advantages of using a reranker in a RAG architecture.', '1. Increased precision: a reranker can improve the likelihood that the most relevant information is used to generate the response. 2. Better contextual understanding: it helps the system grasp the nuances of the query.'),
    ('Explain the concept of "Query Augmentation" in the context of RAG.', '"Query Augmentation" involves enriching or expanding the users initial query to improve the retrieval of relevant information from the knowledge base.'),
    ('How can the performance of a retriever be evaluated in a RAG system?', 'The performance of a retriever can be evaluated using metrics like precision and recall, which measure the number of relevant documents retrieved compared to the total number of documents retrieved and the total number of relevant documents available.'),
    ('What is the definition of LLM (Large Language Model)?', 'LLM (Large Language Model): A deep learning model trained on a vast set of textual data, capable of generating text and performing various linguistic tasks.'),
    ('What is the definition of RAG (Retrieval Augmented Generation)?', 'RAG (Retrieval Augmented Generation): An architecture that combines information retrieval from external sources with text generation by a LLM.'),
    ('What is the definition of hallucinations in the context of language models?', 'Hallucinations: Responses generated by a language model that seem plausible but are not based on factual information.'),
    ('What is the definition of tokenization?', 'Tokenization: The process of dividing text into individual units called "tokens".'),
    ('What is the definition of a chunk in RAG?', 'Chunk: A segment of a document, generally used to manage the context size limitations of LLMs.'),
    ('What is the definition of information retrieval?', 'Information retrieval: The process of searching for documents relevant to a given query.'),
    ('What is the definition of TF-IDF (Term Frequency-Inverse Document Frequency)?', 'TF-IDF (Term Frequency-Inverse Document Frequency): A statistical method that measures the importance of a word in a document relative to a collection of documents.'),
    ('What is the definition of BM25 (Best Match 25)?', 'BM25 (Best Match 25): A ranking function that measures the relevance of a document to a given query.'),
    ('What is the definition of sparse search?', 'Sparse search: Search methods based on keywords to find similar documents.'),
    ('What is the definition of dense search?', 'Dense search: Search methods using semantic representations of documents and queries.'),
    ('What is an embedding vector?', 'Embedding vector: A numerical representation of a piece of text, capturing its semantic meaning.'),
    ('What is the definition of a vector database?', 'Vector database: A database specialized in storing, managing, and querying embedding vectors.'),
    ('What is the definition of a reranker?', 'Reranker: A model that reorders retrieved documents after an initial similarity search.'),
    ('What is Query Augmentation?', 'Query Augmentation: The process of enriching or expanding the users initial query to improve information retrieval.'),
    ('What is the definition of precision in information systems evaluation?', 'Precision: A measure of the number of relevant documents retrieved compared to the total number of documents retrieved.'),
    ('What is the definition of recall in information systems evaluation?', 'Recall: A measure of the number of relevant documents retrieved compared to the total number of relevant documents available.'),
    ('What is NDCG (Normalized Discounted Cumulative Gain)?', 'NDCG (Normalized Discounted Cumulative Gain): A measure of the quality of search result rankings.'),
    ('What is the definition of multimodal RAG?', 'Multimodal RAG: An extension of RAG that allows processing and understanding different types of data, such as text, images, and videos.'),
    ('What is M-ROPE (Multimodal Rotary Position Embedding)?', 'M-ROPE (Multimodal Rotary Position Embedding): A positional embedding technique used in multimodal RAG to capture spatial and temporal information from different modalities.'),
    ('What is the definition of Self-RAG?', 'Self-RAG: A RAG architecture that generates and evaluates multiple possible responses in parallel, using retrieved documents as context.'),
    ('What is RAPTOR in the context of RAG?', 'RAPTOR: A RAG architecture that recursively summarizes retrieved documents to create a hierarchy of summaries, thereby reducing the amount of information to process while preserving context.'),
    ('What is the definition of Corrective RAG?', 'Corrective RAG: A RAG architecture that uses a search evaluator to identify incorrect or ambiguous information sources and corrects them by performing additional searches.'),
    ('What is GraphRAG?', 'GraphRAG: A RAG architecture that uses an LLM-generated knowledge graph to organize data hierarchically, allowing for a holistic understanding of the dataset and more efficient information retrieval.')
    ('In 2 sentences: Describe the concept of self-attention in the Transformer architecture.', 'Self-attention allows each token in a sequence to analyze other tokens in the same sequence to capture relationships and dependencies between them.'),
    ('In 2 sentences: What is the difference between encoder attention and decoder attention?', 'Encoder attention allows tokens in the input sequence to attend to each other. Decoder attention allows each token in the output sequence to attend to other output tokens and to the tokens in the input sequence.'),
    ('In 2 sentences: Explain the role of masking in the decoder self-attention mechanism.', 'Masking in the decoder self-attention mechanism prevents tokens in the output sequence from attending to future tokens, ensuring that the prediction of a token depends only on preceding tokens.'),
    ('In 2 sentences: How does multi-head attention enhance the model ability to capture different types of relationships?', 'Multi-head attention uses multiple independent attention heads, each capable of capturing different aspects of the relationships between tokens, allowing for a richer representation of the input sequence.'),
    ('In 2 sentences: What is the purpose of residual connections in Transformers?', 'Residual connections allow gradients to propagate more effectively through the model layers, mitigating the vanishing gradient problem and improving learning.'),
    ('In 2 sentences: How does layer normalization help stabilize learning?', 'Layer normalization normalizes the activations of each layer, stabilizing learning by preventing gradient explosions and speeding up model convergence.'),
    ('In 2 sentences: Describe the function of the feed-forward layer in the Transformer architecture.', 'The feed-forward layer is a fully connected layer that applies a nonlinear transformation to each token in the sequence, enabling the extraction of higher-level features.'),
    ('In 2 sentences: Explain the role of the Softmax layer in generating the model output.', 'The Softmax layer converts the model output scores into a probability distribution over the vocabulary, allowing the prediction of the next token in the sequence.'),
    ('In 2 sentences: Why are positional embeddings essential in the Transformer architecture?', 'Positional embeddings encode the order of tokens in the sequence because Transformers have no intrinsic notion of position.'),
    ('In 2 sentences: Describe the desired properties of an effective positional embedding method.', 'A good positional embedding should be unique for each position, deterministic, allow estimation of distance between tokens, and work with sequences longer than those seen during training.'),
    ('In 2 sentences: What is the definition of a Transformer?', 'A Transformer is a neural network architecture that uses attention mechanisms to process sequences of data, particularly in the field of natural language processing.'),
    ('In 2 sentences: What is the definition of self-attention?', 'Self-attention is a mechanism that allows each token in a sequence to analyze other tokens in the same sequence to capture relationships and dependencies between them.'),
    ('In 2 sentences: What is the definition of an encoder?', 'An encoder is the part of the Transformer that processes the input sequence and generates a contextual representation of each token.'),
    ('In 2 sentences: What is the definition of a decoder?', 'A decoder is the part of the Transformer that generates the output sequence using the contextual representation from the encoder.'),
    ('In 2 sentences: What is masking in the context of Transformers?', 'Masking is a technique used in the decoder to prevent tokens from attending to future tokens.'),
    ('In 2 sentences: What is the definition of multi-head attention?', 'Multi-head attention involves using multiple independent attention heads to capture different aspects of relationships between tokens.'),
    ('In 2 sentences: What are residual connections?', 'Residual connections are links that allow gradients to propagate more effectively through the model layers.'),
    ('In 2 sentences: What is layer normalization?', 'Layer normalization is a technique that normalizes the activations of each layer to stabilize learning.'),
    ('In 2 sentences: What is the feed-forward layer?', 'The feed-forward layer is a fully connected layer that applies a nonlinear transformation to each token.'),
    ('In 2 sentences: What is the Softmax layer?', 'The Softmax layer is a layer that converts output scores into a probability distribution over the vocabulary.'),
    ('In 2 sentences: What are positional embeddings?', 'Positional embeddings are vectors that encode the order of tokens in a sequence.'),
    ('In 2 sentences: What are queries, keys, and values in the attention mechanism?', 'Queries, keys, and values are three vectors used in the attention mechanism to compute attention weights.'),
    ('In 2 sentences: What is the attention matrix?', 'The attention matrix is a matrix that represents the attention weights between each pair of tokens in a sequence.')
    ('What is a Transformer?', 'A Transformer is a neural network architecture introduced in 2017 in the paper "Attention is All You Need". It is a powerful model that has achieved state-of-the-art results in many natural language processing (NLP) tasks, such as machine translation, text classification, and question answering.\n\nUnlike recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), Transformers do not process input sequences sequentially. Instead, they use an attention mechanism to capture dependencies between all words in a sequence simultaneously, allowing them to process longer sequences and learn more complex relationships between words.'),
    ('What are the key components of a Transformer?', 'A Transformer is composed of two main parts: an encoder and a decoder.\n\n**Encoder**: The encoder takes an input sequence and transforms it into a vector representation. It uses self-attention layers to capture dependencies between the input words and produces a sequence of output vectors, each representing an input word.\n\n**Decoder**: The decoder takes the output of the encoder and generates an output sequence, one word at a time. It uses self-attention and cross-attention layers to capture dependencies between the output words and the input words, respectively. The decoder generates an output word at each step, using the previous output words and the encoder output as context.'),
    ('How does the self-attention mechanism work in a Transformer?', 'The self-attention mechanism allows the Transformer to assign different weights to input words based on their relevance to a given word.\n\nIt works by calculating three vectors for each input word: a **query**, a **key**, and a **value**. The query represents the word for which attention is being calculated, the key represents the relevance of other words, and the value represents the information to extract.\n\nThe attention score between two words is calculated using the dot product of one word query and another word key. These scores are then normalized using a softmax function to obtain attention weights, which are used to compute a weighted sum of the value vectors.'),
    ('What is the difference between self-attention and cross-attention in a Transformer?', '**Self-attention**: Self-attention occurs in both the encoder and the decoder. It computes dependencies between words in the same sequence. In the encoder, it captures relationships between all input words, while in the decoder, it captures relationships between the generated output words.\n\n**Cross-attention**: Cross-attention occurs only in the decoder. It computes dependencies between the output words and the input words. It allows the decoder to consider the entire input sequence when generating each output word.'),
    ('Why do Transformers use residual connections and layer normalization?', 'Residual connections and layer normalization are two techniques used in Transformers to improve the stability and speed of learning.\n\n- **Residual connections**: These allow gradients to propagate more easily through the network layers, mitigating the vanishing gradient problem common in deep networks.\n- **Layer normalization**: This normalizes neuron activations at each layer, stabilizing learning and accelerating convergence.'),
    ('How do Transformers handle the order of words in a sequence?', 'Transformers use **positional encodings** to inject information about the order of words into the model. These encodings are vectors added to the word embeddings, representing a word position in the sequence. Positional encodings allow the model to distinguish words that have the same meaning but appear at different positions in the sequence.'),
    ('What is the function of the feed-forward layer in a Transformer?', 'The feed-forward layer is a fully connected neural network that applies a nonlinear transformation to the output of the multi-head attention layer. It enables the model to learn more complex representations of the input data.'),
    ('What are some examples of applications of Transformers?', 'Transformers have been successfully used in many NLP tasks, such as:\n\n- **Machine translation**: Transformer XL, Marian\n- **Text classification**: BERT, RoBERTa\n- **Question answering**: mBART\n- **Text generation**: GPT-2, GPT-3, CTRL\n- **Text summarization**: T5\n- **Sentiment analysis**: DistilBERT, ALBERT'),
    ('Explain the concept of N-grams in natural language processing.', 'N-grams are sequences of N consecutive words in a text. They are used to model the probability of a word occurring based on the preceding words. For example, a trigram considers the two previous words to predict the next word.'),
    ('In 1-2 sentences, what is the difference between one-hot encoding and word embeddings?', 'One-hot encoding represents each word as a binary vector where only one component is equal to 1, indicating the specific word. Word embeddings represent each word with a dense vector of real numbers, capturing semantic relationships between words.'),
    ('In 1-2 sentences, name two examples of word embedding algorithms.', 'Word2Vec and GloVe are two examples of word embedding algorithms.'),
    ('In 1-2 sentences, what is the goal of a Seq2Seq model?', 'A Seq2Seq model aims to transform an input sequence into an output sequence, such as in machine translation or text generation.'),
    ('In 1-2 sentences, what are the advantages and disadvantages of recurrent neural networks (RNNs)?', 'RNNs can learn long-term dependencies in word sequences thanks to their internal memory. However, they are limited by the vanishing gradient problem and have short-term memory limitations.'),
    ('In 1-2 sentences, what problem do LSTMs try to solve compared to traditional RNNs?', 'LSTMs address the vanishing gradient problem in RNNs by introducing gates that control information flow in the memory cell, allowing information to be retained longer.'),
    ('In 1-2 sentences, explain how the sigmoid function is used in LSTMs.', 'The sigmoid function is used in the gates of LSTMs to control the flow of information. Its output ranges between 0 and 1, indicating the degree to which a gate is open or closed.'),
    ('In 1-2 sentences, describe the role of the cell state in an LSTM.', 'The cell state in an LSTM acts as long-term memory, storing important information from the input sequence. It is updated at each time step based on incoming information and gate operations.'),
    ('In 1-2 sentences, how is the LSTM architecture different from the basic RNN architecture?', 'The LSTM architecture introduces gates (input gate, output gate, forget gate) that control information flow within the memory cell, enabling better management of long-term dependencies than basic RNNs.'),
    ('What is an N-gram and how is it used in language modeling?', 'An N-gram is a sequence of N consecutive words in a text. In language modeling, N-grams are used to predict the next word in a sequence based on the probability of its occurrence after the previous N words.\n\nFor example, in the sentence "I like to eat apples," the trigram (3-gram) "to eat apples" can be used to predict that the next word is "red."\n\nThis approach is simple to implement but has limitations, notably the difficulty of taking into account long-term context in a sentence.'),
    ('How do word embeddings represent the meaning of words?', 'Word embeddings are vector representations of words that encode their semantic meaning. Instead of using one-hot encoding, which does not capture relationships between words, word embeddings represent each word as a vector of real numbers.\n\nThese vectors are learned from large text corpora using algorithms like Word2Vec, GloVe, BERT, or ELMo. The position of words in the vector space reflects their meaning: words with similar meanings will have vectors close to each other.'),
    ('What is the working principle of recurrent neural networks (RNNs)?', 'RNNs are neural networks designed to process sequences of data, such as text. They have an internal "memory" that allows them to take into account information from previous words when processing a current word.\n\nEach word in the sequence is processed by the network, and its internal state is updated based on the current input and its previous state. This allows RNNs to learn short-term dependencies between words in a sentence.'),
    ('What are the advantages and limitations of RNNs for language modeling?', '**Advantages:**\n- RNNs can learn the context of previous words thanks to their internal memory.\n- They are suitable for self-supervised learning.\n\n**Limitations:**\n- RNNs are sequential models, which makes their training slow.\n- They have limited short-term memory and struggle to capture long-term dependencies in long sentences.\n- They are subject to the vanishing/exploding gradient problem, making learning difficult.'),
    ('What is the vanishing/exploding gradient problem and how does it affect RNNs?', 'During the training of RNNs, the backpropagation algorithm is used to adjust the network weights. The vanishing/exploding gradient problem occurs when the gradient becomes very small (vanishing) or very large (exploding) as it propagates through the different layers of the network.\n\nThis can prevent the network from learning properly because the weights will not be updated effectively.'),
    ('How does the LSTM model improve upon the limitations of RNNs?', 'The LSTM (Long Short-Term Memory) model is a variant of RNNs that resolves the short-term memory problem by introducing a "memory cell" with "gates" that regulate the flow of information.\n\nThese gates allow the network to memorize information over longer periods, improving its ability to process long-term dependencies. Additionally, the LSTM architecture mitigates the vanishing/exploding gradient problem.'),
    ('In 1-2 sentences, what are the different architectures of RNNs and to which use cases do they correspond?', '- **One-to-one**: Used for simple classification tasks, like sentiment classification.\n- **One-to-many**: Used for text generation tasks, like image captioning.\n- **Many-to-one**: Used for tasks like sentiment analysis on documents.\n- **Many-to-many**: Used for tasks like machine translation or text summarization.\n- **Many-to-many (aligned)**: Used for sequence labeling tasks, like Part-of-Speech (POS) tagging.'),
    ('In 1-2 sentences, what were the goals and requirements of language models before the arrival of Transformers?', 'Before Transformers, language models aimed to predict a target word from a given context. They had to meet the following requirements:\n- Convert words into numerical values.\n- Establish semantic relationships between encoded words.\n- Provide sufficient context to the model to understand the sentence.\n- Allow efficient processing of long-term dependencies.\n- Be quickly trainable.'),
    ('What is a language model?', 'A language model is a probability distribution over a sequence of words. It predicts the probability of a given sequence of words, allowing it to generate text, translate languages, write different types of creative content, and answer your questions in an informative manner.\n\nFor example, a language model can assign a probability of 0.01 to the sentence "Transformers are encoders, decoders," indicating that it is a plausible sequence of words.\n\nAutoregressive language models work by applying the chain rule of probabilities, predicting the next word based on all previous words in the sequence.'),
    ('How are language models trained?', 'The training process of a language model involves feeding the model a vast set of textual data and optimizing its parameters to accurately predict the next word in a sequence.\n\nOne of the most common training methods is using cross-entropy loss, which measures the difference between the probability distribution predicted by the model and the actual distribution of words in the training data.'),
    ('What is tokenization and why is it important?', 'Tokenization is the process of breaking down text into individual units called tokens. These tokens can be words, characters, or subwords.\n\nTokenization is a crucial step in preprocessing data for language models because it allows the model to process text consistently.\n\nThere are different tokenization methods, such as Byte-Pair Encoding (BPE), which is commonly used for large language models.'),
    ('How is the performance of a language model evaluated?', 'Perplexity is a common metric for evaluating the performance of a language model. It represents how well the model predicts the next token, with lower perplexity indicating better performance.\n\nHowever, perplexity depends on the size of the vocabulary, making it difficult to compare models using different tokenization methods.\n\nNowadays, evaluation datasets such as Ifeval, BBH, MMLU, and MATH are used to assess the performance of LLMs on various tasks and domains.'),
    ('What are the steps involved in data preprocessing for LLMs?', 'Data preprocessing is crucial for training effective LLMs. Typical steps include:\n\n- Downloading a vast set of textual data from the Internet.\n- Extracting raw text by removing HTML, metadata, and unwanted content.\n- Filtering harmful content, personally identifiable information (PII), and biased data.\n- Removing duplicates and low-quality data.\n- Heuristic filtering to eliminate poor-quality documents.\n- Mixing data and classifying into categories for subject diversity.\n- Weighting domains based on their importance for downstream tasks.'),
    ('In 1-2 sentences, how do model size and data affect the performance of LLMs?', 'Scaling laws suggest that increasing compute resources, training data, and model size generally leads to better LLM performance.\n\nLarger models with more parameters can capture more complex relationships in the data, while larger datasets provide more examples to learn from.'),
    ('How is fine-tuning of LLMs used to create user assistants?', 'Fine-tuning involves refining a pre-trained LLM on a task-specific dataset to improve its performance on that task.\n\nTo create user assistants, supervised fine-tuning (SFT) is used, where the model is trained on input-output pairs aligned with human instructions.\n\nThis allows the model to generate more useful responses that are consistent with the user intentions.'),
    ('What are the challenges related to the development and evaluation of LLMs?', 'Despite their impressive progress, LLMs face several challenges:\n\n- **High cost and carbon footprint**: Training LLMs requires significant computational resources, leading to substantial financial and environmental costs.\n- **Bias and hallucinations**: LLMs can reflect biases present in their training data and generate incorrect or misleading information.\n- **Difficulty of evaluation**: Evaluating LLM performance is complex because it is hard to measure their ability to understand and reason.\n- **Alignment with human preferences**: Aligning LLMs with human values and preferences is an ongoing challenge.\n\nActive research is underway to address these challenges and develop more robust, responsible LLMs aligned with human needs.'),
    ('What is a language model and how is it generally evaluated?', 'A language model is a probability distribution over a sequence of words. It is generally evaluated using perplexity, which measures the quality of the models predictions on new texts. The lower the perplexity, the better the models performance.'),
    ('Describe the tokenization process in the context of language models.', 'Tokenization is the process of breaking down text into individual units called tokens. These tokens can be words, characters, or subwords. Byte Pair Encoding (BPE) is a common tokenization method that uses a large text corpus to learn the most frequent pairs of tokens and merges them into a single token.'),
    ('Explain the concept of perplexity and its relationship to model performance.', 'Perplexity is a measure of the quality of a language model predictions. It is calculated as the exponential of the average cross-entropy. A lower perplexity indicates better model performance, meaning the model is more confident in its predictions.'),
    ('In 1-2 sentences, what are the limitations of using perplexity as an evaluation method?', 'Perplexity depends on the vocabulary size, making it difficult to compare models using different tokenization schemes. Additionally, it does not fully capture a models ability to generate human-like and coherent language.'),
    ('In 1-2 sentences, describe the general process of training a large language model.', 'The training process of a LLM involves pre-training the model on a large text corpus, then fine-tuning it on a specific task. Scaling laws suggest that larger models trained on more data yield better results.'),
    ('In 1-2 sentences, what is supervised fine-tuning (SFT) and how is it used to create a user assistant?', 'Supervised fine-tuning (SFT) involves refining a pre-trained LLM on a smaller dataset of human-annotated input-output pairs. In creating a user assistant, SFT is used to align the model with specific human instructions and preferences.'),
    ('In 1-2 sentences, what are the two main limitations of SFT?', 'Two main limitations of SFT are behavior cloning, where the model simply imitates responses observed in the training data, and hallucinations, where the model generates outputs not grounded in the input information.'),
    ('In 1-2 sentences, explain the concept of Reinforcement Learning from Human Feedback (RLHF).', 'RLHF is a technique that uses human feedback to improve a language models performance. It involves training a reward model to rank the models different outputs, then using this reward model to fine-tune the LLM using reinforcement learning algorithms.'),
    ('In 1-2 sentences, what is the difference between RLHF and Direct Preference Optimization (DPO)?', 'RLHF uses a reward model to learn human preferences, while DPO directly optimizes the model preferences based on human feedback. DPO is considered more efficient and stable than RLHF.'),
    ('In 1-2 sentences, describe some challenges associated with evaluating large language models.', 'Evaluating LLMs is difficult due to the subjectivity of human language. Response preferences can vary significantly from one person to another. Additionally, LLMs can generate long and complex responses, making automatic evaluation challenging.')
]
