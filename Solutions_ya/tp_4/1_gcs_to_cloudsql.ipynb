{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we’ll walk through the process of interacting with a Google Cloud Storage (GCS) bucket named dauphine-bucket, specifically focusing on the data directory within the bucket. We’ll cover how to:\n",
    "\n",
    "- List all files in the bucket’s data directory.\n",
    "- Retrieve information about a specific file.\n",
    "- Read files using the Unstructured library.\n",
    "- Visualize the extracted documents with LangChain.\n",
    "\n",
    "This guide is intended for users who are familiar with Python and basic cloud storage concepts.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following:\n",
    "\n",
    "- Python 3.x installed on your system.\n",
    "- Access to the GCP bucket dauphine-bucket/data with the necessary permissions.\n",
    "- Google Cloud SDK installed and authenticated. You can authenticate by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=5eoApHSMjHpXZ0qXGbYK6gtZw9iuGq&access_type=offline&code_challenge=LU9XTFdd5OJuYGUq-_U-Q6RkH3bciVnQ--6pG961YsI&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [abdelhedi.youssef8@gmail.com].\n",
      "Your current project is [dauphine-437611].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python libraries installed:\n",
    "- google-cloud-storage\n",
    "- unstructured\n",
    "- langchain\n",
    "\n",
    "To know more about the libraries, you can visit the following links:\n",
    "- [google-cloud-storage](https://googleapis.dev/python/storage/latest/index.html)\n",
    "- [unstructured](https://docs.unstructured.io/examplecode/codesamples/oss/vector-database)\n",
    "- [langchain](https://langchain.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q google-cloud-storage unstructured langchain python-magic sqlalchemy langchain_google_cloud_sql_pg\n",
    "%pip install -q \"unstructured[pptx]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Listing and loading files from a GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Listing Files in the GCP Bucket\n",
    "\n",
    "Explanation:\n",
    "\n",
    "To interact with a GCS bucket, we’ll use the google-cloud-storage library. We’ll initialize a client, access the bucket, and list all the files within the data directory.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'dauphine-bucket/data':\n",
      "data/\n",
      "data/1 - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Access the bucket\n",
    "bucket_name = 'dauphine-bucket'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# List all files in the 'data' directory\n",
    "blobs = bucket.list_blobs(prefix='data/')\n",
    "\n",
    "print(\"Files in 'dauphine-bucket/data':\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "Running this code will display all the file paths within the data directory of the bucket. The prefix='data/' parameter ensures we only get files from that specific directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Getting Information About One File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Sometimes, you may need detailed information about a specific file, such as its size, content type, or the last time it was updated. We’ll retrieve this metadata for a chosen file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for 'data/1 - Gen AI - Dauphine Tunis.pptx':\n",
      "Size: 6724048 bytes\n",
      "Content Type: application/vnd.openxmlformats-officedocument.presentationml.presentation\n",
      "Updated On: 2024-10-07 09:52:30.256000+00:00\n",
      "Blob name: data/1 - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path (replace with an actual file from your bucket)\n",
    "file_path = 'data/1 - Gen AI - Dauphine Tunis.pptx'\n",
    "\n",
    "# Get the blob object\n",
    "blob = bucket.get_blob(file_path)\n",
    "\n",
    "# TODO\n",
    "if blob:\n",
    "    print(f\"Information for '{file_path}':\")\n",
    "    print(f\"Size: {blob.size} bytes\")\n",
    "    print(f\"Content Type: {blob.content_type}\")\n",
    "    print(f\"Updated On: {blob.updated}\") \n",
    "    print(f\"Blob name: {blob.name}\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found in the bucket.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "This code will output metadata about the specified file. Make sure to replace 'data/your_file.ext' with the actual file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Reading Files with Unstructured\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Unstructured library allows us to parse and process unstructured data from various file formats. We’ll download a file from the bucket and use Unstructured to read and extract its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from turtle import mode\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "\n",
    "\n",
    "DOWNLOADED_LOCAL_FIRECTORY = \"./downloaded_files\"\n",
    "\n",
    "# Function to download the file: file_path from the GCS Bucket\n",
    "def download_file_from_bucket(bucket: Bucket, file_path: str) -> str:\n",
    "    # Download the file locally\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    # data/1 - Gen AI - Dauphine Tunis.pptx' -> 1 - Gen AI - Dauphine Tunis.pptx'\n",
    "    local_file_name = os.path.basename(file_path)\n",
    "\n",
    "    # ./downloaded_files/1 - Gen AI - Dauphine Tunis.pptx\n",
    "    local_filepath = os.path.join(DOWNLOADED_LOCAL_FIRECTORY, local_file_name)\n",
    "\n",
    "    blob.download_to_filename(local_filepath)\n",
    "    print(f\"Downloaded '{file_path}' to '{local_file_name}'\")\n",
    "    return local_filepath\n",
    "\n",
    "\n",
    "def read_file_from_local(local_filepath: str) -> list[Document]:\n",
    "    # Use Unstructured to read the file\n",
    "    loader = UnstructuredLoader(file_path=local_filepath) # TODO\n",
    "    documents = loader.load() # TODO\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing 'data/': [Errno 2] No such file or directory: './downloaded_files\\\\'\n",
      "An error occurred while processing 'data/1 - Gen AI - Dauphine Tunis.pptx': 403 GET https://storage.googleapis.com/download/storage/v1/b/dauphine-bucket/o/data%2F1%20-%20Gen%20AI%20-%20Dauphine%20Tunis.pptx?alt=media: The billing account for the owning project is disabled in state closed: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)\n",
      "An error occurred while processing 'data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx': 403 GET https://storage.googleapis.com/download/storage/v1/b/dauphine-bucket/o/data%2F2.1%20-%20Before%20Transformers%20-%20Gen%20AI%20-%20Dauphine%20Tunis.pptx?alt=media: The billing account for the owning project is disabled in state closed: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)\n",
      "An error occurred while processing 'data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx': 403 GET https://storage.googleapis.com/download/storage/v1/b/dauphine-bucket/o/data%2F2.2%20%20-%20Transformers%20-%20Gen%20AI%20-%20Dauphine%20Tunis.pptx?alt=media: The billing account for the owning project is disabled in state closed: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)\n",
      "An error occurred while processing 'data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx': 403 GET https://storage.googleapis.com/download/storage/v1/b/dauphine-bucket/o/data%2F3%20-%20Retrieval%20Augmented%20Generation%20-%20Gen%20AI%20-%20Dauphine%20Tunis.pptx?alt=media: The billing account for the owning project is disabled in state closed: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)\n"
     ]
    }
   ],
   "source": [
    "# Load all the\n",
    "blobs = list(bucket.list_blobs(prefix='data/'))\n",
    "documents: list[Document] = []\n",
    "if blobs:\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            local_filepath = download_file_from_bucket(bucket, blob.name)\n",
    "            documents.extend(read_file_from_local(local_filepath))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{blob.name}': {e}\")\n",
    "else:\n",
    "    print(\"No files found in the 'data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Visualizing the First Documents Extracted with LangChain\n",
    "\n",
    "Explanation:\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. We’ll use it to load and visualize the documents extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Generative AI with LLM\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '48f84e1bdca7ae95acb10169474c3135'}\n",
      "\n",
      "Content:\n",
      "Florian Bastin\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f4361d35367e66b1a0e01ea107730df7'}\n",
      "\n",
      "Content:\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 3, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'parent_id': 'f4361d35367e66b1a0e01ea107730df7', 'category': 'Title', 'element_id': '8f1c3dd1534501a701e8205a629a5c4b'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents[:3]:\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Join extracted document by page\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- The text extraction block is uninformative because very small text blocks are extracted from the document.\n",
    "- We can join the extracted text by page to get a more meaningful output.\n",
    "- A metadata with the 'page_number' can be helpful\n",
    "- The other metadatas need to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Page Number: 1\n",
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '48f84e1bdca7ae95acb10169474c3135'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 2\n",
      "Content:\n",
      "I.A Pretraining Large Language Model\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "Pre training phase\n",
      "‹#›\n",
      "II. Transformers\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c580c8bf0896b0e7bd6f302dd576638b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 3\n",
      "Content:\n",
      "I.A.1 Introduction\n",
      "Language modelling\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "Autoregressive language models:\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "III. Introduction\n",
      "LLM vs RAG\n",
      "RAG\n",
      "LLM\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ed4deadcc12eabded89280e28df5354a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 4\n",
      "Content:\n",
      "I.A.1 Introduction\n",
      "Language modelling\n",
      "The goal is to generate token by token \n",
      "The steps for generation:\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Decoding\n",
      "8832\n",
      "Model\n",
      "9140           388     527    24592\n",
      "Transformers are encoder \n",
      "‹#›\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "II.A. Before Transformers \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "‹#›\n",
      "III. Introduction\n",
      "RAG Definition\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3ef74a05a5e598bb1f524bbc973f740d'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 5\n",
      "Content:\n",
      "I.A.1 Introduction\n",
      "How the model works ? \n",
      "The general model pipeline is as follows:\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "Model\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "‹#›\n",
      "III.1. Basic Architecture\n",
      "RAG Architecture\n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "‹#›\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c154e5863f9d11af3b016fdd5abfcf8d'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 6\n",
      "Content:\n",
      "I.A.2 Cross Entropy Loss\n",
      "Cross entropy loss\n",
      "The general model pipeline is as follows:\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N Grams\n",
      "Input text: \n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "N-gram generator \n",
      "‹#›\n",
      "II.B Introduction\n",
      "‹#›\n",
      "III.1. Basic Architecture\n",
      "RAG Architecture\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f91104bc670b3c601cf75379a684f4a1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 7\n",
      "Content:\n",
      "I.A.3 Tokenization\n",
      "Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "Model\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "III.1. Basic Architecture\n",
      "RAG Architecture\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c0f28a89fc370c59bde35ffe6dc02a24'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 8\n",
      "Content:\n",
      "I.A.3 Tokenization\n",
      "Tokenization\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "From One-hot encoding to Word Embedding \n",
      "One-hot encoding \n",
      "Word Embedding \n",
      "Index Mot 0 a 1 the 2 he … … 1280 transformers 1281 embedding 1282 partial … … 34567 tunis 34568 dolphin\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "Index: 1280\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "‹#›\n",
      "III.1. Basic Architecture\n",
      "How to ?\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5a09accaaf6ffd9dafc4be3ea7569469'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 9\n",
      "Content:\n",
      "I.A.4 Evaluation\n",
      "Evaluation \n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "‹#›\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "‹#›\n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "TF-IDF\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e211755c09e88bedfc9e82de0253b713'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 10\n",
      "Content:\n",
      "I.A.4 Evaluation\n",
      "Evaluation\n",
      "Hugging Face LLM Leaderboard\n",
      "Evaluation Datasets\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "Model\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '73d3e4fa65984aaac9c8fd2961e119fd'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 11\n",
      "Content:\n",
      "I.A.4 Evaluation\n",
      "Evaluation\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "BIG-Bench Hard [Github]\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "How to give a sentence to a model ? \n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "1\n",
      "2\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "2 ≠1\n",
      "D\n",
      "“transformers”\n",
      "⛔\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "D\n",
      "D\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "D\n",
      "“are”\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '07b5b3260af7df6f299392248ab81635'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 12\n",
      "Content:\n",
      "I.A.5 Data Preprocessing\n",
      "Preprocessing the Data\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "Practice:\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "At the end of training, overfit the model on very quality data\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "decoder\n",
      "encoder\n",
      "Attention mechanism\n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Sparse vs Dense retrieval \n",
      "Sparse embedding (lots of 0)\n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c258e46a21e403d9e4f3eaa9ac9f22e5'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 13\n",
      "Content:\n",
      "I.A.6 Scaling Laws\n",
      "Scaling laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "‹#›\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "3 components: \n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "Attention mechanism\n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "‹#›\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7377eac9f497db5503cff80d203ac94e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 14\n",
      "Content:\n",
      "I.A.7 Training Process\n",
      "Training process\n",
      "Steps \n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "WQ\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Embedding\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "Query\t\t\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Am I a superstar ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "…\n",
      "Are we talking about TV ? \n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "‹#›\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4edca6e6a813ce76686e4f90e2143a4e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 15\n",
      "Content:\n",
      "I.A.8 Cost & Optimizations\n",
      "Optimal model and data size\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "‹#›\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "I am a noun, starting the sentence\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "I am a verb\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "E2\n",
      "K2\n",
      "…\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Might be a TV object  of a model\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.2. Information retrieval \n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd202900ea3196604cdc95357a4e10d2b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 16\n",
      "Content:\n",
      "I.A.8 Cost & Optimizations\n",
      "How much it costs ?\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“The”\n",
      "Optimizing the loss w.r.t weights: \n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“Models” ?\n",
      "“decoder”\n",
      "‹#›\n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "WQ\n",
      "Are we talking about TV ? \n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "Am I a superstar ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "…\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "K1\n",
      "E1\n",
      "…\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "You should be a verb because I am a noun\n",
      "No we are not because I am a Transformer\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "WK\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E2\n",
      "K2\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "K3\n",
      "E3\n",
      "…\n",
      "…\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "…\n",
      "…\n",
      "Might be a TV object  of a model\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "‹#›\n",
      "Vector DB Comparison\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac0d95fa1ee93936c7fe59e8b04191f9'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 17\n",
      "Content:\n",
      "I.B Fine tuning Large Language Model\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "Post training phase\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“The”\n",
      "Feed forward + Softmax model\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "?\n",
      "“decoder”\n",
      "“decoder”\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "WQ\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "WK\n",
      "E2\n",
      "K2\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Efficient similarity search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "‹#›\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '8c07ba4c6ce989e2e94fd2b175952e1a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 18\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "How to get a user assistant ? \n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      "‹#›\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“The”\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“Models” ?\n",
      "“decoder”\n",
      "‹#›\n",
      "StatQuest with Josh Starmer Youtube\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E3\n",
      "E2\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "WQ\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "WK\n",
      "E2\n",
      "K2\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '73e78d6d69b2a46b3a7d162c92074f97'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 19\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "How to get a user assistant ? \n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "ChatGPT\n",
      "Conversational Agent\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN Types of architectures\n",
      "“The”\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Q. Which one fit our use case ? \n",
      "“Models” ?\n",
      "“decoder”\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "StatQuest with Josh Starmer Youtube\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "The sum of each column is 1 \n",
      "This step allows numerical stability \n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\n",
      "4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\n",
      "4/128 70/128 0 85/128 -4/128 -10 0 0 2/128 0 0 0 3/128 -3/128 4/128 5/128\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Softmax\n",
      "K1\n",
      "K1\n",
      "Q1.K1 Q2.K1 Q3.K1 Q4.K1 Q1.K2 Q2.K2 Q3.K2 Q4.K2 Q1.K3 Q2.K3 Q3.K3 Q4.K3 Q1.K4 Q2.K4 Q3.K4 Q4.K4\n",
      "Q1.K1/√dk Q2.K1/√dk Q3.K1/√dk Q4.K1/√dk Q1.K2/√dk Q2.K2/√dk Q3.K2/√dk Q4.K2/√dk Q1.K3/√dk Q2.K3/√dk Q3.K3/√dk Q4.K3/√dk Q1.K4/√dk Q2.K4/√dk Q3.K4/√dk Q4.K4/√dk\n",
      "K2\n",
      "K2\n",
      "K3\n",
      "K3\n",
      "‹#›\n",
      "K4\n",
      "K4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Reranker\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "Bi Encoder vs Cross Encoder\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac2f2ec1ea10b394bdcde84f69281f3a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 20\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "How to get a User Assistant from a Language Model ? \n",
      "‹#›\n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "Model\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "This step allows numerical stability \n",
      "4 70 0 85 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5\n",
      "4/128 70/128 0 85/128 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5/128\n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Query augmentation\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "‹#›\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '609e0c114841e747af75094186fcd7b8'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 21\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "Famous LLM follow user instructions with moderation\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "Wv\n",
      "1x V1 0.97 x V1 0.33 x V2 0.85 x V1 0 x V2 0.03 x V2 0.33 x V2 0.02 x V2 0 x V3 0 x V3 0.33 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "V1\n",
      "E1\n",
      "Embedding\n",
      "VALUES\t\t\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "E2\n",
      "V2\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "V3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "V4\n",
      "E4\n",
      "III.4. RAG Techniques\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '50b34c690dff813c7b1734d873c4e028'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 22\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "‹#›\n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“The”\n",
      "“transformers”\n",
      "Ct-1\n",
      "“are”\n",
      "ht-1\n",
      "“encoder”\n",
      "“decoder”\n",
      "“models”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Attention pattern\n",
      "z1\n",
      "z3\n",
      "z4\n",
      "z2\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "‹#›\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3949fea7038d535d9ef77269097721db'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 23\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      "‹#›\n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "This step allows numerical stability \n",
      "4 70 0 85 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5\n",
      "4/128 70/128 0 85/128 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5/128\n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Softmax\n",
      "K1\n",
      "K1\n",
      "Q1.K1 Q2.K1 Q3.K1 Q4.K1 Q1.K2 Q2.K2 Q3.K2 Q4.K2 Q1.K3 Q2.K3 Q3.K3 Q4.K3 Q1.K4 Q2.K4 Q3.K4 Q4.K4\n",
      "Q1.K1/√dk Q2.K1/√dk Q3.K1/√dk Q4.K1/√dk Q1.K2/√dk Q2.K2/√dk Q3.K2/√dk Q4.K2/√dk Q1.K3/√dk Q2.K3/√dk Q3.K3/√dk Q4.K3/√dk Q1.K4/√dk Q2.K4/√dk Q3.K4/√dk Q4.K4/√dk\n",
      "K2\n",
      "K2\n",
      "K3\n",
      "K3\n",
      "‹#›\n",
      "K4\n",
      "K4\n",
      "III.4. RAG Techniques\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '07c77e5dc87590cb20bc38dcb4ba9804'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 24\n",
      "Content:\n",
      "I.B.A Supervised Fine Tuning\n",
      "Loss\n",
      "How Supervised Fine Tuning Works ? \n",
      "Child \n",
      "Some 👌 \n",
      "Same process than the language model training\n",
      "3892\n",
      "Model\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "‹#›\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Cross attention\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Les\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "70 70 0 85 -4 35 0 0 2 0 30 0 3 -3 4 18 … … … …\n",
      "K1\n",
      "E1\n",
      "Transformers\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "French to english translation example \n",
      "No masking\n",
      "E2\n",
      "K2\n",
      "sont\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "des\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "…\n",
      "III.4. RAG Techniques\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '106077e33bbd1746a880ca3d68974cb4'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 25\n",
      "Content:\n",
      "I.B.2 RLHF\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "SFT Limitations: \n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Cross attention\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "‹#›\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3f3edd5deab91e90566409af84a75f8e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 26\n",
      "Content:\n",
      "I.B.2 RLHF\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Idea:\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "GPT 3 dimension for one attention head\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Embedding (Embedding Dimension, N words) (12 288, 50 257) 49 152 params Key (Key size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Query (Query size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Value up (Value size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Value down (Embedding Dimension, Value size) (12 288, 128) 1 572 864 params per head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Chunking\n",
      "To avoid context limitations, we can do document chunking:\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "‹#›\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '001df5977c422ed6f539c363ef699e86'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 27\n",
      "Content:\n",
      "I.B.3 Reward Model\n",
      "Reward model\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Embedding (Embedding Dimension, N words) (12 288, 4) 49 152 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads 150 994 944 params\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "RAG Frameworks in Python\n",
      "‹#›\n",
      "Langchain \n",
      "LlamaIndex \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e4a961bfbec4d7b23ff8b88a15b85a55'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 28\n",
      "Content:\n",
      "I.B.4 PPO & DPO\n",
      "Training RL model\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "128\n",
      "12 288\n",
      "0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\n",
      "12 288\n",
      "1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02\n",
      "=\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '106d2684a898e6e255b280b7bc1ebdc3'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 29\n",
      "Content:\n",
      "I.B.4 PPO & DPO\n",
      "DPO\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "‹#›\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "Model\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "128\n",
      "12 288\n",
      "0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\n",
      "12 288\n",
      "1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02\n",
      "=\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '069bc37ee7295758412cadf554d65e1a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 30\n",
      "Content:\n",
      "I.B.5 Evaluation & Challenges\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "128\n",
      "12 288\n",
      "0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\n",
      "12 288\n",
      "1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\n",
      "1 0.97 0.33 0.85 0 0.03 0.33 0.02\n",
      "=\n",
      " .\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "Retriever Evaluation : NDCG\n",
      "NDCG can take values from 0 to 1. \n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 30, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6d981d909b94ac8b6769fb893b17ae67'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 31\n",
      "Content:\n",
      "I.B.5 Evaluation & Challenges\n",
      "RLHF challenges\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "‹#›\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      "0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\n",
      "12 288\n",
      "E1\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "0.65 0.3 0.23 0.4\n",
      "1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\n",
      " .\n",
      " =\n",
      " =    V1\n",
      "12 288\n",
      "12 288\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "Answer Evaluation: LLM As a judge\n",
      "Ask an LLM to evaluate answer quality: \n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "Evaluating answers in RAG is not easy\n",
      "‹#›\n",
      "RAGAS\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 31, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7bd9a098819b62b131c8adb9131751a7'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 32\n",
      "Content:\n",
      "I.B.5 Evaluation & Challenges\n",
      "Evaluation\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "‹#›\n",
      "Chatbot Arena, Open LM\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0.1 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "0.62 x V1 0.1 x V2 0 x V3 0 x V4\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      "+\n",
      " …\n",
      " =\n",
      "12 288\n",
      "12 288\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "Multimodal\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "‹#›\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 32, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2e8db4b0ac1f99576f67f58bcc4fd8cc'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 33\n",
      "Content:\n",
      "I.B.5 Evaluation & Challenges\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "‹#›\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0.1 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\n",
      "Step 2:\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "0.62 x V1 0.1 x V2 0 x V3 0 x V4\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      "+\n",
      " …\n",
      " =\n",
      "128\n",
      "128\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "Multimodal: Qwen2- VL\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "‹#›\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 33, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ba3adbe5a56a58502c3b011279a1bf97'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 34\n",
      "Content:\n",
      "I.B.5 Evaluation & Challenges\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "‹#›\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "Rotary Embedding (ROPE)\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "‹#›\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 34, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7c051edb8c46239c37f51d06b6e1498f'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 35\n",
      "Content:\n",
      "Conclusion\n",
      "Conclusion\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Building GPT 3 \n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "General knowledges \n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "Multimodal RAG\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "‹#›\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 35, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2959da6cb52f5c994b0c257ae2c4de2b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 36\n",
      "Content:\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "decoder\n",
      "encoder\n",
      "Attention mechanism\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "Multimodal RAG\n",
      "‹#›\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 36, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6358a792ba02efd9ff5950303edeccbc'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 37\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "N words\n",
      "z1\n",
      "z3\n",
      "z4\n",
      "z2\n",
      "Z\n",
      " |V| (=128)\n",
      "z1\n",
      "z3\n",
      "z4\n",
      "z2\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "Self-RAG \n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "‹#›\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 37, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '943c8089fe28e52d7feb03c826d109e2'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 38\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "N words\n",
      "z1\n",
      "…\n",
      "z1\n",
      "…\n",
      "z2\n",
      "Concatenation\n",
      "z10\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z3\n",
      "z96\n",
      "z96\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "RAPTOR\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "‹#›\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 38, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3c01eedfb26bbc8feeda806a32b63a00'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 39\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "W0\n",
      "…\n",
      "x\n",
      "z1\n",
      "Z\n",
      "| Embeddings Dim|\n",
      "z10\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      " |V| x N heads \n",
      "N words\n",
      "z96\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "Corrective RAG (CRAG)\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "‹#›\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 39, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd9c57f397cf0f142e342868c836c46f3'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 40\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "3\n",
      "Step 3\n",
      "Input sentence: Transformers are encoder decoder\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "Z0\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "WQ0\n",
      "Q0\n",
      "WK0\n",
      "K0\n",
      "Transformers \n",
      "are  \n",
      "encoder \n",
      "decoder\n",
      "WV0\n",
      "V0\n",
      "4\n",
      "5\n",
      "4\n",
      "Step 1\n",
      "Step 2\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "GraphRAG\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "‹#›\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 40, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '305a291fdbeb8fb35ecf92477d4e13e0'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 41\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Q0\n",
      "WQ0\n",
      "K0\n",
      "WK0\n",
      "Z0\n",
      "V0\n",
      "WV0\n",
      "Step 4\n",
      "…\n",
      "…\n",
      "…\n",
      "WQ96\n",
      "Q96\n",
      "WK96\n",
      "K96\n",
      "Z96\n",
      "WV96\n",
      "V96\n",
      "‹#›\n",
      "III. Conclusion\n",
      "Conclusion\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 41, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7376db98e0ebd4ca083005b8cccdc7df'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 42\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "3\n",
      "5\n",
      "3\n",
      "Z0\n",
      "W0\n",
      "4\n",
      "Z\n",
      "5\n",
      "4\n",
      "…\n",
      "4 x 96\n",
      "4 x 96\n",
      "3\n",
      "Z96\n",
      "4\n",
      "Step 5\n",
      "Matmul\n",
      "‹#›\n",
      "III. Conclusion\n",
      "Conclusion\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 42, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '65274390c56ca6c82797e47d34babe8b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 43\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Why do Z and X have the same dimension ?\n",
      "X\n",
      "5\n",
      "Transformers \n",
      "are  \n",
      "encoder \n",
      "decoder\n",
      "4\n",
      "5\n",
      "Z\n",
      "4\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 43, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6e3734ce497700075c099b880be480b9'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 44\n",
      "Content:\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 44, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0c4cfc138c9e69d01d6890dfaa30f5fe'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 45\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 45, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '910d5288446549c9afe163a2f01239ed'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 46\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "‹#›\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 46, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '88c4a6a8db5f9fddc9a9afd7cb41dcfa'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 47\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 47, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '94748fc455ec327fd873924ff0b49286'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 48\n",
      "Content:\n",
      "So far …\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 48, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f068d9a56c787a4884b202d114ea6071'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 49\n",
      "Content:\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 49, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f49efd3e40aa5c5e7a69800d68e710f8'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 50\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 50, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b9271b2168595a88ae7b428016375208'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 51\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "5\n",
      "X\n",
      "Feed forward layer\n",
      "Transformers \n",
      "are  \n",
      "encoder \n",
      "decoder\n",
      "4\n",
      "Z\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "5\n",
      "4\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 51, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6257fdb286815eb315d54f96b31353db'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 52\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "X\n",
      "5\n",
      "Transformers \n",
      "are  \n",
      "encoder \n",
      "decoder\n",
      "4\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "Z\n",
      "5\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "4\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 52, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f2ea9a25564678bdd306c254497756c7'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 53\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "Am I a math architecture ?\n",
      "5\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Am I a plural noun ?\n",
      "4\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "Am I a football star ?\n",
      "…\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 53, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ec2067881742a76c8199b471404a359a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 54\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "Feed forward network\n",
      "Am I a math architecture ?\n",
      "5\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Am I a singular noun ?\n",
      "4\n",
      "…\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "Am I a TV stuff ?\n",
      "…\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 54, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c362e202867145d5ac3437d51f4b7ea1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 55\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "49 152\n",
      "…\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 55, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'feaf1474fdd6401f2ff18a4e4c08bf5c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 56\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Down projection”\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "12 288\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 56, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f210e03aca7e2c35c3f8fa0acc862421'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 57\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "Embedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads 150 994 944 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) 603 979 776 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) 603 979 776 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 57, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd049b6296445ad0c1e7787043f13888f'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 58\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "Embedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads x 96 layers 14 495 514 624 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) x 96 layers 57 982 058 496 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) x 96 layers 57 982 058 496 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016 params\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 58, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '87211810c095debaff115d866195a948'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 59\n",
      "Content:\n",
      "So far …\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 59, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '67042cb3bd07f252a0f6d011615d6d43'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 60\n",
      "Content:\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 60, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1d29b86c5d5251364e9def08624f25ea'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 61\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 61, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2a1cc903b6011ba40fab6f75a3c6af4c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 62\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 62, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0a443406bda861c1b2f20abd454eac53'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 63\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "are\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "encoder\n",
      "decoder\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 63, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '796e3caf96902ae4ecd0246fb347c07d'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 64\n",
      "Content:\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 64, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f636722846561ca2fafb20df9526b112'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 65\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "Embedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads x 96 layers 14 495 514 624 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) x 96 layers 57 982 058 496 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) x 96 layers 57 982 058 496 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016 params\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 65, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b6978eb9dd7e237469f172cee2ed5f09'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 66\n",
      "Content:\n",
      "So far …\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 66, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1133ba637feb9d8904f5948972cea3e0'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 67\n",
      "Content:\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 67, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac9f1623364d6398e5554307653894d7'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 68\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 68, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ccd5f00a6f60bd9a713d43e16a4b4189'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 69\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 69, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '64346c477176cf056df56d3abef50a68'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 70\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 70, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c1a9593890e1fc18e67bc177550a3757'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 71\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 71, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac51ab6d52990484ffb9c4d1c4a409c3'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 72\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "Properties\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 72, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4da9648bc95b0f002595f0a2e3f391e4'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 73\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 73, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '99683a9442622f02e05cb807624be956'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 74\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 74, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '95e605108505460d65f8754ebd3d8c9e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 75\n",
      "Content:\n",
      "So far …\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 75, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '642f5211980567c82720ba3fa749c8b1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 76\n",
      "Content:\n",
      "II. Resources\n",
      "Blogs:\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 76, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e0a64f16c335f37d2c35d86037cc0399'}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to merge documents by page number\n",
    "def merge_documents_by_page(documents: list[Document]) -> list[Document]:\n",
    "    merged_documents: list[Document] = []\n",
    "    page_dict = {}\n",
    "\n",
    "    # Group documents by page number\n",
    "    for doc in documents:\n",
    "        page_number = doc.metadata.get('page_number')\n",
    "        if page_number is not None:\n",
    "            if page_number not in page_dict:\n",
    "                page_dict[page_number] = [doc]\n",
    "            else:\n",
    "                page_dict[page_number].append(doc)\n",
    "\n",
    "    # Merge documents for each page\n",
    "    for page_number, docs in page_dict.items():\n",
    "        if docs:\n",
    "            # Use the metadata of the first document in the group\n",
    "            merged_metadata = docs[0].metadata\n",
    "            # Concatenate the page content of all documents in the group\n",
    "            merged_content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            # Create a new Document with merged content and metadata\n",
    "            merged_documents.append(Document(page_content=merged_content, metadata=merged_metadata))# TODO Add a document with merged content and metadata)\n",
    "\n",
    "    return merged_documents\n",
    "\n",
    "# Merge the documents by page\n",
    "merged_documents = merge_documents_by_page(documents)\n",
    "\n",
    "# Print the merged documents\n",
    "for doc in merged_documents:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Page Number: {doc.metadata.get('page_number')}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Ingesting in Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ingest each merged_document in Cloud SQL.\n",
    "\n",
    "ALREADY DONE by teacher: \n",
    "- Create a Cloud SQL instance\n",
    "- Create a database in the instance\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Create a table in CloudSQL with you initials\n",
    "- Create the schema of the table\n",
    "- Ingest the data in the table\n",
    "\n",
    "\n",
    "Follow this [documentation](https://python.langchain.com/docs/integrations/vectorstores/google_cloud_sql_pg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Understand how to connect to Cloud SQL \n",
    "\n",
    "\n",
    "First we need to connect to Cloud SQL \n",
    "- Follow this [link](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy) to understand how it works\n",
    "\n",
    "Then be familiar ith the following PostgreSQL commands:\n",
    "```bash \n",
    "`psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=postgres\"` # to connect to the user `postgres`\n",
    "# the user we use is `students`\n",
    "# a password provided by the teacher is required\n",
    "`\\l` # to list all databases\n",
    "`\\c gen_ai_db` # to connect to the database `gen_ai_db`\n",
    "`\\dt` # to list all tables\n",
    "`\\d+ table_name` # to describe a table\n",
    "`SELECT * FROM table_name` # to select all rows from a table\n",
    "`\\du` # to list all users\n",
    "`\\q` # to quit\n",
    "`CREATE DATABASE db_name;` # to create a database\n",
    "`CREATE USER user_name WITH PASSWORD 'password';` # to create a user\n",
    "`GRANT ALL PRIVILEGES ON DATABASE db_name TO user_name;` # to grant all privileges to a user on a database\n",
    "`GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO user_name;` # to grant all privileges to a user on all tables in a schema\n",
    "`ALTER USER user_name WITH SUPERUSER;` # to grant superuser privileges to a user\n",
    "`DROP DATABASE db_name;` # to drop a database\n",
    "`DROP USER user_name;` # to drop a user\n",
    "`DROP TABLE table_name;` # to drop a table\n",
    "`REVOKE ALL PRIVILEGES ON DATABASE db_name FROM user_name;` # to revoke all privileges from a user on a database\n",
    "```\n",
    "\n",
    "When Cloud SQL Proxy is downloaded and the tutorial is followed. You should be connected to the instance. \n",
    "You can connect to the dabase as a user `students` with the password provided by the teacher.\n",
    "  - `psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=students\"`\n",
    "  - Enter the password provided by the teacher\n",
    "Try to create a table `initial_tests_table` with the following schema:\n",
    "  - `CREATE TABLE initial_tests_table (id SERIAL PRIMARY KEY, document TEXT, page_number INT, title TEXT, author TEXT, date TEXT);`\n",
    "  - `\\dt` to check if the table has been created\n",
    "  - `\\d+ initial_tests_table` to check the schema of the table\n",
    "  - `DROP TABLE initial_tests_table;` to drop the table\n",
    "  - `\\q` to quit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-cloud-sql-pg langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import PROJECT_ID, REGION, INSTANCE, DATABASE, DB_USER\n",
    "DB_PASSWORD = os.environ[\"DB_PASSWORD\"]\n",
    "TABLE_NAME = os.environ[\"TABLE_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "engine = PostgresEngine.from_instance(\n",
    "    project_id=PROJECT_ID,\n",
    "    instance=INSTANCE,\n",
    "    region=REGION,\n",
    "    database=DATABASE,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "409, message='Conflict', url='https://sqladmin.googleapis.com/sql/v1beta4/projects/dauphine-437611/instances/gen-ai-instance:generateEphemeralCert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProgrammingError\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mainit_vectorstore_table(\n\u001b[0;32m      6\u001b[0m         table_name\u001b[38;5;241m=\u001b[39mTABLE_NAME, \u001b[38;5;66;03m# Vector size for VertexAI model(textembedding-gecko@latest)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProgrammingError:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable already created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\langchain_google_cloud_sql_pg\\engine.py:506\u001b[0m, in \u001b[0;36mPostgresEngine.ainit_vectorstore_table\u001b[1;34m(self, table_name, vector_size, schema_name, content_column, embedding_column, metadata_columns, metadata_json_column, id_column, overwrite_existing, store_metadata)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainit_vectorstore_table\u001b[39m(\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    473\u001b[0m     table_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m     store_metadata: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    483\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m    Create a table for saving of vectors to be used with PostgresVectorStore.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m            Default: True.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_as_async(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainit_vectorstore_table(\n\u001b[0;32m    508\u001b[0m             table_name,\n\u001b[0;32m    509\u001b[0m             vector_size,\n\u001b[0;32m    510\u001b[0m             schema_name,\n\u001b[0;32m    511\u001b[0m             content_column,\n\u001b[0;32m    512\u001b[0m             embedding_column,\n\u001b[0;32m    513\u001b[0m             metadata_columns,\n\u001b[0;32m    514\u001b[0m             metadata_json_column,\n\u001b[0;32m    515\u001b[0m             id_column,\n\u001b[0;32m    516\u001b[0m             overwrite_existing,\n\u001b[0;32m    517\u001b[0m             store_metadata,\n\u001b[0;32m    518\u001b[0m         )\n\u001b[0;32m    519\u001b[0m     )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\langchain_google_cloud_sql_pg\\engine.py:388\u001b[0m, in \u001b[0;36mPostgresEngine._run_as_async\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Otherwise, run in the background thread\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwrap_future(\n\u001b[0;32m    389\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[0;32m    390\u001b[0m )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\langchain_google_cloud_sql_pg\\engine.py:442\u001b[0m, in \u001b[0;36mPostgresEngine._ainit_vectorstore_table\u001b[1;34m(self, table_name, vector_size, schema_name, content_column, embedding_column, metadata_columns, metadata_json_column, id_column, overwrite_existing, store_metadata)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ainit_vectorstore_table\u001b[39m(\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    406\u001b[0m     table_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     store_metadata: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    Create a table for saving of vectors to be used with PostgresVectorStore.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m        :class:`UndefinedObjectError <asyncpg.exceptions.UndefinedObjectError>`: if the data type of the id column is not a postgreSQL data type.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mexecute(text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE EXTENSION IF NOT EXISTS vector\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\ext\\asyncio\\base.py:121\u001b[0m, in \u001b[0;36mStartableContext.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T_co:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart(is_ctxmanager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\ext\\asyncio\\engine.py:273\u001b[0m, in \u001b[0;36mAsyncConnection.start\u001b[1;34m(self, is_ctxmanager)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_connection:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidRequestError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection is already started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_proxied(\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m greenlet_spawn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_engine\u001b[38;5;241m.\u001b[39mconnect)\n\u001b[0;32m    274\u001b[0m )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\util\\_concurrency_py3k.py:201\u001b[0m, in \u001b[0;36mgreenlet_spawn\u001b[1;34m(fn, _require_await, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# this allows an exception to be raised within\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# the moderated greenlet so that it can continue\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# its expected flow.\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mswitch(value)\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3278\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Connection:\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3257\u001b[0m \n\u001b[0;32m   3258\u001b[0m \u001b[38;5;124;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3275\u001b[0m \n\u001b[0;32m   3276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    148\u001b[0m         Connection\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m    149\u001b[0m             err, dialect, engine\n\u001b[0;32m    150\u001b[0m         )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3302\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m   3281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3282\u001b[0m \n\u001b[0;32m   3283\u001b[0m \u001b[38;5;124;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3300\u001b[0m \n\u001b[0;32m   3301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1263\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_checkout\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ConnectionFairy:\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1263\u001b[0m         fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             threadconns\u001b[38;5;241m.\u001b[39mcurrent \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(fairy)\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:712\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    710\u001b[0m     rec \u001b[38;5;241m=\u001b[39m cast(_ConnectionRecord, pool\u001b[38;5;241m.\u001b[39m_do_get())\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     dbapi_connection \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget_connection()\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inc_overflow():\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConnectionPoolEntry:\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:674\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 900\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:896\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m     pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:362\u001b[0m, in \u001b[0;36mPool._should_wrap_creator.<locals>.<lambda>\u001b[1;34m(rec)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# all other cases, just wrap and assume legacy \"creator\" callable\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# thing\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m     creator_fn \u001b[38;5;241m=\u001b[39m cast(_CreatorFnType, creator)\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m rec: \u001b[43mcreator_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\ext\\asyncio\\engine.py:115\u001b[0m, in \u001b[0;36mcreate_async_engine.<locals>.creator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreator\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# note that to send adapted arguments like\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# prepared_statement_cache_size, user would use\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# \"creator\" and emulate this form here\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdbapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_creator_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_creator\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\dialects\\postgresql\\asyncpg.py:953\u001b[0m, in \u001b[0;36mAsyncAdapt_asyncpg_dbapi.connect\u001b[1;34m(self, *arg, **kw)\u001b[0m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncAdaptFallback_asyncpg_connection(\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    946\u001b[0m         await_fallback(creator_fn(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)),\n\u001b[0;32m    947\u001b[0m         prepared_statement_cache_size\u001b[38;5;241m=\u001b[39mprepared_statement_cache_size,\n\u001b[0;32m    948\u001b[0m         prepared_statement_name_func\u001b[38;5;241m=\u001b[39mprepared_statement_name_func,\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncAdapt_asyncpg_connection(\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m--> 953\u001b[0m         \u001b[43mawait_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreator_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    954\u001b[0m         prepared_statement_cache_size\u001b[38;5;241m=\u001b[39mprepared_statement_cache_size,\n\u001b[0;32m    955\u001b[0m         prepared_statement_name_func\u001b[38;5;241m=\u001b[39mprepared_statement_name_func,\n\u001b[0;32m    956\u001b[0m     )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\util\\_concurrency_py3k.py:132\u001b[0m, in \u001b[0;36mawait_only\u001b[1;34m(awaitable)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mMissingGreenlet(\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlet_spawn has not been called; can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt call await_only() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere. Was IO attempted in an unexpected place?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# returns the control to the driver greenlet passing it\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# a coroutine to run. Once the awaitable is done, the driver greenlet\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# switches back to this greenlet with the result of awaitable that is\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# then returned to the caller (or raised as error)\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mawaitable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\sqlalchemy\\util\\_concurrency_py3k.py:196\u001b[0m, in \u001b[0;36mgreenlet_spawn\u001b[1;34m(fn, _require_await, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m switch_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# wait for a coroutine from await_only and then return its\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# result back to it.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# this allows an exception to be raised within\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# the moderated greenlet so that it can continue\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# its expected flow.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mthrow(\u001b[38;5;241m*\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\langchain_google_cloud_sql_pg\\engine.py:200\u001b[0m, in \u001b[0;36mPostgresEngine._create.<locals>.getconn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetconn\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m asyncpg\u001b[38;5;241m.\u001b[39mConnection:\n\u001b[1;32m--> 200\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect_async(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    203\u001b[0m         user\u001b[38;5;241m=\u001b[39mdb_user,\n\u001b[0;32m    204\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[0;32m    205\u001b[0m         db\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[0;32m    206\u001b[0m         enable_iam_auth\u001b[38;5;241m=\u001b[39menable_iam_auth,\n\u001b[0;32m    207\u001b[0m         ip_type\u001b[38;5;241m=\u001b[39mip_type,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\google\\cloud\\sql\\connector\\connector.py:326\u001b[0m, in \u001b[0;36mConnector.connect_async\u001b[1;34m(self, instance_connection_string, driver, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# attempt to get connection info for Cloud SQL instance\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     conn_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mconnect_info()\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# validate driver matches intended database engine\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     DriverMapping\u001b[38;5;241m.\u001b[39mvalidate_engine(driver, conn_info\u001b[38;5;241m.\u001b[39mdatabase_version)\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\google\\cloud\\sql\\connector\\lazy.py:105\u001b[0m, in \u001b[0;36mLazyRefreshCache.connect_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: Connection info \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh operation started\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     conn_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_connection_info(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_project,\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region,\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instance,\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys,\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_iam_auth,\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: Connection info \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh operation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\google\\cloud\\sql\\connector\\client.py:289\u001b[0m, in \u001b[0;36mCloudSQLClient.get_connection_info\u001b[1;34m(self, project, region, instance, keys, enable_iam_auth)\u001b[0m\n\u001b[0;32m    286\u001b[0m     ephemeral_task\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m ephemeral_cert, expiration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ephemeral_task\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ConnectionInfo(\n\u001b[0;32m    292\u001b[0m     ephemeral_cert,\n\u001b[0;32m    293\u001b[0m     metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver_ca_cert\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m     expiration,\n\u001b[0;32m    298\u001b[0m )\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\google\\cloud\\sql\\connector\\client.py:201\u001b[0m, in \u001b[0;36mCloudSQLClient._get_ephemeral\u001b[1;34m(self, project, instance, pub_key, enable_iam_auth)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    200\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retry_50x(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpost, url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m--> 201\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m ret_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    204\u001b[0m ephemeral_cert: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m ret_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mephemeralCert\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\A garder\\maths\\M2\\Semestre 1\\GENAI\\GenAI-GCP\\exercices\\tp_4\\venv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1157\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_context:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   1160\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m   1161\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[0;32m   1162\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1163\u001b[0m )\n",
      "\u001b[1;31mClientResponseError\u001b[0m: 409, message='Conflict', url='https://sqladmin.googleapis.com/sql/v1beta4/projects/dauphine-437611/instances/gen-ai-instance:generateEphemeralCert'"
     ]
    }
   ],
   "source": [
    "# Create a table in the PostgreSQL database with the required columns\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "try:\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=TABLE_NAME, # Vector size for VertexAI model(textembedding-gecko@latest)\n",
    "        vector_size=768,\n",
    "    )\n",
    "except ProgrammingError:\n",
    "    print(\"Table already created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute \\d+ [YOUR_INITIALS]_table in the psql shell to check the schema of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Create an embedding to convert your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko\",\n",
    "    project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresVectorStore\n",
    "\n",
    "vector_store = PostgresVectorStore.create_sync(  # Use .create() to initialize an async vector store\n",
    "    engine=engine,\n",
    "    table_name=TABLE_NAME,\n",
    "    embedding_service=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efba080d-abdd-45ea-af9f-55c3e8901db0',\n",
       " '46d8a4b0-5249-4e51-ad2f-e91025b2cd6a',\n",
       " '05e96eb5-7061-427f-a876-5c20d21c7d88',\n",
       " '165a1178-972c-431c-9017-1186ff93f376',\n",
       " '7674057d-a9a1-4a0d-bcf7-0d08c99e30f6',\n",
       " '8ed2a30d-2fa0-4b77-baf0-308c52389475',\n",
       " 'e2744621-8d73-4c7d-89f3-56d1fdcb820b',\n",
       " '3c95e7c5-97c7-45a9-9cbf-6a0627d24d09',\n",
       " 'd3e80c19-33f8-473d-8c17-7dec639d4759',\n",
       " '27765abc-54a8-4fdb-bd49-8b9ec2120bde',\n",
       " '2a92dae3-3d16-4ecf-80b8-3c14747b8941',\n",
       " 'c8a0c172-788b-4fb7-851e-5f90a9c89037',\n",
       " 'b8d1bcfd-f50b-4b58-8e9a-d14e2743972c',\n",
       " '39398139-77f5-4cd5-a80e-7d57fde8b3a6',\n",
       " 'f7b09388-e63d-4746-a7c5-316c16ae3136',\n",
       " 'e58206e6-65a4-4865-8f77-89f7f1602953',\n",
       " 'bc828649-d095-4206-ba7c-d9d910954a1f',\n",
       " '968ff0d3-afd5-4530-9462-46fdafa257bd',\n",
       " '68d9750a-0c75-4e9e-b84b-7074fae11f21',\n",
       " 'c8f6dfb3-708f-47c7-bf4d-bbd6896d81a3',\n",
       " '7c385d6b-b608-4da6-aeeb-b7cb6beb0dfe',\n",
       " '650b427c-e567-4ca0-a864-1c09a00ce83e',\n",
       " '3713824d-47ac-4fef-b3b2-d4bc35370270',\n",
       " 'd94a2ad4-a8a4-4530-8a4e-c0129baa22e6',\n",
       " 'aaf375da-10f3-4d65-8ea3-2478f198de17',\n",
       " '03adc516-9220-477b-92db-3f7b90d43fa6',\n",
       " '8fc2792f-0717-4fe8-b89d-6d797e8457cb',\n",
       " '69928471-f2fc-47a4-beef-b693c296575e',\n",
       " '84e45a25-7afc-4a48-ac4f-7041d4a65df2',\n",
       " '569e221a-5627-4177-8390-e07286e94cca',\n",
       " '9701f652-7328-47ca-b872-b20e6f9116c4',\n",
       " '99d6d730-b2e3-443d-a936-095b6910b0de',\n",
       " '9e89352e-43ac-4a17-9a06-0dedef0b50a2',\n",
       " '4f0eaabe-90b7-4a05-bdf1-f8df46e9d519',\n",
       " 'cd63e67c-7575-4959-bf56-706185facfbe',\n",
       " '83a762a5-f9b9-4bd1-b65c-40f2b5654105',\n",
       " 'bd9f432e-2052-49a8-aab7-b4d740f0d802',\n",
       " 'c22a0018-c7f8-45ac-9258-970013390d34',\n",
       " '9bd2d618-53b6-4a76-8e03-900b7a8a051d',\n",
       " 'b96d0e16-7697-42e3-b10e-23162fd5c22f',\n",
       " 'fd2daf4c-bd7a-4a8f-85b3-ab6643ee9f0b',\n",
       " 'bebbd51c-96a9-4ca6-8e4e-43be09a1be26',\n",
       " '1fb954e6-544c-4586-9be2-474aba6b48b4',\n",
       " '263705ba-9f88-4386-bb40-bae1ca8f691a',\n",
       " '86ea5535-12f1-40ff-937f-019ee16163e2',\n",
       " '6ac70ae7-8995-4edc-98ff-55b24a171130',\n",
       " '25f77d22-b824-43d0-a6ec-dca6da943c42',\n",
       " 'a814808b-2740-4490-96f3-4cddcb0a31f4',\n",
       " '50dd3265-5be0-4c82-acf6-d47a44731714',\n",
       " '8947bc79-d413-4041-b257-e9f6fa961234',\n",
       " 'f259af31-e4ee-4ad8-9ec9-05e51e03e095',\n",
       " 'd3926a89-896c-438d-b334-cf509004dba1',\n",
       " '3a65a7bd-e26d-478b-8e99-ca2e79ba3449',\n",
       " '08ab6a6a-886d-4cef-a457-360118acc3e6',\n",
       " 'fb55b954-74f3-46cd-932a-1f740a3e2c2b',\n",
       " 'ad44c9ca-e6d9-4c8c-a2cf-50c21456a397',\n",
       " '1bb86d2b-1fb1-4e74-922a-898259c220b1',\n",
       " '4a262038-b6ee-4afb-8fa3-cc6fb0c82728',\n",
       " 'b5e5232b-6077-40f6-a599-53ab4ea72d0c',\n",
       " '8cdedebe-3251-42f0-8a6e-1b0036c10f17',\n",
       " '67d9d6e0-0ca6-4fc9-ae4b-ec3da57ad6dd',\n",
       " '4b066337-01f5-4d17-9e05-c0c41372ead5',\n",
       " '7ae10b9a-6c45-4754-8c86-b3968f121d04',\n",
       " '16da6f8a-bbf5-41a1-9f3b-d7d8923f5f30',\n",
       " 'c126704b-1ce6-4f0e-8121-a40619e62a3b',\n",
       " '186c83f5-51ce-4bca-81c7-15925cf39f58',\n",
       " '53a7a76b-c804-451f-bb17-f87a16fff734',\n",
       " '908757ed-d4fa-40e2-a96b-9f4935dd303b',\n",
       " '37eaa961-e1a2-4c8f-aeee-ed021b8c6f15',\n",
       " 'e0a27270-24c1-405f-8d6e-ef1b902d1a45',\n",
       " '9e4e9728-7e4f-4f41-b09a-9e99dec4f745',\n",
       " 'a332bf58-2140-4a4d-9936-3740449f2259',\n",
       " 'd3d07608-4c47-445e-bb06-92bd497764fe',\n",
       " '437802ab-dd1a-454f-b38c-46ebd862a014',\n",
       " 'a6246575-f253-4723-ae14-131bf5999419',\n",
       " '1f54ec5f-1c32-48d0-a4dc-0cd0a9fdcdcc']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector_store.add_documents(merged_documents)\n",
    "# Excute only once this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '48f84e1bdca7ae95acb10169474c3135'}, page_content='Generative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c580c8bf0896b0e7bd6f302dd576638b'}, page_content='I.A Pretraining Large Language Model\\nA. Pretraining a Large Language Model\\nIntroduction\\nCross entropy loss\\nTokenization\\nEvaluation\\nData preprocessing\\nScaling laws\\nTraining process\\nCost and optimization\\nPre training phase\\n‹#›\\nII. Transformers\\n‹#›\\nII.B. Transformers Architecture\\n‹#›\\nIII. Retrieval Augmented Generation\\nBasic Architecture\\nInformation retrieval \\nVectorstore & Search optimization\\nRAG Techniques\\nEvaluation\\nMultimodal RAG\\nSOTA RAG architectures\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ed4deadcc12eabded89280e28df5354a'}, page_content='I.A.1 Introduction\\nLanguage modelling\\nLanguage Models: probability distribution over a sequence of words p(x1, … xn)\\nP(Transformers, are, encoder, decoder, models) = 0.01\\nP(Transformers, are, are, encoder, decoder, models) = 0.0001  \\tSyntactic knowledge\\nP(Transformers, are, decoder, models) = 0.001 \\tSemantic knowledge\\nAutoregressive language models:\\nThe chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\\nP(Transformers, are, encoder, decoder, models) = P(Transformers)\\n     . P(Transformers are | Transformers)\\n     … \\n     . P(models | Transformers, are, encoder, decoder)\\n‹#›\\nII. Transformers\\nA. Before Transformers \\nN grams\\nEmbeddings\\nRNN \\nLSTM\\nB. Transformers \\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›\\nII.B. Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›\\nIII. Introduction\\nLLM vs RAG\\nRAG\\nLLM\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3ef74a05a5e598bb1f524bbc973f740d'}, page_content='I.A.1 Introduction\\nLanguage modelling\\nThe goal is to generate token by token \\nThe steps for generation:\\nTokenize \\nFeed the model with the token \\nPredict the probability of each possible token \\nSample from the likelihood\\t\\t\\t\\nDetokenize\\t\\nDecoding\\n8832\\nModel\\n9140           388     527    24592\\nTransformers are encoder \\n‹#›\\nPolo Club, Transformer Explainer [Blog]\\nII.A. Before Transformers \\nA. Before Transformers \\nN grams\\nEmbeddings\\nRNN \\nLSTM\\n‹#›\\nThe Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \\nII.B Introduction\\nBahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\\n‹#›\\nIII. Introduction\\nRAG Definition\\nDefinition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\\nBenefits:\\n\\t•\\tProduces more informed and factual responses.\\n\\t•\\tCan handle queries about recent events not present in the training data.\\n\\t•\\tReduces hallucinations common in language models.\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c154e5863f9d11af3b016fdd5abfcf8d'}, page_content='I.A.1 Introduction\\nHow the model works ? \\nThe general model pipeline is as follows:\\nFeed word embedding for previous (context) \\nwords into a network\\nGet vector representation of context from \\nthe network\\nFrom this vector representation, predict a\\n probability distribution for the next token.\\nLena Voita, Language Modeling [Blog] \\n‹#›\\nII.A. Before Transformers \\nOur goal today\\nInput: “Transformers are encoder decoder”\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model \\nModel\\nPredict: “models”\\n‹#›\\nII.B Introduction\\n‹#›\\nIII.1. Basic Architecture\\nRAG Architecture\\nStep 1: Document ingestion\\nStep 2: Contextualized answering\\n‹#›\\nConstruire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO '), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f91104bc670b3c601cf75379a684f4a1'}, page_content='I.A.2 Cross Entropy Loss\\nCross entropy loss\\nThe general model pipeline is as follows:\\nFeed word embedding for previous (context) words into a network\\nGet vector representation of context from the network\\nFrom this vector representation, predict a probability distribution for the next token.\\nMaximizing the likelihood is equivalent to minimizing the cross entropy loss:\\nLena Voita, Language Modeling [Blog] \\n‹#›\\nII.A.1 N Grams\\nThe chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\\nN Grams\\nInput text: \\nTo Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\\nLena Voita, Language Modeling [Blog] \\nN-gram generator \\n‹#›\\nII.B Introduction\\n‹#›\\nIII.1. Basic Architecture\\nRAG Architecture\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c0f28a89fc370c59bde35ffe6dc02a24'}, page_content='I.A.3 Tokenization\\nTokenization\\nHow to split ? \\nWord ? \\nLetter ?\\nHow to split and get token ? \\nByte-Pair Encoding (BPE) process:\\n1. Use a big corpus of text\\n2. Consider first one token per character\\n3. Merge commons pairs \\n4. Stop when a you cannot merge or the Vocab size is reached\\nThis GIF is generated from GPT o1 using the following prompt\\nFrom the following sentence: Transformers are encoder decoder models\\nApply the following steps: \\n- Create a manim code to display this sentence where each character has a different color \\n- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \\n- Change the colors of new pair\\n- Continue until all commons pair are made \\n- Update at each step the manim code \\n- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\\n- Edit the code at the final stage to change color if two adjacent different pair have same color\\n‹#›\\nII.A.2 Embeddings\\nOur goal today\\nInput: “Transformers are encoder decoder”\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model \\nModel\\nPredict: “models”\\n‹#›\\nII.B. Introduction\\n‹#›\\nIII.1. Basic Architecture\\nRAG Architecture\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5a09accaaf6ffd9dafc4be3ea7569469'}, page_content='I.A.3 Tokenization\\nTokenization\\nByte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \\nPretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\\nQ. What is the problem with numbers as tokens ?\\n‹#›\\nII.A.2 Embeddings\\nFrom One-hot encoding to Word Embedding \\nOne-hot encoding \\nWord Embedding \\nIndex Mot 0 a 1 the 2 he … … 1280 transformers 1281 embedding 1282 partial … … 34567 tunis 34568 dolphin\\nOne-hot encoding\\nDim = |Vocab Size| \\nSemantic representation\\nDim = |Chosen embedding size| \\n0\\n0\\n0\\n:\\n1\\n0\\n0\\n:\\n0\\n0\\n-0.81\\n:\\n:\\n:\\n 4.56\\n:\\n:\\n:\\n-4.35\\n2.21\\nEmbedding model\\n“transformers”\\nIndex: 1280\\n‹#›\\nII.B. Introduction\\nGPT 3\\nEach word is generated one by one\\nOnly the decoder part is used\\n‹#›\\nIII.1. Basic Architecture\\nHow to ?\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e211755c09e88bedfc9e82de0253b713'}, page_content=\"I.A.4 Evaluation\\nEvaluation \\nInstead of cross-entropy, it is more common to report its transformation called perplexity:\\nA better model has higher log-likelihood and lower perplexity.\\nPerplexity = 10 ≃ The model hesitates between 10 tokens\\nTo better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\\nthe best perplexity is 1:\\x0bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\\nthe worst perplexity is |V|:\\x0bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\\nQ. Prove that the worst perplexity is |V|\\nLena Voita, Language Modeling [Blog] \\n‹#›\\nII.A.2 Embeddings\\nWord Embedding (Word2Vec, GloVe, BERT, ELMo)\\nRepresent each word as a vector of numbers\\nConvert a discrete representation to continuous, allowing:\\nMore ‘fine-grained’ representations of words\\nUseful computations such as cosine / euclidean distances\\nVisualization and mapping of words\\n‹#›\\nTomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \\nII.B. Introduction\\nTranslation model (FR -> EN example)\\nThe sentence to translate given to the encoder\\nEach generated word added to the decoder \\n‹#›\\nJay Allamar,  2019, The Illustrated Transformer\\nIII.2. Information retrieval \\nTF-IDF\\nGiven a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\\nBM 25\\nf(qi,D) is the number of times that the keyword qi occurs in the document D, \\n|D| is the length of the document D in words\\navgdl is the average document length in the text collection from which documents are drawn. \\nK1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\\nN is the total number of documents in the collection, and \\nn(qi) s the number of documents containing qi\\nText Search using TF-IDF and Elasticsearch\\n‹#›\"), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '73d3e4fa65984aaac9c8fd2961e119fd'}, page_content='I.A.4 Evaluation\\nEvaluation\\nHugging Face LLM Leaderboard\\nEvaluation Datasets\\nPerplexity depends on vocabulary size, ie tokenization method: not used anymore\\nWe now use evaluation Datasets\\nIFEval\\nBBH\\nMMLU-Pro\\nMath\\n…\\nDifferent fields (medical, math, physics, …)\\ncovered in the Dataset to provide diversity\\nHugging Face, Open LLM Leaderboard \\n‹#›\\nII.A.3 RNN\\nOur goal today\\nInput: “Transformers are encoder decoder”\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model \\nModel\\nPredict: “models”\\n‹#›\\nII.B Introduction\\nLot of new knowledges in this paper:\\nNo more RNN, only attention \\nMLP layers and Attention\\nPositional encodings\\nResNet structure\\nParallelism with Multi Head Attention\\n‹#›\\nIII.2. Information retrieval \\nCosine Similarity\\nEuclidean distance\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '07b5b3260af7df6f299392248ab81635'}, page_content='I.A.4 Evaluation\\nEvaluation\\nEvaluation process: \\nGet the likelihood of each answer\\nAsk the model to answer A) B) C) D)\\nQ. If the model is trained of the whole internet, how could it be contaminated? \\nLena Voita, Language Modeling [Blog] \\nBIG-Bench Hard [Github]\\n‹#›\\nII.A.3 RNN\\nHow to give a sentence to a model ? \\nSemantic representation\\nDim = |Chosen embedding size| = 100\\n1\\n2\\n-0.81\\n 4.56\\n:\\n-4.35\\n2.21\\nEmbedding model\\n2 ≠1\\nD\\n“transformers”\\n⛔\\n-0.81\\n 4.56\\n:\\n-4.35\\n2.21\\n-0.81\\n 4.56\\n:\\n-4.35\\n2.21\\nD\\nD\\n-0.02\\n 2.36\\n:\\n-1.12\\n3.13\\nEmbedding model\\nD\\n“are”\\n‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›\\nIII.2. Information retrieval \\nMaximal Marginal Relevance\\nThe goal of this metric is to retrieve dissimilar documents and increase diversity\\nD is the set of all candidate documents, R is the set of already selected documents, q is the query\\nSim1 is the similarity function between a document and the query\\nSim2 is the similarity function between two documents.  \\ndi and  dj are documents in D and R respectively\\nThe parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\\nThe Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c258e46a21e403d9e4f3eaa9ac9f22e5'}, page_content='I.A.5 Data Preprocessing\\nPreprocessing the Data\\n• Idea: use all of the clean internet\\n• Note: internet is dirty & not representative of what we want. \\nPractice:\\n1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\\n2. Text extraction from HTML (challenges: math, boilerplate)\\n3. Filter undesirable content (e.g. NSFW, harmful content, PII)\\n4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\\n5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\\n6. Model based filtering. Predict if page could be references by Wikipedia.\\n7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\\nlaws to get high downstream performance.\\nAt the end of training, overfit the model on very quality data\\nHugging Face, LLM Training Dataset \\nHTML page example\\n‹#›\\nII.A.3 RNN\\nSeq2seq model\\n‹#›\\nII.B.1 Self Attention Mechanism\\nTransformers\\nare\\ndecoder\\nencoder\\nAttention mechanism\\n‹#›\\nIII.2. Information retrieval \\nSparse vs Dense retrieval \\nSparse embedding (lots of 0)\\nSparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \\nDense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \\nHybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\\nQ. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \\nQ. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \\nQ. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7377eac9f497db5503cff80d203ac94e'}, page_content='I.A.6 Scaling Laws\\nScaling laws\\nMore ressources, more data and bigger models -> better models\\n‹#›\\nJared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \\nII.A.3 RNN\\nRecurrent Neural Networks (Sequential Model)\\nAdvantages:\\nCan learn from context of previous word\\nSelf supervised learning model\\nProblems:\\nSequential model \\nVery short term memory \\n‹#›\\nII.B.1 Self Attention Mechanism\\n3 components: \\nQuery: What am I looking for ? \\nKey: What do I have ?\\nValue: What do I reveal to others ?\\nAttention mechanism\\n‹#›\\nIII.2. Information retrieval \\nSparse Lexical and Expansion (SPLADE)\\nVector database are super efficient compare to Splade at the moment\\nWith sparse methods, you cannot get synonyms from a words.\\nSPLADE:\\nUse Bert to get similar words like synonyms\\nProvide these synonyms to a sparse methods \\n‹#›\\nSPLADE for Sparse Vector Search Explained\\nFormal & Al, 2021, SPLADE V2'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4edca6e6a813ce76686e4f90e2143a4e'}, page_content='I.A.7 Training Process\\nTraining process\\nSteps \\nFind scaling recipes (example: learning rate decrease if the size of the model increase)\\nTune hyper parameters on small models of differents size\\nChoose the best models among the smallest ones\\nTrain the biggest model with the \\nQ. Should I use Transformers or LSTM ? \\n‹#›\\nStanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\n‹#›\\nII.B.1 Self Attention Mechanism\\n   are\\nTransformers\\ndecoder\\nencoder\\nQuery: What am I looking for ? \\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nWQ\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.43\\n-3.2\\n..\\n..\\n3.32\\n2.11\\n-4.22\\n..\\n..\\n1.12\\n3.11\\n-4.22\\n..\\n..\\n4.98\\nEmbedding\\n3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\\nE2\\nE3\\nE1\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\nQuery\\t\\t\\nQ1\\nQ2\\nQ3\\nQ4\\nAm I a superstar ?\\nDo I mean Allocation de Retour à l’Emploi ?\\n…\\nAre we talking about TV ? \\n‹#›\\nIII.2. Information retrieval \\nDeep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\\nDense retriever\\nProgressive Data Augmentation strategy for training sampling very difficult negatives\\n‹#›\\nLin & Al, 2023, How to Train Your DRAGON\\nYasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd202900ea3196604cdc95357a4e10d2b'}, page_content='I.A.8 Cost & Optimizations\\nOptimal model and data size\\nDisplay all the models with same amount of compute (left figure)\\nSelect the best model for each compute in terms of training loss (middle & right figure)\\nExtrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\\n‹#›\\nJordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\nEach word is given sequentially (xt)\\nAn intern memory is updated after each word  (ht)\\nA context is provided with this memory\\nBriefly describe the architecture of a RNN [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\nWK\\n1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\\nKey: What do I have ? \\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\n-3.11\\n2.422\\n 7.93\\n2.11\\n-4.22\\n5.93\\nI am a noun, starting the sentence\\nK1\\nE1\\n2.43\\n-3.2\\n3.32\\nI am a verb\\n2.11\\n-3.22\\n5.93\\nE2\\nK2\\n…\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\nK3\\nE3\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\nMight be a TV object  of a model\\n‹#›\\nK4\\nE4\\nIII.2. Information retrieval \\nBest retrieval methods\\nLeaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac0d95fa1ee93936c7fe59e8b04191f9'}, page_content='I.A.8 Cost & Optimizations\\nHow much it costs ?\\nLLAMA 3 400B cost approx. $80m\\nCarbon emitted approx. 2K tickets Tunis - New York\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“The”\\nOptimizing the loss w.r.t weights: \\n“transformers”\\n“are”\\n“encoder”\\n“Models” ?\\n“decoder”\\n‹#›\\nD. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \\nStatQuest with Josh Starmer [Youtube]\\nII.B.1 Self Attention Mechanism\\n   are\\nTransformers\\ndecoder\\nencoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nEmbedding\\nE4\\nE2\\nE3\\nE1\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n 5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nQuery\\t\\t\\nQ4\\nQ1\\nQ2\\nQ3\\nWQ\\nAre we talking about TV ? \\n-3.11\\n2.422\\n 7.93\\nAm I a superstar ?\\nDo I mean Allocation de Retour à l’Emploi ?\\n…\\n2.11\\n-4.22\\n5.93\\n3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\\nK1\\nE1\\n…\\n…\\nI am a noun, starting the sentence\\nYou should be a verb because I am a noun\\nNo we are not because I am a Transformer\\n2.43\\n-3.2\\n3.32\\n2.11\\n-3.22\\n5.93\\nWK\\n…\\n…\\n…\\n…\\n…\\nE2\\nK2\\n1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\n…\\n…\\n…\\n…\\n…\\nK3\\nE3\\n…\\n…\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\n…\\n…\\nMight be a TV object  of a model\\n‹#›\\nK4\\nE4\\nIII.3. Vectorstore & Search optimization\\nVector Database \\nDefinition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \\nThese embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\\n‹#›\\nVector DB Comparison'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '8c07ba4c6ce989e2e94fd2b175952e1a'}, page_content='I.B Fine tuning Large Language Model\\nB. Fine tuning a Large Language Model\\nSupervised Fine Tuning\\nRLHF\\nReward model\\nPPO & DPO\\nEvaluation & Challenges\\nPost training phase\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“The”\\nFeed forward + Softmax model\\n“transformers”\\n“are”\\n“encoder”\\n?\\n“decoder”\\n“decoder”\\n‹#›\\nII.B.1 Self Attention Mechanism\\n   are\\nTransformers\\ndecoder\\nencoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nEmbedding\\nE4\\nE2\\nE3\\nE1\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n 5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nQuery\\t\\t\\nQ4\\nQ1\\nQ2\\nQ3\\nWQ\\n-3.11\\n2.422\\n 7.93\\n2.11\\n-4.22\\n5.93\\n3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\\n4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\\nK1\\nE1\\n2.43\\n-3.2\\n3.32\\n2.11\\n-3.22\\n5.93\\nWK\\nE2\\nK2\\n1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\nK3\\nE3\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\n‹#›\\nK4\\nE4\\nIII.3. Vectorstore & Search optimization\\nEfficient similarity search\\nScalable Nearest Neighbors (ScaNN) - Google\\nFacebook AI Similarity Search (FAISS)\\nHierarchical Navigable Small Worlds (HNSW)\\nDefinition:\\nScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\\nAnnouncing ScaNN: Efficient Vector Similarity Search\\n‹#›\\nHierarchical Navigable Small Worlds (HNSW) [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '73e78d6d69b2a46b3a7d162c92074f97'}, page_content='I.B.A Supervised Fine Tuning\\nHow to get a user assistant ? \\n“GPT-3 models aren’t trained to follow user instructions. \\nOpen AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\\n‹#›\\nOpen AI, 2022, Aligning language models to follow instructions [Blog] \\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“The”\\n“transformers”\\n“are”\\n“encoder”\\n“Models” ?\\n“decoder”\\n‹#›\\nStatQuest with Josh Starmer Youtube\\nII.B.1 Self Attention Mechanism\\n   are\\nTransformers\\ndecoder\\nencoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nEmbedding\\nE4\\nE3\\nE2\\nE1\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n 5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nQuery\\t\\t\\nQ4\\nQ1\\nQ2\\nQ3\\nWQ\\n-3.11\\n2.422\\n 7.93\\n2.11\\n-4.22\\n5.93\\n3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\\n4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\\nK1\\nE1\\n2.43\\n-3.2\\n3.32\\n2.11\\n-3.22\\n5.93\\nWK\\nE2\\nK2\\n1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\nK3\\nE3\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\n‹#›\\nK4\\nE4\\nIII.3. Vectorstore & Search optimization\\nReciprocal Rank Fusion (RRF)\\nDefinition:\\nRRF allows to merge results of different retrievers\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac2f2ec1ea10b394bdcde84f69281f3a'}, page_content='I.B.A Supervised Fine Tuning\\nHow to get a user assistant ? \\nPretrained Large \\nLanguage\\nModel\\nChatGPT\\nConversational Agent\\nPost training: Alignement\\n‹#›\\nII.A.3 RNN\\nRNN Types of architectures\\n“The”\\n“transformers”\\n“are”\\n“encoder”\\nQ. Which one fit our use case ? \\n“Models” ?\\n“decoder”\\nPraveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\\nStatQuest with Josh Starmer Youtube\\n‹#›\\nII.B.1 Self Attention Mechanism\\nThe sum of each column is 1 \\nThis step allows numerical stability \\n1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\\n4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\\n4/128 70/128 0 85/128 -4/128 -10 0 0 2/128 0 0 0 3/128 -3/128 4/128 5/128\\nQ1\\nQ2\\nQ3\\nQ4\\nQ1\\nQ2\\nQ3\\nQ4\\nSoftmax\\nK1\\nK1\\nQ1.K1 Q2.K1 Q3.K1 Q4.K1 Q1.K2 Q2.K2 Q3.K2 Q4.K2 Q1.K3 Q2.K3 Q3.K3 Q4.K3 Q1.K4 Q2.K4 Q3.K4 Q4.K4\\nQ1.K1/√dk Q2.K1/√dk Q3.K1/√dk Q4.K1/√dk Q1.K2/√dk Q2.K2/√dk Q3.K2/√dk Q4.K2/√dk Q1.K3/√dk Q2.K3/√dk Q3.K3/√dk Q4.K3/√dk Q1.K4/√dk Q2.K4/√dk Q3.K4/√dk Q4.K4/√dk\\nK2\\nK2\\nK3\\nK3\\n‹#›\\nK4\\nK4\\nIII.3. Vectorstore & Search optimization\\nReranker\\nA reranker ranks retrieved documents after a first similarity search\\nReranker type:\\nCross-Encoders\\nNeural Rerankers\\nBenefits of Using a Reranker:\\nIncreased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\\nBetter Contextual Understanding: Helps the system understand subtle nuances in the query.\\nChallenges:\\nComputational Overhead: Additional processing can increase response time.\\nResource Intensive: Advanced models require significant computational resources.\\nBi Encoder vs Cross Encoder\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '609e0c114841e747af75094186fcd7b8'}, page_content='I.B.A Supervised Fine Tuning\\nHow to get a User Assistant from a Language Model ? \\n‹#›\\nC. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \\nII.A.4 LSTM\\nOur goal today\\nInput: “Transformers are encoder decoder”\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model \\nModel\\nPredict: “models”\\n‹#›\\nII.B.1 Masking attention Mechanism\\nDefinition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\\nIdea: A later word cannot answer question to a previous word because it is unknown at inference\\nThis step allows numerical stability \\n4 70 0 85 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5\\n4/128 70/128 0 85/128 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5/128\\n1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\\n‹#›\\nIII.4. RAG Techniques\\nQuery augmentation\\nDefinition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \\nBy augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\\nHyDE: Generate a fake answer from a query to improve information retrieval\\n‹#›\\nLuyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '50b34c690dff813c7b1734d873c4e028'}, page_content='I.B.A Supervised Fine Tuning\\nFamous LLM follow user instructions with moderation\\nIdea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nBoth long and short term memory are provided\\n“transformers”\\n“are”\\n“encoder”\\n“transformers”\\n“are”\\n“encoder”\\nRNN architecture\\nLSTM architecture\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\n   are\\nTransformers\\ndecoder\\nencoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nEmbedding\\nE4\\nE2\\nE3\\nE1\\nWv\\n1x V1 0.97 x V1 0.33 x V2 0.85 x V1 0 x V2 0.03 x V2 0.33 x V2 0.02 x V2 0 x V3 0 x V3 0.33 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\\n2.11\\n-4.22\\n5.93\\n-3.11\\n2.422\\n 7.93\\n1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\\nV1\\nE1\\nEmbedding\\nVALUES\\t\\t\\n2.43\\n-3.2\\n3.32\\n2.11\\n-3.22\\n5.93\\nE2\\nV2\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\nV3\\nE3\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\n‹#›\\nV4\\nE4\\nIII.4. RAG Techniques\\nQuery rephrasing\\nQuery rephrasing can be used to rephrase the query from the conversation history \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3949fea7038d535d9ef77269097721db'}, page_content='I.B.A Supervised Fine Tuning\\nSupervised Fine Tuning \\nHow can we get the post training data ? \\nProblem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\\nSolution: Use LLM to scale Data Collection at low cost\\n‹#›\\nAlpaca: A Strong, Replicable Instruction-Following Model [Blog] \\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“The”\\n“transformers”\\nCt-1\\n“are”\\nht-1\\n“encoder”\\n“decoder”\\n“models”\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nᐩ\\nᐩ\\nᐩ\\nᐩ\\nAttention pattern\\nz1\\nz3\\nz4\\nz2\\n‹#›\\nIII.4. RAG Techniques\\nLost In the Middle\\nRetrieved context provided at the beginning or the end of the prompt have more impact on the answer\\n‹#›\\nF. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\\nLangChain, Long Context Reorder [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '07c77e5dc87590cb20bc38dcb4ba9804'}, page_content='I.B.A Supervised Fine Tuning\\nSupervised Fine Tuning \\nHow much data do we need ?\\nProblem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\\nSolution: A few data is required for SFT \\n‹#›\\nZhou & Al 2023, LIMA: Less Is More for Alignment\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nCell state to propagate long memory\\nGate defined by the sigmoid function\\n0 = don’t pass information \\n1 = let everything pass through\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.2 Self Attention Mechanism\\nThis step allows numerical stability \\n4 70 0 85 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5\\n4/128 70/128 0 85/128 -∞ -10 0 0 -∞ -∞ 0 0 -∞ -∞ -∞ 5/128\\n1 0.97 0.33 0.85 0 0.03 0.33 0.02 0 0 0.33 0.02 0 0 0 0.11\\nQ1\\nQ2\\nQ3\\nQ4\\nQ1\\nQ2\\nQ3\\nQ4\\nSoftmax\\nK1\\nK1\\nQ1.K1 Q2.K1 Q3.K1 Q4.K1 Q1.K2 Q2.K2 Q3.K2 Q4.K2 Q1.K3 Q2.K3 Q3.K3 Q4.K3 Q1.K4 Q2.K4 Q3.K4 Q4.K4\\nQ1.K1/√dk Q2.K1/√dk Q3.K1/√dk Q4.K1/√dk Q1.K2/√dk Q2.K2/√dk Q3.K2/√dk Q4.K2/√dk Q1.K3/√dk Q2.K3/√dk Q3.K3/√dk Q4.K3/√dk Q1.K4/√dk Q2.K4/√dk Q3.K4/√dk Q4.K4/√dk\\nK2\\nK2\\nK3\\nK3\\n‹#›\\nK4\\nK4\\nIII.4. RAG Techniques\\nPrompt Engineering\\nWrite clear instructions\\nProvide reference text\\nSplit complex tasks into simpler subtasks\\nGive the model time to \"think\"\\nUse external tools (RAG)\\nTactic:\\nAsk the model to adopt a persona\\nUse delimiters to clearly indicate distinct parts of the input\\nSpecify the steps required to complete a task\\nProvide examples\\nSpecify the desired length of the output\\n‹#›\\nOpen AI, Prompt engineering'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '106077e33bbd1746a880ca3d68974cb4'}, page_content='I.B.A Supervised Fine Tuning\\nLoss\\nHow Supervised Fine Tuning Works ? \\nChild \\nSome 👌 \\nSame process than the language model training\\n3892\\nModel\\n9140 820 19  354 3672 34 347 321  2903 224 9832\\nExpl ain the moon lan ding to a 6 years old\\n‹#›\\nOpen AI, 2022, Aligning language models to follow instructions [Blog] \\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“decoder”\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.2 Cross attention\\n   are\\nTransformers\\ndecoder\\nencoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nEmbedding\\nE4\\nE2\\nE3\\nE1\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n 5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nQuery\\t\\t\\nQ4\\nQ1\\nQ2\\nQ3\\nLes\\n-3.11\\n2.422\\n 7.93\\n2.11\\n-4.22\\n5.93\\n70 70 0 85 -4 35 0 0 2 0 30 0 3 -3 4 18 … … … …\\nK1\\nE1\\nTransformers\\n2.43\\n-3.2\\n3.32\\n2.11\\n-3.22\\n5.93\\nFrench to english translation example \\nNo masking\\nE2\\nK2\\nsont\\n2.11\\n-1.2\\n5.93\\n2.11\\n-4.22\\n1.12\\nK3\\nE3\\ndes\\n-21\\n42.21.2\\n1.23\\n3.11\\n-4.22\\n4.98\\n‹#›\\nK4\\nE4\\n…\\nIII.4. RAG Techniques\\nDocument Loader\\nLoad any type of document (PDF, PPT(x), DOC(x), XLS(x)\\nUnstructured: https://unstructured.io/ \\nLLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3f3edd5deab91e90566409af84a75f8e'}, page_content='I.B.2 RLHF\\nReinforcement Learning from Human Feedback - RLHF\\nSFT Limitations: \\nBehavior cloning\\nHuman abilities to answer perfectly to a given question\\nHallucination if answer from human not in training data\\nData collection cost\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“decoder”\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.2 Cross attention\\n‹#›\\nIII.4. RAG Techniques\\nContext \\nDefinition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\\nA larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\\nA smaller context size limits the amount of information the model can utilize from the input text.\\n‹#›\\nVariable Sequence Length Training for Long-Context Large Language Models'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '001df5977c422ed6f539c363ef699e86'}, page_content='I.B.2 RLHF\\nReinforcement Learning from Human Feedback - RLHF\\nIdea:\\nFrom a question, generate multiples answers\\nAsk a human to classify answers\\nTrain a reward model to learn these preferences\\nReward model: classifier that is trained to classify preferences from possibles answers \\nClassifier \\nModel\\n‹#›\\nOpen AI, 2022, Aligning language models to follow instructions [Blog] \\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“decoder”\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\nGPT 3 dimension for one attention head\\nQuery: What am I looking for ? \\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nKey: What do I have ? \\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nEmbedding (Embedding Dimension, N words) (12 288, 50 257) 49 152 params Key (Key size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Query (Query size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Value up (Value size, Embedding Dimension) (128, 12 288) 1 572 864 params per head Value down (Embedding Dimension, Value size) (12 288, 128) 1 572 864 params per head\\n‹#›\\nIII.4. RAG Techniques\\nChunking\\nTo avoid context limitations, we can do document chunking:\\nChunk by document if the document is small \\nChunk by title or header if the document is big \\n‹#›\\nAnnouncing ScaNN: Efficient Vector Similarity Search'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e4a961bfbec4d7b23ff8b88a15b85a55'}, page_content='I.B.3 Reward Model\\nReward model\\nAlso transformer based LM\\nVariation in sizes used (relative to policy)\\nOutputs scalar from input text\\nClassifier \\nModel\\n‹#›\\nOpen AI, 2022, Aligning language models to follow instructions [Blog] \\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“decoder”\\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\nGPT 3 dimension for all attention heads: 603 979 776 parameters \\nQuery: What am I looking for ? \\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nKey: What do I have ? \\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nEmbedding (Embedding Dimension, N words) (12 288, 4) 49 152 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads 150 994 944 params\\n‹#›\\nIII.4. RAG Techniques\\nRAG Frameworks in Python\\n‹#›\\nLangchain \\nLlamaIndex '), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '106d2684a898e6e255b280b7bc1ebdc3'}, page_content='I.B.4 PPO & DPO\\nTraining RL model\\nAlso transformer based LM\\nVariation in sizes used (relative to policy)\\nOutputs scalar from input text\\nPrevent over optimization \\n‹#›\\nLambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nWhat about Vanishing / Exploding Gradients ? \\nThe additive update function for the cell state gives a derivative that is much more ‘well behaved’\\nThe gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\\nTo get details on LSTM derivative, check out this blog post \\nColah, Understanding LSTM Networks [Blog] \\n‹#›\\nII.B.1 Self Attention Mechanism\\nValue Matrix (12 288, 12 288) decomposition≈\\nIdea: \\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the \\nForce the Value matrix to be low rank\\n128\\n12 288\\n0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\\n12 288\\n1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\\n1 0.97 0.33 0.85 0 0.03 0.33 0.02\\n=\\n128\\n12 288\\n12 288\\n‹#›\\nIII.4. RAG Techniques\\nCloud services\\nCloud Services provide:\\nA Secure environment \\nEnough compute to train big models\\nProduct as a Service (PaaS)\\nLLMs APIs\\nVector Store management\\nEfficient Retrieval\\nMonitoring tools\\n…\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '069bc37ee7295758412cadf554d65e1a'}, page_content='I.B.4 PPO & DPO\\nDPO\\nPPO is much more complex (clipping, rollouts, outer loops) than in theory\\nMaximize the desired output, minimize the other \\n‹#›\\nRafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\\nII.A.4 LSTM\\nOur goal today\\nInput: “Transformers are encoder decoder”\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model \\nModel\\nPredict: “models”\\n‹#›\\nII.B.1 Multi \\nValue Matrix (12 288, 12 288) decomposition\\nIdea: \\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the \\nForce the Value matrix to be low rank\\n128\\n12 288\\n0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\\n12 288\\n1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\\n1 0.97 0.33 0.85 0 0.03 0.33 0.02\\n=\\n128\\n12 288\\n12 288\\n‹#›\\nIII.5. Evaluation\\nRetriever Evaluation: Precision & Recall @ k\\nPrecision @k\\nHow many  retrieved documents are relevant ?\\nRecall @k\\nHow many  relevant documents are retrieved ?\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 30, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6d981d909b94ac8b6769fb893b17ae67'}, page_content='I.B.5 Evaluation & Challenges\\nRLHF gains\\nNisan Stiennon & Al, 2020, Learning to summarize from human feedback\\nDubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\\n‹#›\\nII.B.1 Self Attention Mechanism\\nValue Matrix (12 288, 12 288) decomposition\\nIdea: \\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the \\nForce the Value matrix to be low rank\\n128\\n12 288\\n0.8 0.2 0.3 1.7 -1.3 3.2 3.2 0.32\\n12 288\\n1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\\n1 0.97 0.33 0.85 0 0.03 0.33 0.02\\n=\\n .\\n128\\n12 288\\n12 288\\nWv\\n‹#›\\nIII.5. Evaluation\\nRetriever Evaluation : NDCG\\nNDCG can take values from 0 to 1. \\nNDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \\nNDCG equals 0 when there are no relevant objects in top-K.\\nNDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 31, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7bd9a098819b62b131c8adb9131751a7'}, page_content='I.B.5 Evaluation & Challenges\\nRLHF challenges\\nAnswer preference is not trivial\\nRLHF increases answer size\\nHumans do not agree (agree with themselves only 66% of the time)\\nHuman have lot of variance, model have no variance \\nAsk LLM preferences instead of human preferences\\nStanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\\n‹#›\\nSinghal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\\nII.B.1 Self Attention Mechanism\\nValue Matrix computation optimization\\n0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\\n12 288\\nE1\\n0.32  \\t3.02 \\t…\\t-0.33\\n0.65 0.3 0.23 0.4\\n1.2 0.23 -2.3 3.23 0.32 -4.31 2.11 0.12 2.33 -2.32 0.23 -0.23 -1.76 0.21 0.92 -0.12\\n .\\n =\\n =    V1\\n12 288\\n12 288\\nWv\\n‹#›\\nIII.5. Evaluation\\nAnswer Evaluation: LLM As a judge\\nAsk an LLM to evaluate answer quality: \\nDoes my answer answer to the question ? \\nDoes my answer used information from the context ? \\nDoes my answer give enough facts ?\\n…\\nBLEU, ROUGE, Perplexity are not ideal for RAG use case. \\nEvaluating answers in RAG is not easy\\n‹#›\\nRAGAS'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 32, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2e8db4b0ac1f99576f67f58bcc4fd8cc'}, page_content='I.B.5 Evaluation & Challenges\\nEvaluation\\nHow to evaluate a model like Chat GPT ? \\nDifferent methods (DPO, PPO, SFT) can be compared\\nModels are not calibrated\\nA large diversity of evaluation to cover\\n‹#›\\nChatbot Arena, Open LM\\nII.B.1 Self Attention Mechanism\\nMethod 1 WV (12 288, 12 288)\\n0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0.1 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\\nNumber of computations:\\nN words x 12 288 multiplications\\nN words x 12 288 additions\\n0.62 x V1 0.1 x V2 0 x V3 0 x V4\\n0.1 x v21     0.1 x v22\\t        …\\t0.1 x v2, 12288\\n0.32 x 0.62      3.02 x 0.62\\t        …\\t-0.33 x 0.62\\n+\\n …\\n =\\n12 288\\n12 288\\n‹#›\\nIII.6. Multimodal RAG\\nMultimodal\\nMultimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\\nExemple: GPT 4o, Gemini 1.5, Qwen2- VL\\n‹#›\\nBerrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 33, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ba3adbe5a56a58502c3b011279a1bf97'}, page_content='I.B.5 Evaluation & Challenges\\nOpenAI o1: “Streaming is dead, long live Chain of Thought”\\nChain of thought (COT)\\nIncrease test time compute\\n“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\\n‹#›\\nOpen AI, 2024, Learning to Reason with LLMs [Blog] \\nII.B.1 Self Attention Mechanism\\nMethod 1 WV (12 288, 128)\\n0.62 x V1 0.97 x V1 0 x V2 0.85 x V1 0.1 x V2 0.001 x V2 0 x V2 0.02 x V2 0 x V3 0 x V3 0 x V3 0.02x V3 0 x V4 0 x V4 0 x V4 0.11 x V4\\nStep 2:\\nNumber of computations:\\nN words x 128 multiplications\\nN words x 128 additions\\nMatrix multiplication between value up matrix and result: \\n128 multiplication + 128 addition for each row\\n12 288 times\\n0.62 x V1 0.1 x V2 0 x V3 0 x V4\\n0.1 x v21     0.1 x v22\\t        …\\t0.1 x v2, 128\\n0.32 x 0.62      3.02 x 0.62\\t        …\\t-0.33 x 0.62\\n+\\n …\\n =\\n128\\n128\\n‹#›\\nIII.6. Multimodal RAG\\nMultimodal: Qwen2- VL\\nOutperform GPT 4o on most of the benchmarks\\nMultimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\\n‹#›\\nBerrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 34, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7c051edb8c46239c37f51d06b6e1498f'}, page_content='I.B.5 Evaluation & Challenges\\nOpenAI o1 vs GPT 4o\\nPrompt: \\nFrom the following sentence: Transformers are encoder decoder models\\nApply the following steps: \\n- Create a manim code to display this sentence where each character has a different color \\n- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \\n- Change the colors of new pair\\n- Continue until all commons pair are made \\n- Update at each step the manim code \\n- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\\n- Edit the code at the final stage to change color if two adjacent different pair have same color\\nOpen AI o1\\nGPT 4o\\n‹#›\\n‹#›\\nIII.6. Multimodal RAG\\nRotary Embedding (ROPE)\\nRotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \\nUnlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\\nProperties:\\nFlexibility of being expand to any sequence lengths\\nDecaying inter-token dependency with increasing relative distances\\nCapability of equipping the linear self-attention with relative position encoding.\\n‹#›\\nSu & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:12:43', 'page_number': 35, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2959da6cb52f5c994b0c257ae2c4de2b'}, page_content='Conclusion\\nConclusion\\n“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\\nBuilding GPT 3 \\nData preprocessing is a very important step to get quality data\\nGPT 3 training consists of two phases (pre and post training)\\nSupervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\\nRLHF helps the model to align with human preferences\\nDPO is the new methods for Alignment, replacing RLHF\\nGeneral knowledges \\nData size and model size depends on compute resources (Scaling Laws, Chinchilla). \\nOpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\\nSebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \\n‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›\\nIII.6. Multimodal RAG\\nMultimodal RAG\\nOption 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\\nOption 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\\nOption 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\\n‹#›\\nLangChain, Multi-Vector Retriever for RAG on tables, text, and images'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 36, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6358a792ba02efd9ff5950303edeccbc'}, page_content='II.B.2 Multi-Head Attention\\nTransformers\\nare\\ndecoder\\nencoder\\nAttention mechanism\\n‹#›\\nIII.6. Multimodal RAG\\nMultimodal RAG\\n‹#›\\nYasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 37, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '943c8089fe28e52d7feb03c826d109e2'}, page_content='II.B.2 Multi-Head attention\\nᐩ\\nᐩ\\nᐩ\\nᐩ\\nN words\\nz1\\nz3\\nz4\\nz2\\nZ\\n |V| (=128)\\nz1\\nz3\\nz4\\nz2\\n‹#›\\nIII.7. SOTA RAG architectures\\nSelf-RAG \\nGenerates multiple possible response segments in parallel, utilizing the retrieved documents as context.\\nThe model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\\nThis selection process ensures that the response is both factually correct and contextually appropriate.\\nAsai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\\n‹#›\\nAhmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 38, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3c01eedfb26bbc8feeda806a32b63a00'}, page_content='II.B.2 Multi-Head attention\\nN words\\nz1\\n…\\nz1\\n…\\nz2\\nConcatenation\\nz10\\n |V| x N heads \\n= 128 x 96\\nz3\\nz96\\nz96\\n‹#›\\nIII.7. SOTA RAG architectures\\nRAPTOR\\nRAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \\nThis hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\\nSarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\\n‹#›\\nAhmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 39, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd9c57f397cf0f142e342868c836c46f3'}, page_content='II.B.2 Multi-Head attention\\nN words\\n| Embeddings Dim|\\nW0\\n…\\nx\\nz1\\nZ\\n| Embeddings Dim|\\nz10\\n |V| x N heads \\n= 128 x 96\\n |V| x N heads \\nN words\\nz96\\n‹#›\\nIII.7. SOTA RAG architectures\\nCorrective RAG (CRAG)\\nAdd a retrieval evaluator based on the quality of retrieved sources\\nIf sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \\n‹#›\\nYan & Al, 2024, Corrective Retrieval Augmented Generation'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 40, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '305a291fdbeb8fb35ecf92477d4e13e0'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\n3\\nStep 3\\nInput sentence: Transformers are encoder decoder\\n|Key Dim| = |Query Dim| = |Value Dim| = 3\\n|Embedding Dim| = 5\\n|N words| = 4\\nZ0\\n4\\n3\\n5\\n3\\nWQ0\\nQ0\\nWK0\\nK0\\nTransformers \\nare  \\nencoder \\ndecoder\\nWV0\\nV0\\n4\\n5\\n4\\nStep 1\\nStep 2\\n‹#›\\nIII.7. SOTA RAG architectures\\nGraphRAG\\nThe LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \\nThis graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \\nAt query time, both of these structures are used to provide materials for the LLM context window when answering a question. \\n‹#›\\nMicrosoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 41, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7376db98e0ebd4ca083005b8cccdc7df'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\nQ0\\nWQ0\\nK0\\nWK0\\nZ0\\nV0\\nWV0\\nStep 4\\n…\\n…\\n…\\nWQ96\\nQ96\\nWK96\\nK96\\nZ96\\nWV96\\nV96\\n‹#›\\nIII. Conclusion\\nConclusion\\nSimple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\\nContext Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\\nMulti-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\\nFusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\\nIntelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\\nQuery Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\\nHierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\\nHypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\\nChoose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\\nSemantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\\nContext Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\\nExplainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\\nRetrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 42, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '65274390c56ca6c82797e47d34babe8b'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\n3\\n5\\n3\\nZ0\\nW0\\n4\\nZ\\n5\\n4\\n…\\n4 x 96\\n4 x 96\\n3\\nZ96\\n4\\nStep 5\\nMatmul\\n‹#›\\nIII. Conclusion\\nConclusion\\nAdaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\\nIterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\\nEnsemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\\nGraph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\\nMultimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\\nRAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\\nSelf RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\\nCorrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\\nFew shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 43, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6e3734ce497700075c099b880be480b9'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\nWhy do Z and X have the same dimension ?\\nX\\n5\\nTransformers \\nare  \\nencoder \\ndecoder\\n4\\n5\\nZ\\n4\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 44, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0c4cfc138c9e69d01d6890dfaa30f5fe'}, page_content='II.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 45, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '910d5288446549c9afe163a2f01239ed'}, page_content='II.B.3. Residual connections & Layer normalization\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 46, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '88c4a6a8db5f9fddc9a9afd7cb41dcfa'}, page_content='II.B.3. Residual connections & Layer normalization\\nResidual connections\\nResidual connections mainly help mitigate the vanishing gradient problem\\nAnother effect of residual connections is that the information stays local in the Transformer layer stack\\nReLU\\ny = max(x, 0)\\n‹#›\\nHe & Al, 2015,  Deep Residual Learning for Image Recognition'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 47, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '94748fc455ec327fd873924ff0b49286'}, page_content='II.B.3. Residual connections & Layer normalization\\n Layer normalization\\n“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \\nEmpirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\\nHinton & Al, 2016, Layer Normalization\\n2020, In-layer normalization techniques for training very deep neural networks [Blog]\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 48, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f068d9a56c787a4884b202d114ea6071'}, page_content='So far …\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 49, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f49efd3e40aa5c5e7a69800d68e710f8'}, page_content='II.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\nOptimization \\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 50, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b9271b2168595a88ae7b428016375208'}, page_content='II.B.4. Feed forward layer\\n“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\\n‘Up \\nprojection’\\n‘Down \\nprojection’\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 51, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6257fdb286815eb315d54f96b31353db'}, page_content='II.B.4. Feed forward layer\\n5\\nX\\nFeed forward layer\\nTransformers \\nare  \\nencoder \\ndecoder\\n4\\nZ\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\n5\\n4\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 52, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f2ea9a25564678bdd306c254497756c7'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\nX\\n5\\nTransformers \\nare  \\nencoder \\ndecoder\\n4\\nResidual connection + Layer Nom\\nFeed forward\\nLayer\\nZ\\n5\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\n4\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 53, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ec2067881742a76c8199b471404a359a'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\nFeed forward network\\nFeed forward network\\nAm I a math architecture ?\\n5\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nAm I a plural noun ?\\n4\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\n…\\nAm I a football star ?\\n…\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 54, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c362e202867145d5ac3437d51f4b7ea1'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\nFeed forward network\\nAm I a math architecture ?\\n5\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nAm I a singular noun ?\\n4\\n…\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\nAm I a TV stuff ?\\n…\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 55, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'feaf1474fdd6401f2ff18a4e4c08bf5c'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer : “Up projection”\\nFeed forward network\\nUp projection dimension = 49 152 x embedding dimension \\n49 152\\n…\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 56, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f210e03aca7e2c35c3f8fa0acc862421'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer : “Down projection”\\nAnother layer\\nDown projection dimension = embedding dim x 49 152\\n12 288\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 57, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd049b6296445ad0c1e7787043f13888f'}, page_content='II.B.4. Feed forward layer\\nGPT 3 dimension per layer\\nEmbedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads 150 994 944 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads 150 994 944 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) 603 979 776 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) 603 979 776 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 58, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '87211810c095debaff115d866195a948'}, page_content='II.B.4. Feed forward layer\\nGPT 3 dimension (96 layers):\\nEmbedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads x 96 layers 14 495 514 624 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) x 96 layers 57 982 058 496 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) x 96 layers 57 982 058 496 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016 params\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 59, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '67042cb3bd07f252a0f6d011615d6d43'}, page_content='So far …\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 60, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1d29b86c5d5251364e9def08624f25ea'}, page_content='II.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 61, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2a1cc903b6011ba40fab6f75a3c6af4c'}, page_content='II.B.5. Softmax Layer\\n“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\\nWe also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \\nIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 62, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0a443406bda861c1b2f20abd454eac53'}, page_content='II.B.5. Softmax Layer\\nThe input is he vector from the last word of the sentence \\nThe output is the probability distribution over all words in the dictionary (50k words for GPT 3-\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 63, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '796e3caf96902ae4ecd0246fb347c07d'}, page_content='II.B.5. Softmax Layer\\nQ. Why don’t we take all the previous representations of the other vectors for the inference ?\\nare\\nFor training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\\nOf course, the last word context is learned with attention\\nencoder\\ndecoder\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 64, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f636722846561ca2fafb20df9526b112'}, page_content='II.B.5 II.B.5. Softmax Layer - Temperature\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 65, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b6978eb9dd7e237469f172cee2ed5f09'}, page_content='II.B.5. Softmax Layer\\nGPT 3 dimension (96 layers):\\n175 181 291 520 trainable parameters\\nEmbedding (Embedding Dimension, N words) (12 288, 50 257) 617 558 016 params Key (Key size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Query (Query size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value up (Value size, Embedding Dimension) (128, 12 288) x 96 heads x 96 layers 14 495 514 624 params Value down (Embedding Dimension, Value size) (12 288, 128) x 96 heads x 96 layers 14 495 514 624 params Up Projection (Neuron Dim, Embedding Dimension) (49 152, 12 288) x 96 layers 57 982 058 496 params Down Projection (Embedding Dimension, Neuron Dim) (12 288, 49 152) x 96 layers 57 982 058 496 params Unembedding (N words, Embedding Dimension) (50 257, 12 288) 617 558 016 params\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 66, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1133ba637feb9d8904f5948972cea3e0'}, page_content='So far …\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 67, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac9f1623364d6398e5554307653894d7'}, page_content='II.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 68, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ccd5f00a6f60bd9a713d43e16a4b4189'}, page_content='II.B.6. Positional Embeddings\\n“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \\nTo this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 69, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '64346c477176cf056df56d3abef50a68'}, page_content='II.B.6. Positional Embeddings\\nComputations are not done sequentially (unlike RNN and LSTM)\\nHow to compare “A B C” and “C A B” ?\\nBeneficial to find a method that satisfy the following points:\\nUnambiguous (each position have its own value)\\nDeterministic \\nAllows to estimate distance between tokens\\nWorks with longer sequence than seen during training\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 70, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c1a9593890e1fc18e67bc177550a3757'}, page_content='II.B.6. Positional Embeddings\\nSame size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\\nDon’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\\nWhere:\\npos is the position {1, …, context length}\\ni is the dimension = {1, …, dmodel }\\ndmodel is the embedding dimension (GPT 3 = 12 288)\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 71, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ac51ab6d52990484ffb9c4d1c4a409c3'}, page_content='II.B.6. Positional Embeddings\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 72, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4da9648bc95b0f002595f0a2e3f391e4'}, page_content='II.B.6. Positional Embeddings\\nProperties\\nThe positional values are unique if at least one function has maximum size of this sequence (context window) 👌\\nThe positional values are not random, created using two equations 👌\\nLooking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\\nSince sin and cosine are periodic functions, the model can generalize for longer sequences 👌\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 73, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '99683a9442622f02e05cb807624be956'}, page_content='II.B.6. Positional Embeddings\\n“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\\n‹#›\\nKazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 74, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '95e605108505460d65f8754ebd3d8c9e'}, page_content='II.B.6. Positional Embeddings\\nFigure: Positional encoding representation\\nEach row represent a positional vector for a given token \\n‹#›\\nKazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 75, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '642f5211980567c82720ba3fa749c8b1'}, page_content='So far …\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-02T12:13:26', 'page_number': 76, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e0a64f16c335f37d2c35d86037cc0399'}, page_content='II. Resources\\nBlogs:\\nThe Illustrated Transformer\\nTransformers Explained Visually (Part 3): Multi-head Attention, deep dive\\nPapers:\\nAttention is All You Need\\nVideos:\\nVisual introduction to Transformers (part 1)\\nTransformers visualized (part 2)\\nHow might LLMs store fact (part 3)\\nResidual Network and skip connections\\nStanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\\n‹#›')]\n"
     ]
    }
   ],
   "source": [
    "print(merged_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Perform a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to train a Large Language Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youssef\\AppData\\Local\\Temp\\ipykernel_14876\\1743924723.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5, \n",
    "    }\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Content:  I.A.7 Training Process\n",
      "Training process\n",
      "Steps \n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "WQ\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Embedding\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "Query\t\t\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Am I a superstar ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "…\n",
      "Are we talking about TV ? \n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "‹#›\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:  {}\n",
      "--------------------------------------------------\n",
      "Content:  I.A Pretraining Large Language Model\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "Pre training phase\n",
      "‹#›\n",
      "II. Transformers\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "‹#›\n",
      "Metadata:  {}\n",
      "--------------------------------------------------\n",
      "Content:  I.B Fine tuning Large Language Model\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "Post training phase\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“The”\n",
      "Feed forward + Softmax model\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "?\n",
      "“decoder”\n",
      "“decoder”\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "WQ\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "3.23 -1.23 0.89 0.32 -3.29 3.23 1.23 -2.34 1.83 1.92 0.10 1.28\n",
      "4 70 0 85 -4 -10 0 0 2 0 0 0 3 -3 4 5\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "WK\n",
      "E2\n",
      "K2\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Efficient similarity search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "‹#›\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "Metadata:  {}\n",
      "--------------------------------------------------\n",
      "Content:  I.B.2 RLHF\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "SFT Limitations: \n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Cross attention\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "‹#›\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "Metadata:  {}\n",
      "--------------------------------------------------\n",
      "Content:  I.A.8 Cost & Optimizations\n",
      "Optimal model and data size\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "‹#›\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "1.23 -1.23 0.89 1.12 2..29 3.23 -3.23 -3.34 2.83 0.92 1.10 4.28\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "I am a noun, starting the sentence\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "I am a verb\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "E2\n",
      "K2\n",
      "…\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Might be a TV object  of a model\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.2. Information retrieval \n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "‹#›\n",
      "Metadata:  {}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content: \", doc.page_content)\n",
    "    print(\"Metadata: \", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You have successfully ingested the data in Cloud SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
