{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we’ll walk through the process of interacting with a Google Cloud Storage (GCS) bucket named dauphine-bucket, specifically focusing on the data directory within the bucket. We’ll cover how to:\n",
    "\n",
    "- List all files in the bucket’s data directory.\n",
    "- Retrieve information about a specific file.\n",
    "- Read files using the Unstructured library.\n",
    "- Visualize the extracted documents with LangChain.\n",
    "\n",
    "This guide is intended for users who are familiar with Python and basic cloud storage concepts.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following:\n",
    "\n",
    "- Python 3.x installed on your system.\n",
    "- Access to the GCP bucket dauphine-bucket/data with the necessary permissions.\n",
    "- Google Cloud SDK installed and authenticated. You can authenticate by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=FZObwIV5155UK6ywYxBpyfU6obFTTt&access_type=offline&code_challenge=caUCyCi7ohwJCMnbkMGcXAdhQYNCeI7nwYFGdrRKx6o&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [mariem.inoubli888@gmail.com].\n",
      "Your current project is [dauphine-437611].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python libraries installed:\n",
    "- google-cloud-storage\n",
    "- unstructured\n",
    "- langchain\n",
    "\n",
    "To know more about the libraries, you can visit the following links:\n",
    "- [google-cloud-storage](https://googleapis.dev/python/storage/latest/index.html)\n",
    "- [unstructured](https://docs.unstructured.io/examplecode/codesamples/oss/vector-database)\n",
    "- [langchain](https://langchain.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.18.2)\n",
      "Requirement already satisfied: unstructured in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: langchain in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: python-magic in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.4.27)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.35)\n",
      "Requirement already satisfied: langchain_google_cloud_sql_pg in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (2.35.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (3.10.1)\n",
      "Requirement already satisfied: backoff in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (0.25.9)\n",
      "Requirement already satisfied: wrapt in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured) (1.17.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (3.11.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.1.138)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: cloud-sql-python-connector<2.0.0,>=1.10.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (1.14.0)\n",
      "Requirement already satisfied: pgvector<1.0.0,>=0.2.5 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_google_cloud_sql_pg) (0.3.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cloud-sql-python-connector<2.0.0,>=1.10.0->cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=42.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cloud-sql-python-connector<2.0.0,>=1.10.0->cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (44.0.0)\n",
      "Requirement already satisfied: asyncpg>=0.30.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (0.30.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.28.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.25.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy[asyncio]<3.0.0,>=2.0.25->langchain_google_cloud_sql_pg) (3.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured) (4.66.6)\n",
      "Requirement already satisfied: deepdiff>=6.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (8.0.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography>=42.0.0->cloud-sql-python-connector<2.0.0,>=1.10.0->cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (1.17.1)\n",
      "Requirement already satisfied: orderly-set==5.2.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (5.2.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.12->cryptography>=42.0.0->cloud-sql-python-connector<2.0.0,>=1.10.0->cloud-sql-python-connector[asyncpg]<2.0.0,>=1.10.0->langchain_google_cloud_sql_pg) (2.22)\n",
      "Requirement already satisfied: unstructured[pptx] in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: chardet in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (3.9.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (3.10.1)\n",
      "Requirement already satisfied: backoff in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (0.25.9)\n",
      "Requirement already satisfied: wrapt in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (1.17.0)\n",
      "Requirement already satisfied: python-pptx<=0.6.23 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured[pptx]) (0.6.23)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-pptx<=0.6.23->unstructured[pptx]) (11.0.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-pptx<=0.6.23->unstructured[pptx]) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->unstructured[pptx]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured[pptx]) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->unstructured[pptx]) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from langdetect->unstructured[pptx]) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured[pptx]) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured[pptx]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured[pptx]) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk->unstructured[pptx]) (4.66.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured[pptx]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured[pptx]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured[pptx]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->unstructured[pptx]) (2024.8.30)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (44.0.0)\n",
      "Requirement already satisfied: deepdiff>=6.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (8.0.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from unstructured-client->unstructured[pptx]) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (24.1)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msi\\appdata\\roaming\\python\\python313\\site-packages (from unstructured-client->unstructured[pptx]) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured[pptx]) (1.17.1)\n",
      "Requirement already satisfied: orderly-set==5.2.2 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deepdiff>=6.0->unstructured-client->unstructured[pptx]) (5.2.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pptx]) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk->unstructured[pptx]) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured[pptx]) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install  google-cloud-storage unstructured langchain python-magic sqlalchemy langchain_google_cloud_sql_pg\n",
    "!pip install  \"unstructured[pptx]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-magic-bin in c:\\users\\msi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.4.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Listing and loading files from a GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Listing Files in the GCP Bucket\n",
    "\n",
    "Explanation:\n",
    "\n",
    "To interact with a GCS bucket, we’ll use the google-cloud-storage library. We’ll initialize a client, access the bucket, and list all the files within the data directory.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'dauphine-bucket/data':\n",
      "data/\n",
      "data/1 - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Access the bucket\n",
    "bucket_name = \"dauphine-bucket\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# List all files in the 'data' directory\n",
    "blobs = bucket.list_blobs(prefix=\"data/\")\n",
    "\n",
    "print(\"Files in 'dauphine-bucket/data':\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "Running this code will display all the file paths within the data directory of the bucket. The prefix='data/' parameter ensures we only get files from that specific directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Getting Information About One File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Sometimes, you may need detailed information about a specific file, such as its size, content type, or the last time it was updated. We’ll retrieve this metadata for a chosen file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for 'data/1 - Gen AI - Dauphine Tunis.pptx':\n",
      "Size: 6724048 bytes\n",
      "Content Type: application/vnd.openxmlformats-officedocument.presentationml.presentation\n",
      "Updated On: '2024-10-07 09:52:30.256000+00:00'\n",
      "Blob name: data/1 - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path (replace with an actual file from your bucket)\n",
    "file_path = \"data/1 - Gen AI - Dauphine Tunis.pptx\"\n",
    "\n",
    "# Get the blob object\n",
    "blob = bucket.get_blob(file_path)\n",
    "\n",
    "# TODO\n",
    "if blob:\n",
    "    print(f\"Information for '{file_path}':\")\n",
    "    print(f\"Size: {blob.size} bytes\")\n",
    "    print(f\"Content Type: {blob.content_type}\")\n",
    "    print(f\"Updated On: '{blob.updated}'\")\n",
    "    print(f\"Blob name: {blob.name}\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found in the bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DOWNLOADED_LOCAL_DIRECTORY = \"./downloaded_files\"\n",
    "\n",
    "\n",
    "os.makedirs(DOWNLOADED_LOCAL_DIRECTORY, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "This code will output metadata about the specified file. Make sure to replace 'data/your_file.ext' with the actual file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Reading Files with Unstructured\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Unstructured library allows us to parse and process unstructured data from various file formats. We’ll download a file from the bucket and use Unstructured to read and extract its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from pptx import Presentation\n",
    "\n",
    "\n",
    "DOWNLOADED_LOCAL_DIRECTORY = \"./downloaded_files\"\n",
    "file_path = \"data/1 - Gen AI - Dauphine Tunis.pptx\"\n",
    "\n",
    "# Function to download the file: file_path from the GCS Bucket\n",
    "def download_file_from_bucket(bucket: storage.Bucket, file_path: str) -> str:\n",
    "    blob = bucket.blob(file_path)\n",
    "    local_file_name = os.path.basename(file_path)\n",
    "    local_filepath = os.path.join(DOWNLOADED_LOCAL_DIRECTORY, local_file_name)\n",
    "    blob.download_to_filename(local_filepath)\n",
    "    print(f\"Downloaded '{file_path}' to '{local_filepath}'\")\n",
    "    return local_filepath\n",
    "\n",
    "from pptx import Presentation\n",
    "from datetime import datetime\n",
    "import uuid  # Pour générer des IDs uniques\n",
    "\n",
    "def extract_detailed_metadata(filepath: str) -> list[Document]:\n",
    "    presentation = Presentation(filepath)\n",
    "    documents = []\n",
    "\n",
    "    # Métadonnées du fichier\n",
    "    filename = os.path.basename(filepath)\n",
    "    file_directory = os.path.dirname(filepath)\n",
    "    last_modified = datetime.fromtimestamp(os.path.getmtime(filepath)).isoformat()\n",
    "    filetype = \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n",
    "\n",
    "    # Extraire les éléments de chaque diapositive\n",
    "    for slide_idx, slide in enumerate(presentation.slides):\n",
    "        parent_id = None  # Réinitialiser pour chaque nouvelle diapositive\n",
    "\n",
    "        for shape_idx, shape in enumerate(slide.shapes):\n",
    "            if shape.has_text_frame:  # Vérifie si l'élément contient un texte\n",
    "                for paragraph_idx, paragraph in enumerate(shape.text_frame.paragraphs):\n",
    "                    text = paragraph.text.strip()\n",
    "                    if text:  # Ignore les paragraphes vides\n",
    "                        # Générer un ID unique pour chaque élément\n",
    "                        element_id = str(uuid.uuid4())\n",
    "\n",
    "                        # Déterminer la catégorie et la profondeur\n",
    "                        if parent_id is None or shape_idx == 0:  \n",
    "                            category = \"Title\"\n",
    "                            category_depth = 1\n",
    "                            parent_id = element_id  \n",
    "                        else:\n",
    "                            category = \"Content\"\n",
    "                            category_depth = 3\n",
    "\n",
    "                        # Métadonnées associées\n",
    "                        metadata = {\n",
    "                            \"source\": filepath,\n",
    "                            \"category_depth\": category_depth,\n",
    "                            \"file_directory\": file_directory,\n",
    "                            \"filename\": filename,\n",
    "                            \"last_modified\": last_modified,\n",
    "                            \"page_number\": slide_idx + 1,\n",
    "                            \"languages\": [\"eng\"],\n",
    "                            \"filetype\": filetype,\n",
    "                            \"category\": category,\n",
    "                            \"element_id\": element_id,\n",
    "                        }\n",
    "\n",
    "                        # Ajouter le parent_id si pertinent\n",
    "                        if category == \"Content\":\n",
    "                            metadata[\"parent_id\"] = parent_id\n",
    "\n",
    "                        # Créer un `Document` pour chaque paragraphe\n",
    "                        documents.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_file_from_local(local_filepath: str) -> list[Document]:\n",
    "    if local_filepath.endswith(\".pptx\"):\n",
    "        return extract_detailed_metadata(local_filepath)\n",
    "    else:\n",
    "        # Initialize UnstructuredLoader for other file types\n",
    "        loader = UnstructuredLoader(file_path=local_filepath)\n",
    "        return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing 'data/': [Errno 2] No such file or directory: './downloaded_files\\\\'\n",
      "Downloaded 'data/1 - Gen AI - Dauphine Tunis.pptx' to './downloaded_files\\1 - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx' to './downloaded_files\\2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx' to './downloaded_files\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx' to './downloaded_files\\3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx'\n"
     ]
    }
   ],
   "source": [
    "# Load all the\n",
    "blobs = list(bucket.list_blobs(prefix='data/'))\n",
    "documents: list[Document] = []\n",
    "if blobs:\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            local_filepath = download_file_from_bucket(bucket, blob.name)\n",
    "            documents.extend(read_file_from_local(local_filepath))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{blob.name}': {e}\")\n",
    "else:\n",
    "    print(\"No files found in the 'data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Visualizing the First Documents Extracted with LangChain\n",
    "\n",
    "Explanation:\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. We’ll use it to load and visualize the documents extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Generative AI with LLM\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00cdf085-1e67-4fe4-9239-323f11e984e1'}\n",
      "\n",
      "Content:\n",
      "Florian Bastin\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '8d684e44-003e-4c3e-acdd-811a7e4c178b'}\n",
      "\n",
      "Content:\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 3, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Content', 'element_id': '6c2833cf-7d82-4229-bc66-d3fb07b81066', 'parent_id': '8d684e44-003e-4c3e-acdd-811a7e4c178b'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents[:3]:\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Join extracted document by page\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- The text extraction block is uninformative because very small text blocks are extracted from the document.\n",
    "- We can join the extracted text by page to get a more meaningful output.\n",
    "- A metadata with the 'page_number' can be helpful\n",
    "- The other metadatas need to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00cdf085-1e67-4fe4-9239-323f11e984e1'}, page_content='Generative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›\\nGenerative AI with LLM\\nFlorian Bastin\\n👨🏼\\u200d🎓 Master MASH - Université PSL\\n👨🏼\\u200d💻 LLM Engineer @OctoTechnology\\nLe Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '83b9389e-5739-4d93-9c8f-077671560248'}, page_content='‹#›\\nI.A Pretraining Large Language Model\\nPre training phase\\nA. Pretraining a Large Language Model\\nIntroduction\\nCross entropy loss\\nTokenization\\nEvaluation\\nData preprocessing\\nScaling laws\\nTraining process\\nCost and optimization\\n‹#›\\nII. Transformers\\n‹#›\\nII.B. Transformers Architecture\\n‹#›\\nIII. Retrieval Augmented Generation\\nBasic Architecture\\nInformation retrieval\\nVectorstore & Search optimization\\nRAG Techniques\\nEvaluation\\nMultimodal RAG\\nSOTA RAG architectures'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '252d69f4-8d25-40e8-b047-6626cede19c7'}, page_content='Autoregressive language models:\\nThe chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\\nLanguage modelling\\n‹#›\\nI.A.1 Introduction\\nLanguage Models: probability distribution over a sequence of words p(x1, … xn)\\nP(Transformers, are, encoder, decoder, models) = 0.01\\nP(Transformers, are, are, encoder, decoder, models) = 0.0001  \\tSyntactic knowledge\\nP(Transformers, are, decoder, models) = 0.001 \\tSemantic knowledge\\nP(Transformers, are, encoder, decoder, models) = P(Transformers)\\n. P(Transformers are | Transformers)\\n…\\n. P(models | Transformers, are, encoder, decoder)\\n‹#›\\nII. Transformers\\nA. Before Transformers\\nN grams\\nEmbeddings\\nRNN\\nLSTM\\nB. Transformers\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\n‹#›\\nII.B. Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\nIII. Introduction\\n‹#›\\nLLM vs RAG\\nLLM\\nRAG'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '32e21684-d00b-4193-ab11-2be317556ed6'}, page_content='Language modelling\\n‹#›\\nI.A.1 Introduction\\nModel\\nThe goal is to generate token by token\\nThe steps for generation:\\nTokenize\\nFeed the model with the token\\nPredict the probability of each possible token\\nSample from the likelihood\\nDetokenize\\nTransformers are encoder\\n9140           388     527    24592\\n8832\\nDecoding\\nPolo Club, Transformer Explainer [Blog]\\n‹#›\\nII.A. Before Transformers\\nThe Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn]\\nA. Before Transformers\\nN grams\\nEmbeddings\\nRNN\\nLSTM\\n‹#›\\nII.B Introduction\\nBahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\\nIII. Introduction\\n‹#›\\nDefinition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\\nBenefits:\\n•\\tProduces more informed and factual responses.\\n•\\tCan handle queries about recent events not present in the training data.\\n•\\tReduces hallucinations common in language models.\\nRAG Definition'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4e799123-e1bc-46da-b61c-d26803946446'}, page_content='How the model works ?\\n‹#›\\nI.A.1 Introduction\\nThe general model pipeline is as follows:\\nFeed word embedding for previous (context)\\nwords into a network\\nGet vector representation of context from\\nthe network\\nFrom this vector representation, predict a\\nprobability distribution for the next token.\\nLena Voita, Language Modeling [Blog]\\n‹#›\\nII.A. Before Transformers\\nOur goal today\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model\\nModel\\nInput: “Transformers are encoder decoder”\\nPredict: “models”\\n‹#›\\nII.B Introduction\\nIII.1. Basic Architecture\\n‹#›\\nRAG Architecture\\nConstruire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO\\nStep 1: Document ingestion\\nStep 2: Contextualized answering'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51a2c447-89ac-4e89-94c7-6d24a6c93f1e'}, page_content='Cross entropy loss\\n‹#›\\nI.A.2 Cross Entropy Loss\\nThe general model pipeline is as follows:\\nFeed word embedding for previous (context) words into a network\\nGet vector representation of context from the network\\nFrom this vector representation, predict a probability distribution for the next token.\\nLena Voita, Language Modeling [Blog]\\nMaximizing the likelihood is equivalent to minimizing the cross entropy loss:\\n‹#›\\nII.A.1 N Grams\\nN Grams\\nInput text:\\nTo Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\\nThe chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\\nN-gram generator\\nLena Voita, Language Modeling [Blog]\\n‹#›\\nII.B Introduction\\nIII.1. Basic Architecture\\n‹#›\\nRAG Architecture'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e3bced7f-2fa1-4e34-800c-475392dd5714'}, page_content='Tokenization\\n‹#›\\nI.A.3 Tokenization\\nHow to split ?\\nWord ?\\nLetter ?\\nHow to split and get token ?\\nByte-Pair Encoding (BPE) process:\\n1. Use a big corpus of text\\n2. Consider first one token per character\\n3. Merge commons pairs\\n4. Stop when a you cannot merge or the Vocab size is reached\\nThis GIF is generated from GPT o1 using the following prompt\\nFrom the following sentence: Transformers are encoder decoder models\\nApply the following steps:\\n- Create a manim code to display this sentence where each character has a different color\\n- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system\\n- Change the colors of new pair\\n- Continue until all commons pair are made\\n- Update at each step the manim code\\n- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\\n- Edit the code at the final stage to change color if two adjacent different pair have same color\\n‹#›\\nII.A.2 Embeddings\\nOur goal today\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model\\nModel\\nInput: “Transformers are encoder decoder”\\nPredict: “models”\\n‹#›\\nII.B. Introduction\\nIII.1. Basic Architecture\\n‹#›\\nRAG Architecture'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '878b7f11-742d-4a0a-b4c5-aecd6423f7ab'}, page_content='Tokenization\\n‹#›\\nI.A.3 Tokenization\\nByte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words.\\nPretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\\nQ. What is the problem with numbers as tokens ?\\n‹#›\\nII.A.2 Embeddings\\n0\\n0\\n0\\n:\\n1\\n0\\n0\\n:\\n0\\n0\\n-0.81\\n:\\n:\\n:\\n4.56\\n:\\n:\\n:\\n-4.35\\n2.21\\nIndex: 1280\\nEmbedding model\\nSemantic representation\\nDim = |Chosen embedding size|\\nOne-hot encoding\\nDim = |Vocab Size|\\nFrom One-hot encoding to Word Embedding\\nWord Embedding\\nOne-hot encoding\\n“transformers”\\nII.B. Introduction\\n‹#›\\nGPT 3\\nEach word is generated one by one\\nOnly the decoder part is used\\nIII.1. Basic Architecture\\n‹#›\\nHow to ?'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '21732262-35af-497b-8830-0dcf99119f66'}, page_content=\"Evaluation\\n‹#›\\nI.A.4 Evaluation\\nInstead of cross-entropy, it is more common to report its transformation called perplexity:\\nA better model has higher log-likelihood and lower perplexity.\\nPerplexity = 10 ≃ The model hesitates between 10 tokens\\nTo better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\\nthe best perplexity is 1:\\x0bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\\nthe worst perplexity is |V|:\\x0bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\\nQ. Prove that the worst perplexity is |V|\\nLena Voita, Language Modeling [Blog]\\n‹#›\\nII.A.2 Embeddings\\nWord Embedding (Word2Vec, GloVe, BERT, ELMo)\\nRepresent each word as a vector of numbers\\nConvert a discrete representation to continuous, allowing:\\nMore ‘fine-grained’ representations of words\\nUseful computations such as cosine / euclidean distances\\nVisualization and mapping of words\\nTomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space\\nII.B. Introduction\\n‹#›\\nTranslation model (FR -> EN example)\\nThe sentence to translate given to the encoder\\nEach generated word added to the decoder\\nJay Allamar,  2019, The Illustrated Transformer\\nIII.2. Information retrieval\\n‹#›\\nTF-IDF\\nText Search using TF-IDF and Elasticsearch\\nGiven a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\\nBM 25\\nf(qi,D) is the number of times that the keyword qi occurs in the document D,\\n|D| is the length of the document D in words\\navgdl is the average document length in the text collection from which documents are drawn.\\nK1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\\nN is the total number of documents in the collection, and\\nn(qi) s the number of documents containing qi\"), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '222fc630-6993-4862-8a2d-a9c98bd25abd'}, page_content='Evaluation\\n‹#›\\nI.A.4 Evaluation\\nPerplexity depends on vocabulary size, ie tokenization method: not used anymore\\nWe now use evaluation Datasets\\nIFEval\\nBBH\\nMMLU-Pro\\nMath\\n…\\nDifferent fields (medical, math, physics, …)\\ncovered in the Dataset to provide diversity\\nHugging Face, Open LLM Leaderboard\\nEvaluation Datasets\\nHugging Face LLM Leaderboard\\n‹#›\\nII.A.3 RNN\\nOur goal today\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model\\nModel\\nInput: “Transformers are encoder decoder”\\nPredict: “models”\\n‹#›\\nLot of new knowledges in this paper:\\nNo more RNN, only attention\\nMLP layers and Attention\\nPositional encodings\\nResNet structure\\nParallelism with Multi Head Attention\\nII.B Introduction\\nIII.2. Information retrieval\\n‹#›\\nCosine Similarity\\nEuclidean distance'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fcff9069-7b78-4bd6-8b55-5d0db663a39b'}, page_content='Evaluation\\n‹#›\\nI.A.4 Evaluation\\nEvaluation process:\\nGet the likelihood of each answer\\nAsk the model to answer A) B) C) D)\\nBIG-Bench Hard [Github]\\nQ. If the model is trained of the whole internet, how could it be contaminated?\\nLena Voita, Language Modeling [Blog]\\n‹#›\\nII.A.3 RNN\\nSemantic representation\\nDim = |Chosen embedding size| = 100\\nHow to give a sentence to a model ?\\n-0.81\\n4.56\\n:\\n-4.35\\n2.21\\nEmbedding model\\n“transformers”\\n-0.02\\n2.36\\n:\\n-1.12\\n3.13\\nEmbedding model\\n“are”\\nD\\nD\\n-0.81\\n4.56\\n:\\n-4.35\\n2.21\\n-0.81\\n4.56\\n:\\n-4.35\\n2.21\\nD\\n2\\nD\\n1\\n⛔\\n2 ≠1\\n‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\nIII.2. Information retrieval\\n‹#›\\nThe Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\\nMaximal Marginal Relevance\\nThe goal of this metric is to retrieve dissimilar documents and increase diversity\\nD is the set of all candidate documents, R is the set of already selected documents, q is the query\\nSim1 is the similarity function between a document and the query\\nSim2 is the similarity function between two documents.\\ndi and  dj are documents in D and R respectively\\nThe parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '27ac6a3a-2995-4943-8635-0c38a0f9b52b'}, page_content='Preprocessing the Data\\n‹#›\\nI.A.5 Data Preprocessing\\n• Idea: use all of the clean internet\\n• Note: internet is dirty & not representative of what we want.\\nPractice:\\n1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\\n2. Text extraction from HTML (challenges: math, boilerplate)\\n3. Filter undesirable content (e.g. NSFW, harmful content, PII)\\n4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\\n5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\\n6. Model based filtering. Predict if page could be references by Wikipedia.\\n7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\\nlaws to get high downstream performance.\\nAt the end of training, overfit the model on very quality data\\nHugging Face, LLM Training Dataset\\nHTML page example\\n‹#›\\nII.A.3 RNN\\nSeq2seq model\\n‹#›\\nII.B.1 Self Attention Mechanism\\nTransformers\\nare\\nencoder\\ndecoder\\nAttention mechanism\\nIII.2. Information retrieval\\n‹#›\\nSparse vs Dense retrieval\\nSparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only.\\nDense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query.\\nHybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\\nSparse embedding (lots of 0)\\nQ. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ?\\nQ. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ?\\nQ. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ?'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f0b53894-a322-46ad-a5b2-33d6705dcaf1'}, page_content='Scaling laws\\n‹#›\\nI.A.6 Scaling Laws\\nMore ressources, more data and bigger models -> better models\\nJared Kaplan & Al, 2020, Scaling Laws for Neural Language Models\\n‹#›\\nII.A.3 RNN\\nRecurrent Neural Networks (Sequential Model)\\nAdvantages:\\nCan learn from context of previous word\\nSelf supervised learning model\\nProblems:\\nSequential model\\nVery short term memory\\n‹#›\\nII.B.1 Self Attention Mechanism\\nAttention mechanism\\n3 components:\\nQuery: What am I looking for ?\\nKey: What do I have ?\\nValue: What do I reveal to others ?\\nIII.2. Information retrieval\\n‹#›\\nSparse Lexical and Expansion (SPLADE)\\nSPLADE for Sparse Vector Search Explained\\nVector database are super efficient compare to Splade at the moment\\nWith sparse methods, you cannot get synonyms from a words.\\nSPLADE:\\nUse Bert to get similar words like synonyms\\nProvide these synonyms to a sparse methods\\nFormal & Al, 2021, SPLADE V2'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '272f400b-bbe6-4fed-b0f9-a16e75796499'}, page_content='Training process\\n‹#›\\nI.A.7 Training Process\\nSteps\\nFind scaling recipes (example: learning rate decrease if the size of the model increase)\\nTune hyper parameters on small models of differents size\\nChoose the best models among the smallest ones\\nTrain the biggest model with the\\nStanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\\nQ. Should I use Transformers or LSTM ?\\n‹#›\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.43\\n-3.2\\n..\\n..\\n3.32\\n2.11\\n-4.22\\n..\\n..\\n1.12\\n3.11\\n-4.22\\n..\\n..\\n4.98\\nQuery\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nAre we talking about TV ?\\nDo I mean Allocation de Retour à l’Emploi ?\\nAm I a superstar ?\\n…\\nQuery: What am I looking for ?\\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nIII.2. Information retrieval\\n‹#›\\nDeep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\\nLin & Al, 2023, How to Train Your DRAGON\\nDense retriever\\nProgressive Data Augmentation strategy for training sampling very difficult negatives\\nYasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9b70f6f4-6f3e-4dd6-a6fe-0c1ffc6ee545'}, page_content='Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\\n‹#›\\nI.A.8 Cost & Optimizations\\nDisplay all the models with same amount of compute (left figure)\\nSelect the best model for each compute in terms of training loss (middle & right figure)\\nExtrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\\nOptimal model and data size\\n‹#›\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\nBriefly describe the architecture of a RNN [Blog]\\nEach word is given sequentially (xt)\\nAn intern memory is updated after each word  (ht)\\nA context is provided with this memory\\n‹#›\\nII.B.1 Self Attention Mechanism\\nWK\\nMight be a TV object  of a model\\n…\\nI am a noun, starting the sentence\\nI am a verb\\nKey: What do I have ?\\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nIII.2. Information retrieval\\n‹#›\\nBest retrieval methods\\nLeaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '286ab461-72ff-4eae-816d-c9847ff1a85d'}, page_content='‹#›\\nI.A.8 Cost & Optimizations\\nLLAMA 3 400B cost approx. $80m\\nCarbon emitted approx. 2K tickets Tunis - New York\\nHow much it costs ?\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\nStatQuest with Josh Starmer [Youtube]\\nOptimizing the loss w.r.t weights:\\nD. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series\\n“Models” ?\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nQuery\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nAre we talking about TV ?\\nDo I mean Allocation de Retour à l’Emploi ?\\nAm I a superstar ?\\n…\\nE4\\nWK\\nMight be a TV object  of a model\\n…\\nI am a noun, starting the sentence\\n…\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\nNo we are not because I am a Transformer\\nYou should be a verb because I am a noun\\n2.11\\n-4.22\\n1.12\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\n…\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nIII.3. Vectorstore & Search optimization\\n‹#›\\nVector Database\\nDefinition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types.\\nThese embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\\nVector DB Comparison'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd5d39baa-af27-468c-8360-875ae9367b34'}, page_content='‹#›\\nI.B Fine tuning Large Language Model\\nPost training phase\\nB. Fine tuning a Large Language Model\\nSupervised Fine Tuning\\nRLHF\\nReward model\\nPPO & DPO\\nEvaluation & Challenges\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\n?\\n“decoder”\\nFeed forward + Softmax model\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nQuery\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nE4\\nWK\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nIII.3. Vectorstore & Search optimization\\n‹#›\\nEfficient similarity search\\nAnnouncing ScaNN: Efficient Vector Similarity Search\\nScalable Nearest Neighbors (ScaNN) - Google\\nFacebook AI Similarity Search (FAISS)\\nHierarchical Navigable Small Worlds (HNSW)\\nDefinition:\\nScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\\nHierarchical Navigable Small Worlds (HNSW) [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '314097e3-176f-44fa-a63b-c0ea3fa0e3e5'}, page_content='Open AI, 2022, Aligning language models to follow instructions [Blog]\\n‹#›\\nI.B.A Supervised Fine Tuning\\n“GPT-3 models aren’t trained to follow user instructions.\\nOpen AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\\nHow to get a user assistant ?\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\nStatQuest with Josh Starmer Youtube\\n“Models” ?\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nQuery\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nE4\\nWK\\nEmbedding\\nKEYS\\nK1\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\nK2\\nK3\\nK4\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\n2.11\\n-4.22\\n1.12\\nE1\\nE2\\nE3\\nE4\\nIII.3. Vectorstore & Search optimization\\n‹#›\\nReciprocal Rank Fusion (RRF)\\nDefinition:\\nRRF allows to merge results of different retrievers'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e6c2811f-d3d9-4b11-a890-4797a7789717'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nHow to get a user assistant ?\\nConversational Agent\\nChatGPT\\nPretrained Large\\nLanguage\\nModel\\nPost training: Alignement\\n‹#›\\nII.A.3 RNN\\nRNN Types of architectures\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\nStatQuest with Josh Starmer Youtube\\n“Models” ?\\nPraveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\\nQ. Which one fit our use case ?\\n‹#›\\nII.B.1 Self Attention Mechanism\\nSoftmax\\nQ1\\nQ2\\nQ3\\nQ4\\nK1\\nK2\\nK3\\nK4\\nQ1\\nQ2\\nQ3\\nQ4\\nK1\\nK2\\nK3\\nK4\\nThis step allows numerical stability\\nThe sum of each column is 1\\nIII.3. Vectorstore & Search optimization\\n‹#›\\nReranker\\nA reranker ranks retrieved documents after a first similarity search\\nReranker type:\\nCross-Encoders\\nNeural Rerankers\\nBenefits of Using a Reranker:\\nIncreased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\\nBetter Contextual Understanding: Helps the system understand subtle nuances in the query.\\nChallenges:\\nComputational Overhead: Additional processing can increase response time.\\nResource Intensive: Advanced models require significant computational resources.\\nBi Encoder vs Cross Encoder'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e09a8940-6d60-43cf-817e-cb0b1e20e9d3'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nHow to get a User Assistant from a Language Model ?\\nC. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog]\\n‹#›\\nII.A.4 LSTM\\nOur goal today\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model\\nModel\\nInput: “Transformers are encoder decoder”\\nPredict: “models”\\n‹#›\\nII.B.1 Masking attention Mechanism\\nThis step allows numerical stability\\nDefinition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\\nIdea: A later word cannot answer question to a previous word because it is unknown at inference\\nIII.4. RAG Techniques\\n‹#›\\nQuery augmentation\\nLuyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\\nDefinition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base.\\nBy augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\\nHyDE: Generate a fake answer from a query to improve information retrieval'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd9633e8f-774c-409e-99cc-9e84a7de6771'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nIdea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\\nFamous LLM follow user instructions with moderation\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nBoth long and short term memory are provided\\n“transformers”\\n“are”\\n“encoder”\\nColah, Understanding LSTM Networks [Blog]\\nRNN architecture\\nLSTM architecture\\n“transformers”\\n“are”\\n“encoder”\\nII.B.1 Self Attention Mechanism\\nEmbedding\\nTransformers\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nE1\\nE2\\nE3\\nE4\\n2.11\\n-4.22\\n1.12\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\nE1\\nE2\\nE3\\nE4\\nV1\\nV2\\nV3\\nV4\\nVALUES\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nWv\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nQuery rephrasing\\nQuery rephrasing can be used to rephrase the query from the conversation history'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0ed2ed9e-2548-464e-9c9f-244e4c081442'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nProblem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\\nSolution: Use LLM to scale Data Collection at low cost\\nSupervised Fine Tuning\\nHow can we get the post training data ?\\nAlpaca: A Strong, Replicable Instruction-Following Model [Blog]\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\n“models”\\nCt-1\\nht-1\\nColah, Understanding LSTM Networks [Blog]\\nAttention pattern\\nz1\\nz2\\nz3\\nz4\\nII.B.1 Self Attention Mechanism\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nᐩ\\nᐩ\\nᐩ\\nᐩ\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nLost In the Middle\\nRetrieved context provided at the beginning or the end of the prompt have more impact on the answer\\nLangChain, Long Context Reorder [Blog]\\nF. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b7f35d95-85e2-41fe-a2ff-32ec24bdab9c'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nProblem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\\nSolution: A few data is required for SFT\\nZhou & Al 2023, LIMA: Less Is More for Alignment\\nSupervised Fine Tuning\\nHow much data do we need ?\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nCell state to propagate long memory\\nGate defined by the sigmoid function\\n0 = don’t pass information\\n1 = let everything pass through\\nColah, Understanding LSTM Networks [Blog]\\n‹#›\\nII.B.2 Self Attention Mechanism\\nSoftmax\\nQ1\\nQ2\\nQ3\\nQ4\\nK1\\nK2\\nK3\\nK4\\nQ1\\nQ2\\nQ3\\nQ4\\nK1\\nK2\\nK3\\nK4\\nThis step allows numerical stability\\nIII.4. RAG Techniques\\n‹#›\\nOpen AI, Prompt engineering\\nPrompt Engineering\\nWrite clear instructions\\nProvide reference text\\nSplit complex tasks into simpler subtasks\\nGive the model time to \"think\"\\nUse external tools (RAG)\\nTactic:\\nAsk the model to adopt a persona\\nUse delimiters to clearly indicate distinct parts of the input\\nSpecify the steps required to complete a task\\nProvide examples\\nSpecify the desired length of the output'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ec318639-0e6f-4a76-be30-d17b6a560711'}, page_content='‹#›\\nI.B.A Supervised Fine Tuning\\nSame process than the language model training\\nHow Supervised Fine Tuning Works ?\\nOpen AI, 2022, Aligning language models to follow instructions [Blog]\\nModel\\nExpl ain the moon lan ding to a 6 years old\\n9140 820 19  354 3672 34 347 321  2903 224 9832\\n3892\\nChild\\nSome 👌\\nLoss\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nColah, Understanding LSTM Networks [Blog]\\n“decoder”\\nII.B.2 Cross attention\\nFrench to english translation example\\nNo masking\\nEmbedding\\nTransformers\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nQuery\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nE4\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nTransformers\\nLes\\nsont\\ndes\\n…\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nDocument Loader\\nLoad any type of document (PDF, PPT(x), DOC(x), XLS(x)\\nUnstructured: https://unstructured.io/\\nLLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'abcf61ef-db37-4476-bc46-3170e1b039e1'}, page_content='‹#›\\nI.B.2 RLHF\\nSFT Limitations:\\nBehavior cloning\\nHuman abilities to answer perfectly to a given question\\nHallucination if answer from human not in training data\\nData collection cost\\nReinforcement Learning from Human Feedback - RLHF\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nColah, Understanding LSTM Networks [Blog]\\n“decoder”\\nII.B.2 Cross attention\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nContext\\nDefinition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\\nA larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\\nA smaller context size limits the amount of information the model can utilize from the input text.\\nVariable Sequence Length Training for Long-Context Large Language Models'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3239c6b0-1880-4fe1-9528-90df6905af17'}, page_content='‹#›\\nI.B.2 RLHF\\nIdea:\\nFrom a question, generate multiples answers\\nAsk a human to classify answers\\nTrain a reward model to learn these preferences\\nReward model: classifier that is trained to classify preferences from possibles answers\\nReinforcement Learning from Human Feedback - RLHF\\nOpen AI, 2022, Aligning language models to follow instructions [Blog]\\nClassifier\\nModel\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nColah, Understanding LSTM Networks [Blog]\\n“decoder”\\nII.B.1 Self Attention Mechanism\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nKey: What do I have ?\\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nQuery: What am I looking for ?\\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nGPT 3 dimension for one attention head\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nChunking\\nAnnouncing ScaNN: Efficient Vector Similarity Search\\nTo avoid context limitations, we can do document chunking:\\nChunk by document if the document is small\\nChunk by title or header if the document is big'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'aea79fa4-11ed-435b-858c-e14f60e1dd20'}, page_content='‹#›\\nI.B.3 Reward Model\\nAlso transformer based LM\\nVariation in sizes used (relative to policy)\\nOutputs scalar from input text\\nReward model\\nOpen AI, 2022, Aligning language models to follow instructions [Blog]\\nClassifier\\nModel\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nColah, Understanding LSTM Networks [Blog]\\n“decoder”\\nII.B.1 Self Attention Mechanism\\nValues: What do I reveal to others ?\\n|E| : Embedding (1, 1512)\\n|WV|: Value matrix (12 288, 12 288)\\nKey: What do I have ?\\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nQuery: What am I looking for ?\\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nGPT 3 dimension for all attention heads: 603 979 776 parameters\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nRAG Frameworks in Python\\nLangchain\\nLlamaIndex'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9e0a4e53-74f5-4e1d-a212-09ab963c9fec'}, page_content='‹#›\\nI.B.4 PPO & DPO\\nAlso transformer based LM\\nVariation in sizes used (relative to policy)\\nOutputs scalar from input text\\nTraining RL model\\nLambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\\nPrevent over optimization\\n‹#›\\nII.A.4 LSTM\\nLong Short Term Memory (LSTM)\\nWhat about Vanishing / Exploding Gradients ?\\nColah, Understanding LSTM Networks [Blog]\\nThe additive update function for the cell state gives a derivative that is much more ‘well behaved’\\nThe gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\\nTo get details on LSTM derivative, check out this blog post\\nII.B.1 Self Attention Mechanism\\nValue Matrix (12 288, 12 288) decomposition≈\\n=\\n12 288\\n128\\n128\\n12 288\\n12 288\\nIdea:\\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the\\nForce the Value matrix to be low rank\\n12 288\\n‹#›\\nIII.4. RAG Techniques\\n‹#›\\nCloud services\\nCloud Services provide:\\nA Secure environment\\nEnough compute to train big models\\nProduct as a Service (PaaS)\\nLLMs APIs\\nVector Store management\\nEfficient Retrieval\\nMonitoring tools\\n…'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51a89cb8-ead1-4592-a6de-41a4fd8c5a3a'}, page_content='‹#›\\nI.B.4 PPO & DPO\\nPPO is much more complex (clipping, rollouts, outer loops) than in theory\\nMaximize the desired output, minimize the other\\nDPO\\nRafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\\n‹#›\\nII.A.4 LSTM\\nOur goal today\\nPredict the word “models” from the input sentence\\nRequirements:\\nFind a way to transform word into numerical values\\nProvide semantic relationship between the encoding words\\nProvide context to our model to understand the sentence\\nProvide long context to our model to understand the sentence\\nCreate a fast trainable model\\nModel\\nInput: “Transformers are encoder decoder”\\nPredict: “models”\\nII.B.1 Multi\\nValue Matrix (12 288, 12 288) decomposition\\n=\\n12 288\\n128\\n128\\n12 288\\n12 288\\nIdea:\\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the\\nForce the Value matrix to be low rank\\n12 288\\n‹#›\\nIII.5. Evaluation\\n‹#›\\nRetriever Evaluation: Precision & Recall @ k\\nPrecision @k\\nHow many  retrieved documents are relevant ?\\nRecall @k\\nHow many  relevant documents are retrieved ?'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 30, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51dbab0f-39bb-46f6-b713-6efeba2a49d6'}, page_content='‹#›\\nI.B.5 Evaluation & Challenges\\nRLHF gains\\nNisan Stiennon & Al, 2020, Learning to summarize from human feedback\\nDubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\\nII.B.1 Self Attention Mechanism\\nValue Matrix (12 288, 12 288) decomposition\\n=\\n12 288\\n128\\n128\\n12 288\\n12 288\\nIdea:\\nThe number of # is 150m for the Value matrix\\nTo avoid this high dimension and respects the\\nForce the Value matrix to be low rank\\n12 288\\n.\\nWv\\n‹#›\\nIII.5. Evaluation\\n‹#›\\nRetriever Evaluation : NDCG\\nNDCG can take values from 0 to 1.\\nNDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance.\\nNDCG equals 0 when there are no relevant objects in top-K.\\nNDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better.'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 31, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5a4e4862-75b5-46bb-95a0-89de948f38de'}, page_content='‹#›\\nI.B.5 Evaluation & Challenges\\nRLHF challenges\\nSinghal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\\nAnswer preference is not trivial\\nRLHF increases answer size\\nHumans do not agree (agree with themselves only 66% of the time)\\nHuman have lot of variance, model have no variance\\nAsk LLM preferences instead of human preferences\\nStanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\\nII.B.1 Self Attention Mechanism\\nValue Matrix computation optimization\\n.\\n12 288\\n12 288\\nE1\\n=\\n0.32  \\t3.02 \\t…\\t-0.33\\n12 288\\n=    V1\\nWv\\n‹#›\\nIII.5. Evaluation\\n‹#›\\nAnswer Evaluation: LLM As a judge\\nRAGAS\\nAsk an LLM to evaluate answer quality:\\nDoes my answer answer to the question ?\\nDoes my answer used information from the context ?\\nDoes my answer give enough facts ?\\n…\\nBLEU, ROUGE, Perplexity are not ideal for RAG use case.\\nEvaluating answers in RAG is not easy'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 32, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a5b8309b-2a11-4114-bf3e-a158facbc208'}, page_content='‹#›\\nI.B.5 Evaluation & Challenges\\nEvaluation\\nChatbot Arena, Open LM\\nHow to evaluate a model like Chat GPT ?\\nDifferent methods (DPO, PPO, SFT) can be compared\\nModels are not calibrated\\nA large diversity of evaluation to cover\\nII.B.1 Self Attention Mechanism\\nMethod 1 WV (12 288, 12 288)\\n12 288\\n=\\n0.1 x v21     0.1 x v22\\t        …\\t0.1 x v2, 12288\\n12 288\\n+\\n0.32 x 0.62      3.02 x 0.62\\t        …\\t-0.33 x 0.62\\n…\\nNumber of computations:\\nN words x 12 288 multiplications\\nN words x 12 288 additions\\n‹#›\\nIII.6. Multimodal RAG\\n‹#›\\nMultimodal\\nBerrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\\nMultimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\\nExemple: GPT 4o, Gemini 1.5, Qwen2- VL'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 33, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'aac3db58-13b3-4efe-9e8a-e722c7ccc769'}, page_content='‹#›\\nI.B.5 Evaluation & Challenges\\nOpenAI o1: “Streaming is dead, long live Chain of Thought”\\nOpen AI, 2024, Learning to Reason with LLMs [Blog]\\n“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\\nChain of thought (COT)\\nIncrease test time compute\\nII.B.1 Self Attention Mechanism\\nMethod 1 WV (12 288, 128)\\n128\\n=\\n0.1 x v21     0.1 x v22\\t        …\\t0.1 x v2, 128\\n128\\n+\\n0.32 x 0.62      3.02 x 0.62\\t        …\\t-0.33 x 0.62\\n…\\nStep 2:\\nNumber of computations:\\nN words x 128 multiplications\\nN words x 128 additions\\nMatrix multiplication between value up matrix and result:\\n128 multiplication + 128 addition for each row\\n12 288 times\\n‹#›\\nIII.6. Multimodal RAG\\n‹#›\\nMultimodal: Qwen2- VL\\nBerrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\\nOutperform GPT 4o on most of the benchmarks\\nMultimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 34, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '08d4de06-54bf-4d74-9dc8-f9958aab3743'}, page_content='‹#›\\nI.B.5 Evaluation & Challenges\\nOpenAI o1 vs GPT 4o\\nPrompt:\\nFrom the following sentence: Transformers are encoder decoder models\\nApply the following steps:\\n- Create a manim code to display this sentence where each character has a different color\\n- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system\\n- Change the colors of new pair\\n- Continue until all commons pair are made\\n- Update at each step the manim code\\n- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\\n- Edit the code at the final stage to change color if two adjacent different pair have same color\\nOpen AI o1\\nGPT 4o\\n‹#›\\nIII.6. Multimodal RAG\\n‹#›\\nRotary Embedding (ROPE)\\nSu & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\\nRotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation.\\nUnlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\\nProperties:\\nFlexibility of being expand to any sequence lengths\\nDecaying inter-token dependency with increasing relative distances\\nCapability of equipping the linear self-attention with relative position encoding.'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 35, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '19ab36f5-a4f1-4a42-99e9-9afde43770a0'}, page_content='‹#›\\nConclusion\\nConclusion\\nBuilding GPT 3\\nData preprocessing is a very important step to get quality data\\nGPT 3 training consists of two phases (pre and post training)\\nSupervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\\nRLHF helps the model to align with human preferences\\nDPO is the new methods for Alignment, replacing RLHF\\nGeneral knowledges\\nData size and model size depends on compute resources (Scaling Laws, Chinchilla).\\nOpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\\n“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\\nSebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog]\\n‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\nIII.6. Multimodal RAG\\n‹#›\\nMultimodal RAG\\nLangChain, Multi-Vector Retriever for RAG on tables, text, and images\\nOption 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\\nOption 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\\nOption 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 36, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7ab0e679-3095-408c-8761-5681fcdf7292'}, page_content='‹#›\\nII.B.2 Multi-Head Attention\\nTransformers\\nare\\nencoder\\ndecoder\\nAttention mechanism\\nIII.6. Multimodal RAG\\n‹#›\\nMultimodal RAG\\nYasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 37, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '79ecfc5d-7682-4601-acde-97ed7ebae941'}, page_content='z1\\nz2\\nz3\\nz4\\nII.B.2 Multi-Head attention\\nᐩ\\nᐩ\\nᐩ\\nᐩ\\nZ\\nz1\\nz2\\nz3\\nz4\\nN words\\n|V| (=128)\\n‹#›\\nIII.7. SOTA RAG architectures\\n‹#›\\nSelf-RAG\\nAhmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\\nGenerates multiple possible response segments in parallel, utilizing the retrieved documents as context.\\nThe model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\\nThis selection process ensures that the response is both factually correct and contextually appropriate.\\nAsai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 38, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '96117f15-71e4-42b7-bd7e-942967b6b788'}, page_content='II.B.2 Multi-Head attention\\nConcatenation\\n…\\nz1\\nz10\\nz96\\nN words\\n|V| x N heads\\n= 128 x 96\\nz1\\nz2\\nz3\\nz96\\n…\\n‹#›\\nIII.7. SOTA RAG architectures\\n‹#›\\nRAPTOR\\nAhmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\\nRAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step.\\nThis hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\\nSarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 39, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fa76d935-e820-4cb0-a7d0-2a386c8bb3c4'}, page_content='II.B.2 Multi-Head attention\\n…\\nz1\\nz10\\nz96\\nN words\\n|V| x N heads\\n= 128 x 96\\nx\\nW0\\n|V| x N heads\\n| Embeddings Dim|\\nN words\\n| Embeddings Dim|\\nZ\\n‹#›\\nIII.7. SOTA RAG architectures\\n‹#›\\nCorrective RAG (CRAG)\\nYan & Al, 2024, Corrective Retrieval Augmented Generation\\nAdd a retrieval evaluator based on the quality of retrieved sources\\nIf sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 40, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '36b2aa73-63bf-42b1-b919-c720268f7a77'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\nInput sentence: Transformers are encoder decoder\\n|Key Dim| = |Query Dim| = |Value Dim| = 3\\n|Embedding Dim| = 5\\n|N words| = 4\\nTransformers\\nare\\nencoder\\ndecoder\\n5\\nWK0\\nWQ0\\nWV0\\nZ0\\nQ0\\nV0\\n3\\n4\\n4\\n3\\n5\\n3\\n4\\nK0\\nStep 1\\nStep 2\\nStep 3\\n‹#›\\nIII.7. SOTA RAG architectures\\n‹#›\\nGraphRAG\\nMicrosoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\\nThe LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph.\\nThis graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset.\\nAt query time, both of these structures are used to provide materials for the LLM context window when answering a question.'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 41, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6557cdb8-8bd8-4496-bee7-eab6a46efaf7'}, page_content='II.B.2 Multi-Head attention - Summary exercice\\n…\\nWK0\\nWQ0\\nWV0\\nWK96\\nWQ96\\nWV96\\nQ96\\nV96\\nK96\\nQ0\\nV0\\nK0\\n…\\n…\\nZ0\\nZ96\\nStep 4\\n‹#›\\nIII. Conclusion\\n‹#›\\nConclusion\\nSimple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\\nContext Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\\nMulti-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\\nFusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\\nIntelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\\nQuery Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\\nHierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\\nHypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\\nChoose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\\nSemantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\\nContext Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\\nExplainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\\nRetrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 42, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2d7e6446-c3de-4b04-aef0-30c79d6a36b4'}, page_content='…\\nZ0\\nZ96\\nW0\\n4\\n3\\n4\\n3\\n3\\n4 x 96\\n4 x 96\\n5\\nMatmul\\n4\\n5\\nII.B.2 Multi-Head attention - Summary exercice\\nStep 5\\nZ\\n‹#›\\nIII. Conclusion\\n‹#›\\nConclusion\\nAdaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\\nIterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\\nEnsemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\\nGraph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\\nMultimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\\nRAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\\nSelf RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\\nCorrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\\nFew shot examples: Provides a few examples in the prompt to help the LLM understand the desired output'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 43, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5ea29042-9f6a-45e9-ad52-e5d5988e271f'}, page_content='4\\n5\\nII.B.2 Multi-Head attention - Summary exercice\\nZ\\nWhy do Z and X have the same dimension ?\\nTransformers\\nare\\nencoder\\ndecoder\\n5\\n4\\nX\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 44, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e5319d26-f0fc-4433-be7a-0ce4295f6907'}, page_content='‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 45, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fe2ccc25-e6bc-44e9-b0a4-ab627a6c022c'}, page_content='‹#›\\nII.B.3. Residual connections & Layer normalization'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 46, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'babb96ec-0869-40ef-a9bc-6ae85c91af8e'}, page_content='II.B.3. Residual connections & Layer normalization\\nResidual connections\\nResidual connections mainly help mitigate the vanishing gradient problem\\nAnother effect of residual connections is that the information stays local in the Transformer layer stack\\nReLU\\ny = max(x, 0)\\nHe & Al, 2015,  Deep Residual Learning for Image Recognition\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 47, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '94723525-dd85-4f0a-bb94-5fe78b8cdd50'}, page_content='II.B.3. Residual connections & Layer normalization\\nLayer normalization\\n“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.\\nEmpirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\\nHinton & Al, 2016, Layer Normalization\\n2020, In-layer normalization techniques for training very deep neural networks [Blog]\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 48, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '04791d6c-3be7-4e9d-bcf3-d0ec06b01aa4'}, page_content='‹#›\\nSo far …'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 49, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '083d1b9a-de88-4f7e-8056-ba43e436bebf'}, page_content='‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings\\nOptimization'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 50, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3f62e8ad-b97e-402c-9f17-6776b034782e'}, page_content='‹#›\\nII.B.4. Feed forward layer\\n“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\\n‘Up\\nprojection’\\n‘Down\\nprojection’'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 51, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd927bdef-fc4a-40e3-939c-441a4e35e48c'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\n4\\n5\\nZ\\nTransformers\\nare\\nencoder\\ndecoder\\n5\\n4\\nX\\n‹#›\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 52, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7b2d53b7-9cbd-401a-b47b-1c75e59a2530'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\n4\\n5\\nZ\\nTransformers\\nare\\nencoder\\ndecoder\\n5\\n4\\nX\\n‹#›\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\nResidual connection + Layer Nom\\nFeed forward\\nLayer'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 53, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '636b5d03-526e-4845-b094-559739fdd805'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\n‹#›\\n4\\n5\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\n…\\n…\\nAm I a math architecture ?\\nAm I a football star ?\\nAm I a plural noun ?\\nFeed forward network\\nFeed forward network'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 54, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5e9f3916-024b-4e8e-9253-754ebcced980'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer\\n‹#›\\n4\\n5\\nTransformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\\nDecoder: “I am before encoder, maybe related to maths, maybe an architecture\\n…\\n…\\nFeed forward network\\nAm I a TV stuff ?\\nAm I a singular noun ?\\nAm I a math architecture ?'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 55, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a958f776-e563-4409-a1b3-d087c6ee3a1c'}, page_content='II.B.4. Feed forward layer\\nFeed forward layer : “Up projection”\\n‹#›\\nFeed forward network\\nUp projection dimension = 49 152 x embedding dimension\\n…\\n49 152'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 56, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9a7cdd00-f571-4d59-8729-5255e521257f'}, page_content='II.B.4. Feed forward layer\\n‹#›\\nAnother layer\\nDown projection dimension = embedding dim x 49 152\\n12 288\\nFeed forward layer : “Down projection”'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 57, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'faef1f31-6b3e-4dbd-8ab0-140d19d0a765'}, page_content='II.B.4. Feed forward layer\\nGPT 3 dimension per layer\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 58, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a2e8727e-b779-4977-b27a-463c3bbaa460'}, page_content='II.B.4. Feed forward layer\\nGPT 3 dimension (96 layers):\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 59, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '26936863-578f-4226-89d3-7faf051d4454'}, page_content='‹#›\\nSo far …'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 60, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'af1676bf-2288-46e4-9a65-f6fee47ecccc'}, page_content='‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 61, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '963b1a4b-74eb-42e9-b378-2d97f4ca0ccf'}, page_content='‹#›\\nII.B.5. Softmax Layer\\n“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\\nWe also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\\nIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 62, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ad3531da-6212-4d75-b564-be833cb8b1ef'}, page_content='‹#›\\nII.B.5. Softmax Layer\\nThe input is he vector from the last word of the sentence\\nThe output is the probability distribution over all words in the dictionary (50k words for GPT 3-'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 63, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ca3895be-2a17-416c-abb7-e0ba95e26570'}, page_content='‹#›\\nII.B.5. Softmax Layer\\nQ. Why don’t we take all the previous representations of the other vectors for the inference ?\\nFor training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\\nOf course, the last word context is learned with attention\\ndecoder\\nencoder\\nare'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 64, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '194fcbf4-78b4-496c-981d-425dcd87046b'}, page_content='‹#›\\nII.B.5 II.B.5. Softmax Layer - Temperature'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 65, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'efdcec06-c798-483d-a1dc-620e4e970d93'}, page_content='II.B.5. Softmax Layer\\nGPT 3 dimension (96 layers):\\n175 181 291 520 trainable parameters\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 66, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '804ca0a8-a504-436a-9534-ab1882a7b883'}, page_content='‹#›\\nSo far …'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 67, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '71e03e90-4164-46d0-a911-005ab1331ee2'}, page_content='‹#›\\nII.B.  Transformers Architecture\\nIntroduction\\nSelf Attention / Cross Attention\\nMulti-Head Attention\\nResidual connection & Layer normalization\\nFeed forward layer\\nSoftmax Layer\\nPositional Embeddings'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 68, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1aa1df75-09bd-4b33-b6b1-ad92bcfbca51'}, page_content='‹#›\\nII.B.6. Positional Embeddings\\n“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\\nTo this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 69, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4d574433-f698-4ca1-b13c-a620cdc99a64'}, page_content='II.B.6. Positional Embeddings\\n‹#›\\nComputations are not done sequentially (unlike RNN and LSTM)\\nHow to compare “A B C” and “C A B” ?\\nBeneficial to find a method that satisfy the following points:\\nUnambiguous (each position have its own value)\\nDeterministic\\nAllows to estimate distance between tokens\\nWorks with longer sequence than seen during training'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 70, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '92ad2416-ca20-4b2b-b6b6-3a84d89ee558'}, page_content='II.B.6. Positional Embeddings\\n‹#›\\nSame size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\\nDon’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\\nWhere:\\npos is the position {1, …, context length}\\ni is the dimension = {1, …, dmodel }\\ndmodel is the embedding dimension (GPT 3 = 12 288)'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 71, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f4367c45-bc34-4a9a-88f0-ccc959378355'}, page_content='II.B.6. Positional Embeddings\\n‹#›'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 72, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'cd8d0329-f84c-4dc9-80c1-20b3a630c6b0'}, page_content='II.B.6. Positional Embeddings\\n‹#›\\nProperties\\nThe positional values are unique if at least one function has maximum size of this sequence (context window) 👌\\nThe positional values are not random, created using two equations 👌\\nLooking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\\nSince sin and cosine are periodic functions, the model can generalize for longer sequences 👌'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 73, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1b10cf0a-3b67-4449-ab68-595ab0abf5d6'}, page_content='II.B.6. Positional Embeddings\\n‹#›\\nKazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\\n“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 74, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '24222e51-dfb6-476c-8e5b-5c3af44d3a6f'}, page_content='II.B.6. Positional Embeddings\\n‹#›\\nKazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\\nFigure: Positional encoding representation\\nEach row represent a positional vector for a given token'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 75, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '91c9afe5-4eee-45cc-be36-49ebffee8965'}, page_content='‹#›\\nSo far …'), Document(metadata={'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 76, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '23be6d71-7d5b-470b-9e9f-52f967f449ca'}, page_content='‹#›\\nII. Resources\\nBlogs:\\nThe Illustrated Transformer\\nTransformers Explained Visually (Part 3): Multi-head Attention, deep dive\\nPapers:\\nAttention is All You Need\\nVideos:\\nVisual introduction to Transformers (part 1)\\nTransformers visualized (part 2)\\nHow might LLMs store fact (part 3)\\nResidual Network and skip connections\\nStanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy')]\n",
      "--------------------------------------------------\n",
      "Page Number: 1\n",
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00cdf085-1e67-4fe4-9239-323f11e984e1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 2\n",
      "Content:\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "Pre training phase\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval\n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '83b9389e-5739-4d93-9c8f-077671560248'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 3\n",
      "Content:\n",
      "Autoregressive language models:\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      ". P(Transformers are | Transformers)\n",
      "…\n",
      ". P(models | Transformers, are, encoder, decoder)\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers\n",
      "N grams\n",
      "Embeddings\n",
      "RNN\n",
      "LSTM\n",
      "B. Transformers\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '252d69f4-8d25-40e8-b047-6626cede19c7'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 4\n",
      "Content:\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token\n",
      "The steps for generation:\n",
      "Tokenize\n",
      "Feed the model with the token\n",
      "Predict the probability of each possible token\n",
      "Sample from the likelihood\n",
      "Detokenize\n",
      "Transformers are encoder\n",
      "9140           388     527    24592\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "‹#›\n",
      "II.A. Before Transformers\n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn]\n",
      "A. Before Transformers\n",
      "N grams\n",
      "Embeddings\n",
      "RNN\n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "Benefits:\n",
      "•\tProduces more informed and factual responses.\n",
      "•\tCan handle queries about recent events not present in the training data.\n",
      "•\tReduces hallucinations common in language models.\n",
      "RAG Definition\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '32e21684-d00b-4193-ab11-2be317556ed6'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 5\n",
      "Content:\n",
      "How the model works ?\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "Feed word embedding for previous (context)\n",
      "words into a network\n",
      "Get vector representation of context from\n",
      "the network\n",
      "From this vector representation, predict a\n",
      "probability distribution for the next token.\n",
      "Lena Voita, Language Modeling [Blog]\n",
      "‹#›\n",
      "II.A. Before Transformers\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO\n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4e799123-e1bc-46da-b61c-d26803946446'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 6\n",
      "Content:\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "Lena Voita, Language Modeling [Blog]\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "N Grams\n",
      "Input text:\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator\n",
      "Lena Voita, Language Modeling [Blog]\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51a2c447-89ac-4e89-94c7-6d24a6c93f1e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 7\n",
      "Content:\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ?\n",
      "Word ?\n",
      "Letter ?\n",
      "How to split and get token ?\n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs\n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps:\n",
      "- Create a manim code to display this sentence where each character has a different color\n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system\n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made\n",
      "- Update at each step the manim code\n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e3bced7f-2fa1-4e34-800c-475392dd5714'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 8\n",
      "Content:\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words.\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      "4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size|\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size|\n",
      "From One-hot encoding to Word Embedding\n",
      "Word Embedding\n",
      "One-hot encoding\n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '878b7f11-742d-4a0a-b4c5-aecd6423f7ab'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 9\n",
      "Content:\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog]\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder\n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D,\n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn.\n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and\n",
      "n(qi) s the number of documents containing qi\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '21732262-35af-497b-8830-0dcf99119f66'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 10\n",
      "Content:\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard\n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention\n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "II.B Introduction\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '222fc630-6993-4862-8a2d-a9c98bd25abd'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 11\n",
      "Content:\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Evaluation process:\n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated?\n",
      "Lena Voita, Language Modeling [Blog]\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "How to give a sentence to a model ?\n",
      "-0.81\n",
      "4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      "2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "D\n",
      "D\n",
      "-0.81\n",
      "4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      "4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "D\n",
      "2\n",
      "D\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.\n",
      "di and  dj are documents in D and R respectively\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fcff9069-7b78-4bd6-8b55-5d0db663a39b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 12\n",
      "Content:\n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want.\n",
      "Practice:\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "At the end of training, overfit the model on very quality data\n",
      "Hugging Face, LLM Training Dataset\n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Sparse vs Dense retrieval\n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only.\n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query.\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ?\n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ?\n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ?\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '27ac6a3a-2995-4943-8635-0c38a0f9b52b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 13\n",
      "Content:\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "Problems:\n",
      "Sequential model\n",
      "Very short term memory\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components:\n",
      "Query: What am I looking for ?\n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f0b53894-a322-46ad-a5b2-33d6705dcaf1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 14\n",
      "Content:\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '272f400b-bbe6-4fed-b0f9-a16e75796499'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 15\n",
      "Content:\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog]\n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9b70f6f4-6f3e-4dd6-a6fe-0c1ffc6ee545'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 16\n",
      "Content:\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights:\n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database\n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types.\n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "Vector DB Comparison\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '286ab461-72ff-4eae-816d-c9847ff1a85d'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 17\n",
      "Content:\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "Post training phase\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd5d39baa-af27-468c-8360-875ae9367b34'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 18\n",
      "Content:\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog]\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions.\n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      "How to get a user assistant ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '314097e3-176f-44fa-a63b-c0ea3fa0e3e5'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 19\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "How to get a user assistant ?\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "Pretrained Large\n",
      "Language\n",
      "Model\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ?\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability\n",
      "The sum of each column is 1\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "Bi Encoder vs Cross Encoder\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e6c2811f-d3d9-4b11-a890-4797a7789717'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 20\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "How to get a User Assistant from a Language Model ?\n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog]\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability\n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base.\n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e09a8940-6d60-43cf-817e-cb0b1e20e9d3'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 21\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd9633e8f-774c-409e-99cc-9e84a7de6771'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 22\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "Supervised Fine Tuning\n",
      "How can we get the post training data ?\n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog]\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '0ed2ed9e-2548-464e-9c9f-244e4c081442'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 23\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT\n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning\n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information\n",
      "1 = let everything pass through\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'b7f35d95-85e2-41fe-a2ff-32ec24bdab9c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 24\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "Same process than the language model training\n",
      "How Supervised Fine Tuning Works ?\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog]\n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "3892\n",
      "Child\n",
      "Some 👌\n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "Unstructured: https://unstructured.io/\n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ec318639-0e6f-4a76-be30-d17b6a560711'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 25\n",
      "Content:\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "SFT Limitations:\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context\n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'abcf61ef-db37-4476-bc46-3170e1b039e1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 26\n",
      "Content:\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "Idea:\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog]\n",
      "Classifier\n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "Chunk by document if the document is small\n",
      "Chunk by title or header if the document is big\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3239c6b0-1880-4fe1-9528-90df6905af17'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 27\n",
      "Content:\n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog]\n",
      "Classifier\n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain\n",
      "LlamaIndex\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'aea79fa4-11ed-435b-858c-e14f60e1dd20'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 28\n",
      "Content:\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "Prevent over optimization\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ?\n",
      "Colah, Understanding LSTM Networks [Blog]\n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "12 288\n",
      "128\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "Idea:\n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the\n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment\n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9e0a4e53-74f5-4e1d-a212-09ab963c9fec'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 29\n",
      "Content:\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "12 288\n",
      "128\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "Idea:\n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the\n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51a89cb8-ead1-4592-a6de-41a4fd8c5a3a'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 30\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "12 288\n",
      "128\n",
      "128\n",
      "12 288\n",
      "12 288\n",
      "Idea:\n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the\n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      ".\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "NDCG can take values from 0 to 1.\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance.\n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 30, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '51dbab0f-39bb-46f6-b713-6efeba2a49d6'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 31\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance\n",
      "Ask LLM preferences instead of human preferences\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      ".\n",
      "12 288\n",
      "12 288\n",
      "E1\n",
      "=\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "12 288\n",
      "=    V1\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "Ask an LLM to evaluate answer quality:\n",
      "Does my answer answer to the question ?\n",
      "Does my answer used information from the context ?\n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case.\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 31, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5a4e4862-75b5-46bb-95a0-89de948f38de'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 32\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ?\n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "12 288\n",
      "=\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      "…\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 32, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a5b8309b-2a11-4114-bf3e-a158facbc208'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 33\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog]\n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "128\n",
      "=\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      "…\n",
      "Step 2:\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "Matrix multiplication between value up matrix and result:\n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 33, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'aac3db58-13b3-4efe-9e8a-e722c7ccc769'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 34\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt:\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps:\n",
      "- Create a manim code to display this sentence where each character has a different color\n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system\n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made\n",
      "- Update at each step the manim code\n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation.\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 34, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '08d4de06-54bf-4d74-9dc8-f9958aab3743'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 35\n",
      "Content:\n",
      "‹#›\n",
      "Conclusion\n",
      "Conclusion\n",
      "Building GPT 3\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "General knowledges\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla).\n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog]\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 35, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '19ab36f5-a4f1-4a42-99e9-9afde43770a0'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 36\n",
      "Content:\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 36, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7ab0e679-3095-408c-8761-5681fcdf7292'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 37\n",
      "Content:\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "|V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 37, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '79ecfc5d-7682-4601-acde-97ed7ebae941'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 38\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "N words\n",
      "|V| x N heads\n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step.\n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 38, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '96117f15-71e4-42b7-bd7e-942967b6b788'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 39\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "N words\n",
      "|V| x N heads\n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "|V| x N heads\n",
      "| Embeddings Dim|\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 39, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fa76d935-e820-4cb0-a7d0-2a386c8bb3c4'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 40\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph.\n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset.\n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 40, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '36b2aa73-63bf-42b1-b919-c720268f7a77'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 41\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 41, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '6557cdb8-8bd8-4496-bee7-eab6a46efaf7'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 42\n",
      "Content:\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "4 x 96\n",
      "4 x 96\n",
      "5\n",
      "Matmul\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 42, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '2d7e6446-c3de-4b04-aef0-30c79d6a36b4'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 43\n",
      "Content:\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "5\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 43, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5ea29042-9f6a-45e9-ad52-e5d5988e271f'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 44\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 44, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'e5319d26-f0fc-4433-be7a-0ce4295f6907'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 45\n",
      "Content:\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 45, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'fe2ccc25-e6bc-44e9-b0a4-ab627a6c022c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 46\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 46, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'babb96ec-0869-40ef-a9bc-6ae85c91af8e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 47\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.\n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 47, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '94723525-dd85-4f0a-bb94-5fe78b8cdd50'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 48\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 48, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '04791d6c-3be7-4e9d-bcf3-d0ec06b01aa4'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 49\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 49, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '083d1b9a-de88-4f7e-8056-ba43e436bebf'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 50\n",
      "Content:\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up\n",
      "projection’\n",
      "‘Down\n",
      "projection’\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 50, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3f62e8ad-b97e-402c-9f17-6776b034782e'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 51\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "5\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 51, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd927bdef-fc4a-40e3-939c-441a4e35e48c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 52\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "5\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 52, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '7b2d53b7-9cbd-401a-b47b-1c75e59a2530'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 53\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "4\n",
      "5\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "Am I a football star ?\n",
      "Am I a plural noun ?\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 53, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '636b5d03-526e-4845-b094-559739fdd805'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 54\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "4\n",
      "5\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "Am I a TV stuff ?\n",
      "Am I a singular noun ?\n",
      "Am I a math architecture ?\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 54, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '5e9f3916-024b-4e8e-9253-754ebcced980'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 55\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension\n",
      "…\n",
      "49 152\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 55, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a958f776-e563-4409-a1b3-d087c6ee3a1c'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 56\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 56, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9a7cdd00-f571-4d59-8729-5255e521257f'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 57\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 57, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'faef1f31-6b3e-4dbd-8ab0-140d19d0a765'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 58\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 58, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'a2e8727e-b779-4977-b27a-463c3bbaa460'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 59\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 59, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '26936863-578f-4226-89d3-7faf051d4454'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 60\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 60, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'af1676bf-2288-46e4-9a65-f6fee47ecccc'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 61\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 61, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '963b1a4b-74eb-42e9-b378-2d97f4ca0ccf'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 62\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence\n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 62, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ad3531da-6212-4d75-b564-be833cb8b1ef'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 63\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 63, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'ca3895be-2a17-416c-abb7-e0ba95e26570'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 64\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 64, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '194fcbf4-78b4-496c-981d-425dcd87046b'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 65\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 65, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'efdcec06-c798-483d-a1dc-620e4e970d93'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 66\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 66, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '804ca0a8-a504-436a-9534-ab1882a7b883'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 67\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 67, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '71e03e90-4164-46d0-a911-005ab1331ee2'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 68\n",
      "Content:\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 68, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1aa1df75-09bd-4b33-b6b1-ad92bcfbca51'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 69\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic\n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 69, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4d574433-f698-4ca1-b13c-a620cdc99a64'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 70\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 70, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '92ad2416-ca20-4b2b-b6b6-3a84d89ee558'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 71\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 71, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'f4367c45-bc34-4a9a-88f0-ccc959378355'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 72\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 72, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'cd8d0329-f84c-4dc9-80c1-20b3a630c6b0'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 73\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 73, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '1b10cf0a-3b67-4449-ab68-595ab0abf5d6'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 74\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 74, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '24222e51-dfb6-476c-8e5b-5c3af44d3a6f'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 75\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 75, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '91c9afe5-4eee-45cc-be36-49ebffee8965'}\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 76\n",
      "Content:\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:30:30.758126', 'page_number': 76, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '23be6d71-7d5b-470b-9e9f-52f967f449ca'}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to merge documents by page number\n",
    "def merge_documents_by_page(documents: list[Document]) -> list[Document]:\n",
    "    merged_documents: list[Document] = []\n",
    "    page_dict = {}\n",
    "\n",
    "    # Group documents by page number\n",
    "    for doc in documents:\n",
    "        page_number = doc.metadata.get('page_number')\n",
    "        if page_number is not None:\n",
    "            if page_number not in page_dict:\n",
    "                page_dict[page_number] = [doc]\n",
    "            else:\n",
    "                page_dict[page_number].append(doc)\n",
    "\n",
    "    # Merge documents for each page\n",
    "    for page_number, docs in page_dict.items():\n",
    "        if docs:\n",
    "            # Use the metadata of the first document in the group\n",
    "            merged_metadata = docs[0].metadata\n",
    "            # Concatenate the page content of all documents in the group\n",
    "            merged_content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            # Create a new Document with merged content and metadata\n",
    "            merged_documents.append(Document(\n",
    "                page_content=merged_content,\n",
    "                metadata=merged_metadata\n",
    "            ))\n",
    "\n",
    "    return merged_documents\n",
    "\n",
    "# Example: Assume documents are already extracted\n",
    "merged_documents = merge_documents_by_page(documents)\n",
    "print(merged_documents)\n",
    "# Print the merged documents\n",
    "for doc in merged_documents:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Page Number: {doc.metadata.get('page_number')}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\\n\")\n",
    "    print(f\"Metadata:\\n{doc.metadata}\\n\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Ingesting in Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ingest each merged_document in Cloud SQL.\n",
    "\n",
    "ALREADY DONE by teacher: \n",
    "- Create a Cloud SQL instance\n",
    "- Create a database in the instance\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Create a table in CloudSQL with you initials\n",
    "- Create the schema of the table\n",
    "- Ingest the data in the table\n",
    "\n",
    "\n",
    "Follow this [documentation](https://python.langchain.com/docs/integrations/vectorstores/google_cloud_sql_pg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Understand how to connect to Cloud SQL \n",
    "\n",
    "\n",
    "First we need to connect to Cloud SQL \n",
    "- Follow this [link](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy) to understand how it works\n",
    "\n",
    "Then be familiar ith the following PostgreSQL commands:\n",
    "```bash \n",
    "`psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=postgres\"` # to connect to the user `postgres`\n",
    "# the user we use is `students`\n",
    "# a password provided by the teacher is required\n",
    "`\\l` # to list all databases\n",
    "`\\c gen_ai_db` # to connect to the database `gen_ai_db`\n",
    "`\\dt` # to list all tables\n",
    "`\\d+ table_name` # to describe a table\n",
    "`SELECT * FROM table_name` # to select all rows from a table\n",
    "`\\du` # to list all users\n",
    "`\\q` # to quit\n",
    "`CREATE DATABASE db_name;` # to create a database\n",
    "`CREATE USER user_name WITH PASSWORD 'password';` # to create a user\n",
    "`GRANT ALL PRIVILEGES ON DATABASE db_name TO user_name;` # to grant all privileges to a user on a database\n",
    "`GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO user_name;` # to grant all privileges to a user on all tables in a schema\n",
    "`ALTER USER user_name WITH SUPERUSER;` # to grant superuser privileges to a user\n",
    "`DROP DATABASE db_name;` # to drop a database\n",
    "`DROP USER user_name;` # to drop a user\n",
    "`DROP TABLE table_name;` # to drop a table\n",
    "`REVOKE ALL PRIVILEGES ON DATABASE db_name FROM user_name;` # to revoke all privileges from a user on a database\n",
    "```\n",
    "\n",
    "When Cloud SQL Proxy is downloaded and the tutorial is followed. You should be connected to the instance. \n",
    "You can connect to the dabase as a user `students` with the password provided by the teacher.\n",
    "  - `psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=students\"`\n",
    "  - Enter the password provided by the teacher\n",
    "Try to create a table `initial_tests_table` with the following schema:\n",
    "  - `CREATE TABLE initial_tests_table (id SERIAL PRIMARY KEY, document TEXT, page_number INT, title TEXT, author TEXT, date TEXT);`\n",
    "  - `\\dt` to check if the table has been created\n",
    "  - `\\d+ initial_tests_table` to check the schema of the table\n",
    "  - `DROP TABLE initial_tests_table;` to drop the table\n",
    "  - `\\q` to quit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-cloud-sql-pg langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import PROJECT_ID, REGION, INSTANCE, DATABASE, DB_USER\n",
    "DB_PASSWORD = os.environ[\"DB_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"meriam_in_table\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "engine = PostgresEngine.from_instance(\n",
    "    project_id=PROJECT_ID,\n",
    "    instance=INSTANCE,\n",
    "    region=REGION,\n",
    "    database=DATABASE,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already created\n"
     ]
    }
   ],
   "source": [
    "# Create a table in the PostgreSQL database with the required columns\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "try:\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=TABLE_NAME, \n",
    "        vector_size=768,\n",
    "    )\n",
    "except ProgrammingError:\n",
    "    print(\"Table already created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute \\d+ [YOUR_INITIALS]_table in the psql shell to check the schema of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Create an embedding to convert your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\",\n",
    "    project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresVectorStore\n",
    "\n",
    "vector_store = PostgresVectorStore.create_sync(  \n",
    "    engine=engine,\n",
    "    table_name=TABLE_NAME,\n",
    "    embedding_service=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0ac93dc4-0395-4355-b85c-ee9710807bb3',\n",
       " 'cd212bcc-61c2-41b9-abfe-89ade65e704c',\n",
       " '77f7706a-9a93-4248-89a0-74d8db02baad',\n",
       " '6e6f012e-72f9-4412-bfbe-f883f0980b06',\n",
       " '4cf2835b-db45-4723-9f47-0b5a4f48c537',\n",
       " 'b833b0e8-df45-47ec-82c6-ced77b53739d',\n",
       " '2e1e03ce-3828-4977-901d-8ac1092aafaa',\n",
       " 'c41fc8d3-ed27-42a0-b1d7-cfc199f1a2d3',\n",
       " '2c181663-258c-4489-93f0-ee1064dc8791',\n",
       " '261a1f35-524d-4e59-a8ab-6809cef17201',\n",
       " '6eaaa5f0-3e15-4905-8a19-4c662efd1ffc',\n",
       " '90db450a-6df1-4d65-8956-d0c292b8955f',\n",
       " 'fbe3c6ab-4c0b-496a-9e6d-1ffe678b790a',\n",
       " 'f87a5753-941d-4738-a13f-b0801122ed10',\n",
       " '33210c6c-fd9b-4ead-96f3-5395303785a1',\n",
       " 'd0b9300a-1b49-436b-ac91-cc369ed6c68a',\n",
       " '8219242b-30c5-41e5-922e-272d84419c43',\n",
       " 'dc8ec39b-6594-4811-958a-9292ba60e06e',\n",
       " '0619635c-d762-4c8b-b160-7431fa7e525d',\n",
       " 'dd8478d7-08bb-484f-aeba-20271364725e',\n",
       " '4ae04af7-c2c3-4e22-bf5b-bd19316259b8',\n",
       " '0e3286ac-2459-4bac-bfca-c950f7f44616',\n",
       " '06ec101d-a56c-461c-ae0a-07bfbf4157a5',\n",
       " '43231224-1e38-4f45-8d2e-dddf819c3d26',\n",
       " 'e6d6c45d-46e5-4887-a20a-b2df0d96e186',\n",
       " 'b9a3f405-97f2-4c0a-982f-b956ce4f18a7',\n",
       " '278ea4f7-8f3b-4e7a-9f44-1706a404ab8f',\n",
       " 'f05dcc3d-d2e6-4e8e-91e0-95b4d8c72d99',\n",
       " '0221c75d-3b82-4efd-bc80-d961eee4ab03',\n",
       " '80a66235-f965-4823-9d48-c4e83430a686',\n",
       " '0c048330-a814-4d64-8f77-47f1a82825df',\n",
       " '64ab2918-23d1-4a4c-b38e-86217ea75c6d',\n",
       " '9e6165a5-9f99-4103-b7d2-8bfd84bb1c50',\n",
       " 'f80e9508-a61e-40d0-8ac9-83167657f8fb',\n",
       " '1c837b13-fdd7-4362-8666-aa6732d0cd07',\n",
       " '8a9e272d-a6c0-4b6d-bdc9-2293fd916c69',\n",
       " '4fd524f1-6f90-4040-852d-83bd3d0922f2',\n",
       " '18d291c3-741d-4073-98a2-cfbd5db6d023',\n",
       " '5c68974a-8370-41bf-984e-9d162b076129',\n",
       " 'a956ae4e-00b1-4fd0-8f78-a83d97515469',\n",
       " '6542faf3-3cc5-498a-b1a0-3c2c5bef65fa',\n",
       " '76bef335-0d75-40d0-a069-abe9c6477925',\n",
       " '40b00e7b-f7e6-456b-b159-685452805b17',\n",
       " '6dc28b18-8512-453d-a80f-d2019b987032',\n",
       " '431cea94-901f-4e38-ba89-317e4e2a8592',\n",
       " 'da257a94-1f09-4bf9-8aff-3393936f5162',\n",
       " 'a36f2a8e-8608-40fa-bd63-953c7f648aa1',\n",
       " '95500c47-2f3e-41c6-99e3-40ecaae1d200',\n",
       " 'b44c99af-4890-4ece-974f-2f63fefaa726',\n",
       " '1b0f3820-e8f5-401b-be41-d8d73406f332',\n",
       " 'a603f40f-769f-42fb-9a19-09b01792c688',\n",
       " 'a6321699-960c-4aad-9c42-10e3ff6a617d',\n",
       " '3dd28a50-7397-45b1-9fde-b0e7a03f8ffe',\n",
       " '41fdda03-843d-40b6-9fc3-67aa24ceec35',\n",
       " 'dc105751-3c92-482e-90ae-fff2fe41094e',\n",
       " 'c3f5577c-431c-4392-8e26-e82147377c4a',\n",
       " 'f443d2ec-1726-401b-bc59-d7c8fc50a904',\n",
       " '798540c5-dbfa-4f08-ac71-b2cb389fa8aa',\n",
       " '2ab53c3c-b07d-4ec0-8eb9-83076f2498f9',\n",
       " '6c9dd75a-cf11-4893-8f6d-d52064f39ab9',\n",
       " '986c69b3-240c-4438-9570-ffc75f13ab5d',\n",
       " '1417ffdb-46c1-4eb5-9142-40e5b0e610d6',\n",
       " 'e9e15dcd-42d6-4fca-9928-fae17c6ca572',\n",
       " '62e53f59-423b-4c12-860d-018aa1c736cd',\n",
       " '5ccf9bc8-cc42-4ebb-83c5-aef1e5031983',\n",
       " 'baa09d79-8141-4261-8aa0-5070a0a2046c',\n",
       " '8808868a-a859-4fd9-97dc-db3fefa6aeab',\n",
       " 'af0d9f15-b0c7-43df-a9f4-d924be02ba40',\n",
       " 'dac25992-714a-49d1-9896-f32259bb3b7a',\n",
       " '62ca8288-1c67-4c00-a890-a8737bfeaa25',\n",
       " '0726fce3-4602-44a0-a67f-a07ddd8ee258',\n",
       " '4273590c-2b5e-471e-8b2f-d9ccb56b83ef',\n",
       " '8aca7f77-7d83-4bc6-8e43-9a86b6f6f95f',\n",
       " '0eab2689-6070-4951-aa34-fef4f2716114',\n",
       " 'bddcaa4c-cfe6-407e-ae54-38079d4f9a59',\n",
       " '4ad2d02a-9851-4dfd-af03-294507727210']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector_store.add_documents(merged_documents)\n",
    "# Excute only once this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Perform a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to train a Large Language Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    ")\n",
    "\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Content:  Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '272f400b-bbe6-4fed-b0f9-a16e75796499'}\n",
      "--------------------------------------------------\n",
      "Content:  ‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "Pre training phase\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval\n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '83b9389e-5739-4d93-9c8f-077671560248'}\n",
      "--------------------------------------------------\n",
      "Content:  ‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "Post training phase\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd5d39baa-af27-468c-8360-875ae9367b34'}\n",
      "--------------------------------------------------\n",
      "Content:  Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog]\n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ?\n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\n",
      "-3.11\n",
      "2.422\n",
      "7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval\n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9b70f6f4-6f3e-4dd6-a6fe-0c1ffc6ee545'}\n",
      "[Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '272f400b-bbe6-4fed-b0f9-a16e75796499'}, page_content='Training process\\n‹#›\\nI.A.7 Training Process\\nSteps\\nFind scaling recipes (example: learning rate decrease if the size of the model increase)\\nTune hyper parameters on small models of differents size\\nChoose the best models among the smallest ones\\nTrain the biggest model with the\\nStanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\\nQ. Should I use Transformers or LSTM ?\\n‹#›\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.43\\n-3.2\\n..\\n..\\n3.32\\n2.11\\n-4.22\\n..\\n..\\n1.12\\n3.11\\n-4.22\\n..\\n..\\n4.98\\nQuery\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\n2.11\\n-4.22\\n..\\n..\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nAre we talking about TV ?\\nDo I mean Allocation de Retour à l’Emploi ?\\nAm I a superstar ?\\n…\\nQuery: What am I looking for ?\\n|E| : Embedding (1, 12 288)\\n|WQ|: Query matrix (12 288, 128)\\nIII.2. Information retrieval\\n‹#›\\nDeep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\\nLin & Al, 2023, How to Train Your DRAGON\\nDense retriever\\nProgressive Data Augmentation strategy for training sampling very difficult negatives\\nYasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '83b9389e-5739-4d93-9c8f-077671560248'}, page_content='‹#›\\nI.A Pretraining Large Language Model\\nPre training phase\\nA. Pretraining a Large Language Model\\nIntroduction\\nCross entropy loss\\nTokenization\\nEvaluation\\nData preprocessing\\nScaling laws\\nTraining process\\nCost and optimization\\n‹#›\\nII. Transformers\\n‹#›\\nII.B. Transformers Architecture\\n‹#›\\nIII. Retrieval Augmented Generation\\nBasic Architecture\\nInformation retrieval\\nVectorstore & Search optimization\\nRAG Techniques\\nEvaluation\\nMultimodal RAG\\nSOTA RAG architectures'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'd5d39baa-af27-468c-8360-875ae9367b34'}, page_content='‹#›\\nI.B Fine tuning Large Language Model\\nPost training phase\\nB. Fine tuning a Large Language Model\\nSupervised Fine Tuning\\nRLHF\\nReward model\\nPPO & DPO\\nEvaluation & Challenges\\n‹#›\\nII.A.3 RNN\\nRNN limitations: Exploding / Vanishing gradient problem\\n“transformers”\\n“are”\\n“encoder”\\n“decoder”\\n“The”\\n?\\n“decoder”\\nFeed forward + Softmax model\\nEmbedding\\nTransformers\\n‹#›\\nII.B.1 Self Attention Mechanism\\nare\\nencoder\\ndecoder\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n3.11\\n-4.22\\n4.98\\nQuery\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\n2.11\\n-4.22\\n5.93\\nWQ\\nE1\\nE2\\nE3\\nQ1\\nQ2\\nQ3\\nQ4\\nE4\\nWK\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\n2.11\\n-4.22\\n1.12\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nIII.3. Vectorstore & Search optimization\\n‹#›\\nEfficient similarity search\\nAnnouncing ScaNN: Efficient Vector Similarity Search\\nScalable Nearest Neighbors (ScaNN) - Google\\nFacebook AI Similarity Search (FAISS)\\nHierarchical Navigable Small Worlds (HNSW)\\nDefinition:\\nScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\\nHierarchical Navigable Small Worlds (HNSW) [Blog]'), Document(metadata={'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-12-09T20:29:53.760732', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '9b70f6f4-6f3e-4dd6-a6fe-0c1ffc6ee545'}, page_content='Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\\n‹#›\\nI.A.8 Cost & Optimizations\\nDisplay all the models with same amount of compute (left figure)\\nSelect the best model for each compute in terms of training loss (middle & right figure)\\nExtrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\\nOptimal model and data size\\n‹#›\\nII.A.3 RNN\\nRecurrent Neural Networks (Seq2seq model)\\nBriefly describe the architecture of a RNN [Blog]\\nEach word is given sequentially (xt)\\nAn intern memory is updated after each word  (ht)\\nA context is provided with this memory\\n‹#›\\nII.B.1 Self Attention Mechanism\\nWK\\nMight be a TV object  of a model\\n…\\nI am a noun, starting the sentence\\nI am a verb\\nKey: What do I have ?\\n|E| : Embedding (1, 12 288)\\n|WK|: Query matrix (12 288, 128)\\nEmbedding\\n2.11\\n-4.22\\n5.93\\n2.43\\n-3.2\\n3.32\\n2.11\\n-4.22\\n1.12\\n3.11\\n-4.22\\n4.98\\nE1\\nE2\\nE3\\nE4\\nK1\\nK2\\nK3\\nK4\\nKEYS\\n-3.11\\n2.422\\n7.93\\n2.11\\n-3.22\\n5.93\\n2.11\\n-1.2\\n5.93\\n-21\\n42.21.2\\n1.23\\nIII.2. Information retrieval\\n‹#›\\nBest retrieval methods\\nLeaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475')]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content: \", doc.page_content)\n",
    "    print(\"Metadata: \", doc.metadata)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You have successfully ingested the data in Cloud SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
