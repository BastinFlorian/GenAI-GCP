{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we’ll walk through the process of interacting with a Google Cloud Storage (GCS) bucket named dauphine-bucket, specifically focusing on the data directory within the bucket. We’ll cover how to:\n",
    "\n",
    "- List all files in the bucket’s data directory.\n",
    "- Retrieve information about a specific file.\n",
    "- Read files using the Unstructured library.\n",
    "- Visualize the extracted documents with LangChain.\n",
    "\n",
    "This guide is intended for users who are familiar with Python and basic cloud storage concepts.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following:\n",
    "\n",
    "- Python 3.x installed on your system.\n",
    "- Access to the GCP bucket dauphine-bucket/data with the necessary permissions.\n",
    "- Google Cloud SDK installed and authenticated. You can authenticate by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=QydxrDHW5SusgPNzsplgQe8IMJ03mR&access_type=offline&code_challenge=Lpcsly_6W0stkBu1zONsUFwNgyx7Su1ppodbxvko5Pk&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [krimismr@gmail.com].\n",
      "Your current project is [dauphine-437611].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python libraries installed:\n",
    "- google-cloud-storage\n",
    "- unstructured\n",
    "- langchain\n",
    "\n",
    "To know more about the libraries, you can visit the following links:\n",
    "- [google-cloud-storage](https://googleapis.dev/python/storage/latest/index.html)\n",
    "- [unstructured](https://docs.unstructured.io/examplecode/codesamples/oss/vector-database)\n",
    "- [langchain](https://langchain.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q google-cloud-storage unstructured langchain python-magic sqlalchemy langchain_google_cloud_sql_pg\n",
    "%pip install -q \"unstructured[pptx]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Listing and loading files from a GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Listing Files in the GCP Bucket\n",
    "\n",
    "Explanation:\n",
    "\n",
    "To interact with a GCS bucket, we’ll use the google-cloud-storage library. We’ll initialize a client, access the bucket, and list all the files within the data directory.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'dauphine-bucket/data':\n",
      "data/\n",
      "data/1 - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Access the bucket\n",
    "bucket_name = 'dauphine-bucket'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# List all files in the 'data' directory\n",
    "blobs = bucket.list_blobs(prefix='data/')\n",
    "\n",
    "print(\"Files in 'dauphine-bucket/data':\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "Running this code will display all the file paths within the data directory of the bucket. The prefix='data/' parameter ensures we only get files from that specific directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Getting Information About One File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Sometimes, you may need detailed information about a specific file, such as its size, content type, or the last time it was updated. We’ll retrieve this metadata for a chosen file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for 'data/1 - Gen AI - Dauphine Tunis.pptx':\n",
      "Size: 6724048 bytes\n",
      "Content Type: application/vnd.openxmlformats-officedocument.presentationml.presentation\n",
      "Updated On: 2024-10-07 09:52:30.256000+00:00\n",
      "Blob name: data/1 - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path (replace with an actual file from your bucket)\n",
    "file_path = 'data/1 - Gen AI - Dauphine Tunis.pptx'\n",
    "\n",
    "# Get the blob object\n",
    "blob = bucket.get_blob(file_path)\n",
    "\n",
    "if blob:\n",
    "    print(f\"Information for '{file_path}':\")\n",
    "    print(f\"Size: {blob.size} bytes\")\n",
    "    print(f\"Content Type: {blob.content_type}\")\n",
    "    print(f\"Updated On: {blob.updated}\")\n",
    "    print(f\"Blob name: {blob.name}\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found in the bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "This code will output metadata about the specified file. Make sure to replace 'data/your_file.ext' with the actual file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Reading Files with Unstructured\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Unstructured library allows us to parse and process unstructured data from various file formats. We’ll download a file from the bucket and use Unstructured to read and extract its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from pptx import Presentation\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Specify download directory\n",
    "DOWNLOADED_LOCAL_DIRECTORY = os.path.abspath(\"./downloaded_files\")\n",
    "os.makedirs(DOWNLOADED_LOCAL_DIRECTORY, exist_ok=True)\n",
    "\n",
    "def download_file_from_bucket(bucket: Bucket, file_path: str) -> str:\n",
    "    # Download the file locally\n",
    "    blob = bucket.blob(file_path)\n",
    "    local_file_name = os.path.basename(file_path)\n",
    "    local_filepath = os.path.join(DOWNLOADED_LOCAL_DIRECTORY, local_file_name)\n",
    "    blob.download_to_filename(local_filepath)\n",
    "    print(f\"Downloaded '{file_path}' to '{local_filepath}'\")\n",
    "    return local_filepath\n",
    "\n",
    "def read_pptx(filepath: str) -> str:\n",
    "    prs = Presentation(filepath)\n",
    "    text = \"\"\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_file_from_local(local_filepath: str) -> list:\n",
    "    if local_filepath.endswith(\".pptx\"):\n",
    "        text = read_pptx(local_filepath)\n",
    "        return [Document(page_content=text)]\n",
    "    else:\n",
    "        loader = UnstructuredLoader(local_filepath)\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing 'data/': [Errno 2] No such file or directory: 'c:\\\\Users\\\\USER\\\\Desktop\\\\GENERATIVE AI DAUPHINE\\\\GenAI-GCP\\\\TPs\\\\tp_4\\\\downloaded_files\\\\'\n",
      "Downloaded 'data/1 - Gen AI - Dauphine Tunis.pptx' to 'c:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-GCP\\TPs\\tp_4\\downloaded_files\\1 - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx' to 'c:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-GCP\\TPs\\tp_4\\downloaded_files\\2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx' to 'c:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-GCP\\TPs\\tp_4\\downloaded_files\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "Downloaded 'data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx' to 'c:\\Users\\USER\\Desktop\\GENERATIVE AI DAUPHINE\\GenAI-GCP\\TPs\\tp_4\\downloaded_files\\3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx'\n"
     ]
    }
   ],
   "source": [
    "# Load all the\n",
    "blobs = list(bucket.list_blobs(prefix='data/'))\n",
    "documents: list[Document] = []\n",
    "if blobs:\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            local_filepath = download_file_from_bucket(bucket, blob.name)\n",
    "            documents.extend(read_file_from_local(local_filepath))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{blob.name}': {e}\")\n",
    "else:\n",
    "    print(\"No files found in the 'data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Visualizing the First Documents Extracted with LangChain\n",
    "\n",
    "Explanation:\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. We’ll use it to load and visualize the documents extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "\n",
      "Metadata:\n",
      "{}\n",
      "\n",
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "\n",
      "Metadata:\n",
      "{}\n",
      "\n",
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "‹#›\n",
      "II.B Introduction\n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "\n",
      "‹#›\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents[:3]:\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Join extracted document by page\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- The text extraction block is uninformative because very small text blocks are extracted from the document.\n",
    "- We can join the extracted text by page to get a more meaningful output.\n",
    "- A metadata with the 'page_number' can be helpful\n",
    "- The other metadatas need to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Page Number: 1\n",
      "Content:\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Generative AI with LLM\n",
      "Florian Bastin\n",
      "👨🏼‍🎓 Master MASH - Université PSL\n",
      "👨🏼‍💻 LLM Engineer @OctoTechnology\n",
      "Le Monde, Casino, Channel, Club Med, Pernod Ricard, Suez\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000000'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 2\n",
      "Content:\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000001'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 3\n",
      "Content:\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Autoregressive language models:\n",
      "\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Language Models: probability distribution over a sequence of words p(x1, … xn)\n",
      "\t\t\t\n",
      "P(Transformers, are, encoder, decoder, models) = 0.01\n",
      "\n",
      "P(Transformers, are, are, encoder, decoder, models) = 0.0001  \tSyntactic knowledge\n",
      "\n",
      "P(Transformers, are, decoder, models) = 0.001 \tSemantic knowledge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(Transformers, are, encoder, decoder, models) = P(Transformers)\n",
      "     . P(Transformers are | Transformers)\n",
      "     … \n",
      "     . P(models | Transformers, are, encoder, decoder)\n",
      "\n",
      "‹#›\n",
      "II. Transformers\n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "\n",
      "B. Transformers \n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "III. Introduction\n",
      "‹#›\n",
      "LLM vs RAG\n",
      "LLM\n",
      "RAG\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000002'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 4\n",
      "Content:\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "\n",
      "\n",
      "\n",
      "RAG Definition\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "\n",
      "\n",
      "\n",
      "RAG Definition\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "\n",
      "\n",
      "\n",
      "RAG Definition\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "\n",
      "\n",
      "\n",
      "RAG Definition\n",
      "Language modelling\n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "Model\n",
      "The goal is to generate token by token \n",
      "\n",
      "The steps for generation:\n",
      "\n",
      "Tokenize \n",
      "Feed the model with the token \n",
      "Predict the probability of each possible token \n",
      "Sample from the likelihood\t\t\t\n",
      "Detokenize\t\n",
      "Transformers are encoder \n",
      "9140           388     527    24592\n",
      "\n",
      "8832\n",
      "Decoding\n",
      "Polo Club, Transformer Explainer [Blog]\n",
      "\n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "The Story of AI Evolution: Before ML Era to Transformers, GPT-3 and Beyond [LinkedIn] \n",
      "A. Before Transformers \n",
      "N grams\n",
      "Embeddings\n",
      "RNN \n",
      "LSTM\n",
      "‹#›\n",
      "II.B Introduction\n",
      "Bahdanau & Al, 2016, Neural Machine Translation by Jointly Learning to Align and Translate\n",
      "III. Introduction\n",
      "‹#›\n",
      "Definition: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based and generation-based models. It enhances the capabilities of language models by providing them with access to external knowledge bases or documents during the generation process. This allows the model to generate more accurate and up-to-date information by retrieving relevant data instead of relying solely on its internal parameters.\n",
      "\n",
      "Benefits:\n",
      "\t•\tProduces more informed and factual responses.\n",
      "\t•\tCan handle queries about recent events not present in the training data.\n",
      "\t•\tReduces hallucinations common in language models.\n",
      "\n",
      "\n",
      "\n",
      "RAG Definition\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000003'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 5\n",
      "Content:\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "How the model works ? \n",
      "‹#›\n",
      "I.A.1 Introduction\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) \n",
      "words into a network\n",
      "Get vector representation of context from \n",
      "the network\n",
      "From this vector representation, predict a\n",
      " probability distribution for the next token.\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A. Before Transformers \n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO \n",
      "Step 1: Document ingestion\n",
      "Step 2: Contextualized answering\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000004'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 6\n",
      "Content:\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Cross entropy loss\n",
      "‹#›\n",
      "I.A.2 Cross Entropy Loss\n",
      "The general model pipeline is as follows:\n",
      "\n",
      "Feed word embedding for previous (context) words into a network\n",
      "Get vector representation of context from the network\n",
      "From this vector representation, predict a probability distribution for the next token.\n",
      "\n",
      "\n",
      "\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "\n",
      "Maximizing the likelihood is equivalent to minimizing the cross entropy loss:\n",
      "‹#›\n",
      "II.A.1 N Grams\n",
      "\n",
      "N Grams\n",
      "\n",
      "Input text: \n",
      "\n",
      "To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it …\n",
      "The chain rule of probability:  p(x1, x,2, …, xn) = p(x1) p(x2| x1) p(x3| x2,x1) …\n",
      "N-gram generator \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.B Introduction\n",
      "\n",
      "\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000005'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 7\n",
      "Content:\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "How to split ? \n",
      "Word ? \n",
      "Letter ?\n",
      "How to split and get token ? \n",
      "Byte-Pair Encoding (BPE) process:\n",
      "1. Use a big corpus of text\n",
      "2. Consider first one token per character\n",
      "3. Merge commons pairs \n",
      "4. Stop when a you cannot merge or the Vocab size is reached\n",
      "This GIF is generated from GPT o1 using the following prompt\n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B. Introduction\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "RAG Architecture\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000006'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 8\n",
      "Content:\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "Tokenization\n",
      "‹#›\n",
      "I.A.3 Tokenization\n",
      "\n",
      "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. \n",
      "\n",
      "Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses spaCy and ftfy, to count the frequency of each word in the training corpus.\n",
      "\n",
      "\n",
      "Q. What is the problem with numbers as tokens ?\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "0\n",
      "0\n",
      "0\n",
      ":\n",
      "1\n",
      "0\n",
      "0\n",
      ":\n",
      "0\n",
      "0\n",
      "-0.81\n",
      ":\n",
      ":\n",
      ":\n",
      " 4.56\n",
      ":\n",
      ":\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Index: 1280\n",
      "Embedding model\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| \n",
      "\n",
      "\n",
      "One-hot encoding\n",
      "Dim = |Vocab Size| \n",
      "\n",
      "\n",
      "From One-hot encoding to Word Embedding \n",
      "Word Embedding \n",
      "One-hot encoding \n",
      "“transformers”\n",
      "II.B. Introduction\n",
      "‹#›\n",
      "GPT 3\n",
      "Each word is generated one by one\n",
      "Only the decoder part is used\n",
      "III.1. Basic Architecture\n",
      "‹#›\n",
      "How to ?\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000007'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 9\n",
      "Content:\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Evaluation \n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Instead of cross-entropy, it is more common to report its transformation called perplexity:\n",
      "A better model has higher log-likelihood and lower perplexity.\n",
      "Perplexity = 10 ≃ The model hesitates between 10 tokens\n",
      "To better understand which values we can expect, let's evaluate the best and the worst possible perplexities.\n",
      "the best perplexity is 1:\u000bIf our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1.\n",
      "the worst perplexity is |V|:\u000bIn the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability 1/|V|\n",
      "\n",
      "\n",
      "\n",
      "Q. Prove that the worst perplexity is |V|\n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.2 Embeddings\n",
      "\n",
      "Word Embedding (Word2Vec, GloVe, BERT, ELMo)\n",
      "Represent each word as a vector of numbers\n",
      "Convert a discrete representation to continuous, allowing:\n",
      "More ‘fine-grained’ representations of words\n",
      "Useful computations such as cosine / euclidean distances\n",
      "Visualization and mapping of words\n",
      "Tomas Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space \n",
      "II.B. Introduction\n",
      "‹#›\n",
      "Translation model (FR -> EN example)\n",
      "The sentence to translate given to the encoder\n",
      "Each generated word added to the decoder \n",
      "Jay Allamar,  2019, The Illustrated Transformer\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "TF-IDF\n",
      "Text Search using TF-IDF and Elasticsearch\n",
      "Given a query Q, containing keywords {q1, …, qn}, the BM25 score of a document D is:\n",
      "\n",
      "BM 25\n",
      "f(qi,D) is the number of times that the keyword qi occurs in the document D, \n",
      "|D| is the length of the document D in words\n",
      "avgdl is the average document length in the text collection from which documents are drawn. \n",
      "K1 and b are free parameters, usually chosen, in absence of an advanced optimization, as K1∈[1.2,2.0] and b=0.75\n",
      "N is the total number of documents in the collection, and \n",
      "n(qi) s the number of documents containing qi\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000008'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 10\n",
      "Content:\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "Perplexity depends on vocabulary size, ie tokenization method: not used anymore\n",
      "We now use evaluation Datasets\n",
      "IFEval\n",
      "BBH\n",
      "MMLU-Pro\n",
      "Math\n",
      "…\n",
      "Different fields (medical, math, physics, …)\n",
      "covered in the Dataset to provide diversity\n",
      "Hugging Face, Open LLM Leaderboard \n",
      "Evaluation Datasets\n",
      "Hugging Face LLM Leaderboard\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "Lot of new knowledges in this paper:\n",
      "No more RNN, only attention \n",
      "MLP layers and Attention\n",
      "Positional encodings\n",
      "ResNet structure\n",
      "Parallelism with Multi Head Attention\n",
      "\n",
      "\n",
      "II.B Introduction\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Cosine Similarity\n",
      "Euclidean distance\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000009'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 11\n",
      "Content:\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Evaluation\n",
      "‹#›\n",
      "I.A.4 Evaluation\n",
      "\n",
      "Evaluation process: \n",
      "Get the likelihood of each answer\n",
      "Ask the model to answer A) B) C) D)\n",
      "\n",
      "BIG-Bench Hard [Github]\n",
      "Q. If the model is trained of the whole internet, how could it be contaminated? \n",
      "Lena Voita, Language Modeling [Blog] \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "Semantic representation\n",
      "Dim = |Chosen embedding size| = 100\n",
      "\n",
      "\n",
      "How to give a sentence to a model ? \n",
      "\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "Embedding model\n",
      "“transformers”\n",
      "-0.02\n",
      " 2.36\n",
      ":\n",
      "-1.12\n",
      "3.13\n",
      "Embedding model\n",
      "“are”\n",
      "\n",
      "D\n",
      "D\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "-0.81\n",
      " 4.56\n",
      ":\n",
      "-4.35\n",
      "2.21\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "D\n",
      "\n",
      "1\n",
      "⛔\n",
      "2 ≠1\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\n",
      "Maximal Marginal Relevance\n",
      "The goal of this metric is to retrieve dissimilar documents and increase diversity\n",
      "\n",
      "D is the set of all candidate documents, R is the set of already selected documents, q is the query\n",
      "Sim1 is the similarity function between a document and the query\n",
      "Sim2 is the similarity function between two documents.  \n",
      "di and  dj are documents in D and R respectively\n",
      "\n",
      "The parameter λ (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000010'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 12\n",
      "Content:\n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse vs Dense retrieval \n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse vs Dense retrieval \n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse vs Dense retrieval \n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse vs Dense retrieval \n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "Preprocessing the Data\n",
      "‹#›\n",
      "I.A.5 Data Preprocessing\n",
      "• Idea: use all of the clean internet\n",
      "• Note: internet is dirty & not representative of what we want. \n",
      "\n",
      "Practice:\n",
      "\n",
      "1. Download all of internet. Common crawl: 250 billion pages, > 1PB (>1e6 GB)\n",
      "2. Text extraction from HTML (challenges: math, boilerplate)\n",
      "3. Filter undesirable content (e.g. NSFW, harmful content, PII)\n",
      "4. Deduplicates (url/document/line). E.g. all the headers/footers/menu in forums are always same\n",
      "5. Heuristic filtering. Remove low quality documents (e.g. # words, word length, outlier tokens, dirty tokens)\n",
      "6. Model based filtering. Predict if page could be references by Wikipedia.\n",
      "7. Data mix. Classify data categories (code/books/entertainment). Reweight domains using scaling\n",
      "laws to get high downstream performance.\n",
      "\n",
      "At the end of training, overfit the model on very quality data\n",
      "\n",
      "Hugging Face, LLM Training Dataset \n",
      "HTML page example\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Seq2seq model\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse vs Dense retrieval \n",
      "Sparse Retrieval (TF IDF, BM 25, …) are methods to retrieve similar documents based on keywords only. \n",
      "Dense Retrieval (Cos Sim, Euclidean distance) allows to retrieve document using semantic embedding representation of documents and query. \n",
      "\n",
      "Hybrid Search is a method involving both sparse and dense retrievers to provide both advantages of the two approaches\n",
      "Sparse embedding (lots of 0)\n",
      "Q. If I want to retrieve document based on the user query ‘LeCun Meta’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What are the most wonderful shots of Lebron James ?’, what kind of retriever do I use ? \n",
      "Q. If I want to retrieve document based on the user query ‘What is the capital city of the biggest city in the world ?’, what kind of retriever do I use ? \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000011'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 13\n",
      "Content:\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Scaling laws\n",
      "‹#›\n",
      "I.A.6 Scaling Laws\n",
      "More ressources, more data and bigger models -> better models\n",
      "\n",
      "\n",
      "Jared Kaplan & Al, 2020, Scaling Laws for Neural Language Models \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Sequential Model)\n",
      "Advantages:\n",
      "Can learn from context of previous word\n",
      "Self supervised learning model\n",
      "\n",
      "\n",
      "Problems:\n",
      "Sequential model \n",
      "Very short term memory \n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Attention mechanism\n",
      "3 components: \n",
      "\n",
      "Query: What am I looking for ? \n",
      "Key: What do I have ?\n",
      "Value: What do I reveal to others ?\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Sparse Lexical and Expansion (SPLADE)\n",
      "\n",
      "SPLADE for Sparse Vector Search Explained\n",
      "Vector database are super efficient compare to Splade at the moment\n",
      "With sparse methods, you cannot get synonyms from a words.\n",
      "\n",
      "SPLADE:\n",
      "Use Bert to get similar words like synonyms\n",
      "Provide these synonyms to a sparse methods \n",
      "\n",
      "Formal & Al, 2021, SPLADE V2\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000012'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 14\n",
      "Content:\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000013'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 15\n",
      "Content:\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000014'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 16\n",
      "Content:\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "\n",
      "Vector DB Comparison\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "\n",
      "Vector DB Comparison\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "\n",
      "Vector DB Comparison\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "\n",
      "Vector DB Comparison\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "\n",
      "LLAMA 3 400B cost approx. $80m\n",
      "\n",
      "Carbon emitted approx. 2K tickets Tunis - New York\n",
      " \n",
      "How much it costs ?\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer [Youtube]\n",
      "Optimizing the loss w.r.t weights: \n",
      "D. Barack Ore, 2020, The Exploding and Vanishing Gradients Problem in Time Series \n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "E4\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "…\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "No we are not because I am a Transformer\n",
      "You should be a verb because I am a noun\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Vector Database \n",
      "Definition: A vector database is a specialized database designed to store, manage, and query high-dimensional vector embeddings of data such as text, images, or other content types. \n",
      "These embeddings are numerical representations produced by machine learning models that capture the semantic meaning of the data.\n",
      "\n",
      "Vector DB Comparison\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000015'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 17\n",
      "Content:\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "\n",
      "\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "\n",
      "\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "\n",
      "\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "\n",
      "\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "‹#›\n",
      "I.B Fine tuning Large Language Model\n",
      "\n",
      "Post training phase\n",
      "\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "?\n",
      "“decoder”\n",
      "Feed forward + Softmax model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Efficient similarity search\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "\n",
      "\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000016'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 18\n",
      "Content:\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "“GPT-3 models aren’t trained to follow user instructions. \n",
      "Open AI Instruct GPT models (highlighted) generate much more helpful outputs in response to user instructions.”\n",
      " \n",
      "How to get a user assistant ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "WK\n",
      "Embedding\n",
      "KEYS\t\t\n",
      "K1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reciprocal Rank Fusion (RRF)\n",
      "Definition:\n",
      "RRF allows to merge results of different retrievers\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000017'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 19\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "\n",
      "\n",
      "\n",
      "Bi Encoder vs Cross Encoder\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "\n",
      "\n",
      "\n",
      "Bi Encoder vs Cross Encoder\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "\n",
      "\n",
      "\n",
      "Bi Encoder vs Cross Encoder\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "\n",
      "\n",
      "\n",
      "Bi Encoder vs Cross Encoder\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a user assistant ? \n",
      "\n",
      "Conversational Agent\n",
      "ChatGPT\n",
      "\n",
      "Pretrained Large \n",
      "Language\n",
      "Model\n",
      "\n",
      "\n",
      "Post training: Alignement\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "RNN Types of architectures\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "StatQuest with Josh Starmer Youtube\n",
      "“Models” ?\n",
      "Praveen Raj, 2023, Understanding Recurrent Neural Networks (RNN) — NLP\n",
      "Q. Which one fit our use case ? \n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "\n",
      "The sum of each column is 1 \n",
      "III.3. Vectorstore & Search optimization\n",
      "‹#›\n",
      "Reranker\n",
      "\n",
      "A reranker ranks retrieved documents after a first similarity search\n",
      "\n",
      "Reranker type:\n",
      "Cross-Encoders\n",
      "Neural Rerankers\n",
      "\n",
      "Benefits of Using a Reranker:\n",
      "Increased Accuracy: Improves the likelihood that the most relevant information is used in generating the response.\n",
      "Better Contextual Understanding: Helps the system understand subtle nuances in the query.\n",
      "\n",
      "Challenges:\n",
      "Computational Overhead: Additional processing can increase response time.\n",
      "Resource Intensive: Advanced models require significant computational resources.\n",
      "\n",
      "\n",
      "\n",
      "Bi Encoder vs Cross Encoder\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000018'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 20\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "How to get a User Assistant from a Language Model ? \n",
      "C. Wolfe, 2023, Understanding and Using Supervised Fine-Tuning (SFT) for Language Models [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "‹#›\n",
      "II.B.1 Masking attention Mechanism\n",
      "This step allows numerical stability \n",
      "Definition: the masking mechanism allows later words to not influence earlier words by setting lower left values by -∞\n",
      "Idea: A later word cannot answer question to a previous word because it is unknown at inference\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query augmentation\n",
      "Luyu Gao & Al, 2022 Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Definition: query augmentation refers to the process of enhancing or expanding the user’s original query to improve the retrieval of relevant documents or information from a knowledge base. \n",
      "By augmenting the query, the system aims to retrieve more comprehensive and pertinent data, which can then be used to generate more accurate and informative responses.\n",
      "\n",
      "HyDE: Generate a fake answer from a query to improve information retrieval\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000019'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 21\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000020'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 22\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 1: Human Alignment - how to know the favorite answer for a human ? Costly to ask a human\n",
      "Solution: Use LLM to scale Data Collection at low cost\n",
      "\n",
      "\n",
      " \n",
      "Supervised Fine Tuning \n",
      "How can we get the post training data ? \n",
      "Alpaca: A Strong, Replicable Instruction-Following Model [Blog] \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "“decoder”\n",
      "“The”\n",
      "“models”\n",
      "Ct-1\n",
      "ht-1\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "Attention pattern\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Lost In the Middle\n",
      "Retrieved context provided at the beginning or the end of the prompt have more impact on the answer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain, Long Context Reorder [Blog]\n",
      "F. Liu & Al, 2023, Lost in the Middle: How Language Models Use Long Contexts\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000021'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 23\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Problem 2: 52K instruction is nothing compared to the amount of data needed to train a LM\n",
      "Solution: A few data is required for SFT \n",
      " \n",
      "Zhou & Al 2023, LIMA: Less Is More for Alignment\n",
      "Supervised Fine Tuning \n",
      "How much data do we need ?\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Cell state to propagate long memory\n",
      "Gate defined by the sigmoid function\n",
      "0 = don’t pass information \n",
      "1 = let everything pass through\n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Self Attention Mechanism\n",
      "Softmax\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "This step allows numerical stability \n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Open AI, Prompt engineering\n",
      "Prompt Engineering\n",
      "Write clear instructions\n",
      "Provide reference text\n",
      "Split complex tasks into simpler subtasks\n",
      "Give the model time to \"think\"\n",
      "Use external tools (RAG)\n",
      "\n",
      "Tactic:\n",
      "Ask the model to adopt a persona\n",
      "Use delimiters to clearly indicate distinct parts of the input\n",
      "Specify the steps required to complete a task\n",
      "Provide examples\n",
      "Specify the desired length of the output\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000022'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 24\n",
      "Content:\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Same process than the language model training\n",
      "\n",
      "How Supervised Fine Tuning Works ? \n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Model\n",
      "Expl ain the moon lan ding to a 6 years old\n",
      "9140 820 19  354 3672 34 347 321  2903 224 9832\n",
      "\n",
      "3892\n",
      "Child \n",
      "\n",
      "Some 👌 \n",
      "Loss\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "French to english translation example \n",
      "\n",
      "No masking\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "E4\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Transformers\n",
      "Les\n",
      "sont\n",
      "des\n",
      "…\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Document Loader\n",
      "Load any type of document (PDF, PPT(x), DOC(x), XLS(x)\n",
      "\n",
      "Unstructured: https://unstructured.io/ \n",
      "LLama Parse: https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000023'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 25\n",
      "Content:\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000024'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 26\n",
      "Content:\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "Idea:\n",
      "\n",
      "From a question, generate multiples answers\n",
      "Ask a human to classify answers\n",
      "Train a reward model to learn these preferences\n",
      "\n",
      "Reward model: classifier that is trained to classify preferences from possibles answers \n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for one attention head\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Chunking\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "To avoid context limitations, we can do document chunking:\n",
      "\n",
      "Chunk by document if the document is small \n",
      "Chunk by title or header if the document is big \n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000025'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 27\n",
      "Content:\n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain \n",
      "\n",
      "LlamaIndex \n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain \n",
      "\n",
      "LlamaIndex \n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain \n",
      "\n",
      "LlamaIndex \n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain \n",
      "\n",
      "LlamaIndex \n",
      "‹#›\n",
      "I.B.3 Reward Model\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Reward model\n",
      "Open AI, 2022, Aligning language models to follow instructions [Blog] \n",
      "\n",
      "Classifier \n",
      "Model\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Values: What do I reveal to others ?\n",
      "|E| : Embedding (1, 1512)\n",
      "|WV|: Value matrix (12 288, 12 288)\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "GPT 3 dimension for all attention heads: 603 979 776 parameters \n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "\n",
      "‹#›\n",
      "RAG Frameworks in Python\n",
      "Langchain \n",
      "\n",
      "LlamaIndex \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000026'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 28\n",
      "Content:\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "Also transformer based LM\n",
      "Variation in sizes used (relative to policy)\n",
      "Outputs scalar from input text\n",
      "\n",
      "\n",
      "Training RL model\n",
      "Lambert, 2022, Illustrating Reinforcement Learning from Human Feedback (RLHF) [Blog]\n",
      "\n",
      "Prevent over optimization \n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "What about Vanishing / Exploding Gradients ? \n",
      "\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "The additive update function for the cell state gives a derivative that is much more ‘well behaved’\n",
      "The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.\n",
      "To get details on LSTM derivative, check out this blog post \n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition≈\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Cloud services\n",
      "Cloud Services provide:\n",
      "A Secure environment \n",
      "Enough compute to train big models\n",
      "Product as a Service (PaaS)\n",
      "LLMs APIs\n",
      "Vector Store management\n",
      "Efficient Retrieval\n",
      "Monitoring tools\n",
      "…\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000027'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 29\n",
      "Content:\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "‹#›\n",
      "I.B.4 PPO & DPO\n",
      "\n",
      "PPO is much more complex (clipping, rollouts, outer loops) than in theory\n",
      "Maximize the desired output, minimize the other \n",
      "\n",
      "DPO\n",
      "Rafael Rafailov, 2024, Direct Preference Optimization:. Your Language Model is Secretly a Reward Model Paper\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Our goal today\n",
      "Predict the word “models” from the input sentence\n",
      "\n",
      "Requirements:\n",
      "Find a way to transform word into numerical values\n",
      "Provide semantic relationship between the encoding words\n",
      "Provide context to our model to understand the sentence\n",
      "Provide long context to our model to understand the sentence\n",
      "Create a fast trainable model \n",
      "\n",
      "\n",
      "\n",
      "Model\n",
      "Input: “Transformers are encoder decoder”\n",
      "Predict: “models”\n",
      "II.B.1 Multi \n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation: Precision & Recall @ k\n",
      "\n",
      "Precision @k\n",
      "How many  retrieved documents are relevant ?\n",
      "Recall @k\n",
      "How many  relevant documents are retrieved ?\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000028'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 30\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "\n",
      "NDCG can take values from 0 to 1. \n",
      "\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "\n",
      "NDCG can take values from 0 to 1. \n",
      "\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "\n",
      "NDCG can take values from 0 to 1. \n",
      "\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "\n",
      "NDCG can take values from 0 to 1. \n",
      "\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF gains\n",
      "Nisan Stiennon & Al, 2020, Learning to summarize from human feedback\n",
      "Dubois∗ & Al, 2024, Alpaca Farm: A Simulation Framework for Methods that Learn from Human Feedback\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix (12 288, 12 288) decomposition\n",
      "=\n",
      "\n",
      "12 288\n",
      "\n",
      "128\n",
      "\n",
      "\n",
      "128\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "Idea: \n",
      "The number of # is 150m for the Value matrix\n",
      "To avoid this high dimension and respects the \n",
      "Force the Value matrix to be low rank\n",
      "12 288\n",
      " .\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Retriever Evaluation : NDCG\n",
      "\n",
      "NDCG can take values from 0 to 1. \n",
      "\n",
      "NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n",
      "NDCG equals 0 when there are no relevant objects in top-K.\n",
      "NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 30, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000029'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 31\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "\n",
      "Ask an LLM to evaluate answer quality: \n",
      "\n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "\n",
      "\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "\n",
      "\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "\n",
      "Ask an LLM to evaluate answer quality: \n",
      "\n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "\n",
      "\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "\n",
      "\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "\n",
      "Ask an LLM to evaluate answer quality: \n",
      "\n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "\n",
      "\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "\n",
      "\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "\n",
      "Ask an LLM to evaluate answer quality: \n",
      "\n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "\n",
      "\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "\n",
      "\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "RLHF challenges\n",
      "Singhal & Al, 2024, A Long Way to Go: Investigating Length Correlations in RLHF\n",
      "Answer preference is not trivial\n",
      "RLHF increases answer size\n",
      "Humans do not agree (agree with themselves only 66% of the time)\n",
      "Human have lot of variance, model have no variance \n",
      "Ask LLM preferences instead of human preferences\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "\n",
      "II.B.1 Self Attention Mechanism\n",
      "Value Matrix computation optimization\n",
      " .\n",
      "12 288\n",
      "\n",
      "12 288\n",
      "\n",
      "E1\n",
      " =\n",
      "0.32  \t3.02 \t…\t-0.33\n",
      "\n",
      "12 288\n",
      " =    V1\n",
      "\n",
      "Wv\n",
      "‹#›\n",
      "III.5. Evaluation\n",
      "‹#›\n",
      "Answer Evaluation: LLM As a judge\n",
      "RAGAS\n",
      "\n",
      "Ask an LLM to evaluate answer quality: \n",
      "\n",
      "Does my answer answer to the question ? \n",
      "Does my answer used information from the context ? \n",
      "Does my answer give enough facts ?\n",
      "…\n",
      "\n",
      "\n",
      "BLEU, ROUGE, Perplexity are not ideal for RAG use case. \n",
      "\n",
      "\n",
      "Evaluating answers in RAG is not easy\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 31, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000030'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 32\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "Evaluation\n",
      "Chatbot Arena, Open LM\n",
      "How to evaluate a model like Chat GPT ? \n",
      "Different methods (DPO, PPO, SFT) can be compared\n",
      "Models are not calibrated\n",
      "A large diversity of evaluation to cover\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 12 288)\n",
      "\n",
      "12 288\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 12288\n",
      "\n",
      "12 288\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Number of computations:\n",
      "N words x 12 288 multiplications\n",
      "N words x 12 288 additions\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "\n",
      "Multimodal LLM: LLM capable of processing and understanding multiple types (or “modes”) of input data, such as text, images, audio, video, and other sensory inputs, in a unified manner.\n",
      "\n",
      "Exemple: GPT 4o, Gemini 1.5, Qwen2- VL\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 32, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000031'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 33\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1: “Streaming is dead, long live Chain of Thought”\n",
      "Open AI, 2024, Learning to Reason with LLMs [Blog] \n",
      "“Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them.”\n",
      "Chain of thought (COT)\n",
      "Increase test time compute\n",
      "II.B.1 Self Attention Mechanism\n",
      "Method 1 WV (12 288, 128)\n",
      "\n",
      "128\n",
      " =\n",
      "0.1 x v21     0.1 x v22\t        …\t0.1 x v2, 128\n",
      "\n",
      "128\n",
      "+\n",
      "0.32 x 0.62      3.02 x 0.62\t        …\t-0.33 x 0.62\n",
      " …\n",
      "Step 2:\n",
      "\n",
      "Number of computations:\n",
      "N words x 128 multiplications\n",
      "N words x 128 additions\n",
      "\n",
      "Matrix multiplication between value up matrix and result: \n",
      "128 multiplication + 128 addition for each row\n",
      "12 288 times\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal: Qwen2- VL\n",
      "Berrios & Al, 2023, Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\n",
      "Outperform GPT 4o on most of the benchmarks\n",
      "\n",
      "Multimodal Rotary Position (M-ROPE): “By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.”\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 33, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000032'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 34\n",
      "Content:\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "\n",
      "‹#›\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "\n",
      "‹#›\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "\n",
      "‹#›\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "\n",
      "‹#›\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.B.5 Evaluation & Challenges\n",
      "\n",
      "OpenAI o1 vs GPT 4o\n",
      "Prompt: \n",
      "From the following sentence: Transformers are encoder decoder models\n",
      "Apply the following steps: \n",
      "- Create a manim code to display this sentence where each character has a different color \n",
      "- Iterate through the sentence merging commons pairs as done n the Byte Pair Encoding system \n",
      "- Change the colors of new pair\n",
      "- Continue until all commons pair are made \n",
      "- Update at each step the manim code \n",
      "- Edit the previous code to not keep one color after merging on the merge pair. The selected color should be the one with the highest number of letters\n",
      "- Edit the code at the final stage to change color if two adjacent different pair have same color\n",
      "Open AI o1\n",
      "GPT 4o\n",
      "\n",
      "‹#›\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Rotary Embedding (ROPE)\n",
      "Su & AL, 2023, RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
      "Rotary Position Embedding, or RoPE, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. \n",
      "\n",
      "Unlike traditional position embeddings, which add fixed vectors to represent positions, RoPE encodes positional information directly into the attention mechanism by rotating the query and key vectors in the Transformer architecture. This approach allows the model to better handle long-range dependencies while maintaining the flexibility of the attention mechanism.\n",
      "\n",
      "Properties:\n",
      "Flexibility of being expand to any sequence lengths\n",
      "Decaying inter-token dependency with increasing relative distances\n",
      "Capability of equipping the linear self-attention with relative position encoding.\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 34, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000033'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 35\n",
      "Content:\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Conclusion\n",
      "Building GPT 3 \n",
      "\n",
      "Data preprocessing is a very important step to get quality data\n",
      "GPT 3 training consists of two phases (pre and post training)\n",
      "Supervised Fine Tuning is the first step of post training phase (ask a human to write the answer)\n",
      "RLHF helps the model to align with human preferences\n",
      "DPO is the new methods for Alignment, replacing RLHF\n",
      "\n",
      "General knowledges \n",
      "\n",
      "Data size and model size depends on compute resources (Scaling Laws, Chinchilla). \n",
      "OpenAI o1 improves efficiency with longer RLHF training and answer inference time (COT)\n",
      "\n",
      " \n",
      "\n",
      "“SFT+DPO approach seems to be the most popular preference tuning strategy at the moment due to the ease of use compared to other methods, such as RLHF with PPO.”\n",
      "Sebastian Raschka, 2024, New LLM Pre-training and Post-training Paradigms [Blog] \n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "LangChain, Multi-Vector Retriever for RAG on tables, text, and images\n",
      "Option 1:  Store raw image using multimodal embedding. Retrieve images based on multimodal embedding similarity. Provide the raw image to the generator.\n",
      "\n",
      "Option 2:  Store the summary of the image using a multimodal LLM. Retrieve images based on its summary embedding. Provide the summary to the generator.\n",
      "\n",
      "Option 3:  Store the image and its summary using a multimodal LLM. Retrieve images based on its summary embedding. Provide the image to the generator.\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880163.1827123, 'page_number': 35, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000034'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 36\n",
      "Content:\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "‹#›\n",
      "II.B.2 Multi-Head Attention\n",
      "Transformers\n",
      "are\n",
      "encoder\n",
      "decoder\n",
      "Attention mechanism\n",
      "\n",
      "\n",
      "\n",
      "III.6. Multimodal RAG\n",
      "‹#›\n",
      "Multimodal RAG\n",
      "Yasunaga & Al, 2023, Retrieval-Augmented Multimodal Language Modeling\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 36, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000035'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 37\n",
      "Content:\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG \n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG \n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG \n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG \n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "II.B.2 Multi-Head attention\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "ᐩ\n",
      "Z\n",
      "\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z4\n",
      "N words\n",
      "\n",
      " |V| (=128)\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Self-RAG \n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "Generates multiple possible response segments in parallel, utilizing the retrieved documents as context.\n",
      "The model ranks the generated segments based on their critique scores, selecting the most accurate and relevant segment as the final output.\n",
      "This selection process ensures that the response is both factually correct and contextually appropriate.\n",
      "Asai & Al, 2023, Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 37, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000036'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 38\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "II.B.2 Multi-Head attention\n",
      "Concatenation\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "z1\n",
      "z2\n",
      "z3\n",
      "z96\n",
      "…\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "RAPTOR\n",
      "Ahmed, 2024, SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI Generation [Blog]\n",
      "RAPTOR recursively summarizes retrieved documents. Instead of processing the full text of multiple documents directly, it creates concise summaries that retain the most important information at each recursive step. \n",
      "This hierarchy of summaries reduces the amount of information that needs to be processed while preserving the context and key facts from the original documents.\n",
      "Sarthi  & AL, 2024, RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 38, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000037'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 39\n",
      "Content:\n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "II.B.2 Multi-Head attention\n",
      "\n",
      "…\n",
      "z1\n",
      "z10\n",
      "z96\n",
      "\n",
      "N words\n",
      "\n",
      " |V| x N heads \n",
      "= 128 x 96\n",
      "x\n",
      "W0\n",
      "\n",
      " |V| x N heads \n",
      "\n",
      "| Embeddings Dim|\n",
      "\n",
      "\n",
      "N words\n",
      "| Embeddings Dim|\n",
      "Z\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "Corrective RAG (CRAG)\n",
      "Yan & Al, 2024, Corrective Retrieval Augmented Generation\n",
      "Add a retrieval evaluator based on the quality of retrieved sources\n",
      "If sources are considered as incorrect, or ambiguous, augment or replace the context by a Web Search query \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 39, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000038'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 40\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Input sentence: Transformers are encoder decoder\n",
      "\n",
      "|Key Dim| = |Query Dim| = |Value Dim| = 3\n",
      "|Embedding Dim| = 5\n",
      "|N words| = 4\n",
      "\t\t\t\t\n",
      "\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "Z0\n",
      "Q0\n",
      "V0\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "K0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "‹#›\n",
      "III.7. SOTA RAG architectures\n",
      "‹#›\n",
      "GraphRAG\n",
      "Microsoft, 2024, GraphRAG: Unlocking LLM discovery on narrative private data\n",
      "The LLM processes the entire private dataset, creating references to all entities and relationships within the source data, which are then used to create an LLM-generated knowledge graph. \n",
      "This graph is then used to create a bottom-up clustering that organizes the data hierarchically into semantic clusters. This partitioning allows for pre-summarization of semantic concepts and themes, which aids in holistic understanding of the dataset. \n",
      "At query time, both of these structures are used to provide materials for the LLM context window when answering a question. \n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 40, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000039'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 41\n",
      "Content:\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "…\n",
      "WK0\n",
      "WQ0\n",
      "WV0\n",
      "WK96\n",
      "WQ96\n",
      "WV96\n",
      "Q96\n",
      "V96\n",
      "K96\n",
      "Q0\n",
      "V0\n",
      "K0\n",
      "…\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "Step 4\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "Simple RAG: Encodes document content into a vector store, enabling quick retrieval of relevant information to enhance model responses.\n",
      "Context Enrichment: Adds surrounding context to each retrieved chunk, improving the coherence and completeness of the returned information.\n",
      "Multi-faceted Filtering: Applies various filtering techniques (metadata, similarity thresholds etc.) to refine and improve the quality of retrieved results.\n",
      "Fusion Retrieval: Combines vector-based similarity search with keyword-based retrieval to improve document retrieval.\n",
      "Intelligent Reranking: Reassesses and reorders initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing.\n",
      "Query Transformation: Modifies or expands the original query with query rewriting, step-back prompting and sub-query decomposition.\n",
      "Hierarchical Indices: First identifies relevant document sections through summaries, then drills down to specific details within those sections.\n",
      "Hypothetical Questions: HyDE transforms queries into hypothetical documents that contain answers, bridging the gap between query and document distributions in vector space.\n",
      "Choose Chunk Size: Selects an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.\n",
      "Semantic Chunking: Unlike traditional methods that split text by fixed character/word counts, semantic chunking creates more meaningful, context-aware segments.\n",
      "Context Compression: Compresses and extracts the most pertinent parts of documents in the context of a given query.\n",
      "Explainable Retrieval: Not only retrieves relevant documents based on a query but also provides explanations for why each retrieved document is relevant.\n",
      "Retrieval w/ Feedback: Utilizes user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 41, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000040'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 42\n",
      "Content:\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "…\n",
      "Z0\n",
      "Z96\n",
      "W0\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "4 x 96\n",
      "\n",
      "4 x 96\n",
      "\n",
      "5\n",
      "Matmul\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Step 5\n",
      "Z\n",
      "‹#›\n",
      "III. Conclusion\n",
      "‹#›\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Adaptive Retrieval: Classifies queries into different categories and uses tailored retrieval strategies (factual, analytical, contextual etc.) for each, considering query context and preferences.\n",
      "Iterative Retrieval: Analyzes initial results and generates follow-up queries to fill in gaps or clarify information.\n",
      "Ensemble Retrieval: Applies different embedding models or retrieval algorithms and uses voting or weighting mechanisms to determine the final set of retrieved documents.\n",
      "Graph RAG= Retrieves entities and their relationships from a knowledge graph relevant to the query, combining with unstructured text for more informative responses.\n",
      "Multimodal: Integrates models that can retrieve and understand different data modalities, combining insights from text, images, and more.\n",
      "RAPTOR: Uses abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.\n",
      "Self RAG: Multi-step process integrating decision, document retrieval, relevance filtering and generative feedback for more powerful model responses.\n",
      "Corrective RAG: Dynamically evaluates and corrects the retrieval process, combining vector databases, feedback, and models to improve responses.\n",
      "Few shot examples: Provides a few examples in the prompt to help the LLM understand the desired output\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 42, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000041'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 43\n",
      "Content:\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "II.B.2 Multi-Head attention - Summary exercice\n",
      "Z\n",
      "Why do Z and X have the same dimension ?\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 43, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000042'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 44\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 44, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000043'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 45\n",
      "Content:\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 45, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000044'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 46\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      "Residual connections\n",
      "Residual connections mainly help mitigate the vanishing gradient problem\n",
      "Another effect of residual connections is that the information stays local in the Transformer layer stack\n",
      "ReLU\n",
      "y = max(x, 0)\n",
      "He & Al, 2015,  Deep Residual Learning for Image Recognition\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 46, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000045'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 47\n",
      "Content:\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "II.B.3. Residual connections & Layer normalization\n",
      " Layer normalization\n",
      "“Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. \n",
      "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.”\n",
      "Hinton & Al, 2016, Layer Normalization\n",
      "2020, In-layer normalization techniques for training very deep neural networks [Blog]\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 47, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000046'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 48\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 48, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000047'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 49\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "Optimization \n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 49, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000048'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 50\n",
      "Content:\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.”\n",
      "‘Up \n",
      "projection’\n",
      "‘Down \n",
      "projection’\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 50, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000049'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 51\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 51, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000050'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 52\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "Z\n",
      "Transformers \n",
      "\n",
      "are  \n",
      "\n",
      "encoder \n",
      "\n",
      "decoder\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "X\n",
      "‹#›\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "\n",
      "Residual connection + Layer Nom\n",
      "Feed forward\n",
      "Layer\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 52, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000051'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 53\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Am I a math architecture ?\n",
      "\n",
      "Am I a football star ?\n",
      "\n",
      "Am I a plural noun ?\n",
      "\n",
      "Feed forward network\n",
      "Feed forward network\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 53, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000052'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 54\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer\n",
      "‹#›\n",
      "\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Transformers: “I am related to maths stuff, a plural noun, at the beginning of the sentence”\n",
      "\n",
      "Decoder: “I am before encoder, maybe related to maths, maybe an architecture\n",
      "…\n",
      "…\n",
      "Feed forward network\n",
      "\n",
      "Am I a TV stuff ?\n",
      "\n",
      "Am I a singular noun ?\n",
      "\n",
      "Am I a math architecture ?\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 54, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000053'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 55\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "II.B.4. Feed forward layer\n",
      "Feed forward layer : “Up projection”\n",
      "‹#›\n",
      "Feed forward network\n",
      "Up projection dimension = 49 152 x embedding dimension \n",
      "\n",
      "…\n",
      "\n",
      "49 152\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 55, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000054'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 56\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "II.B.4. Feed forward layer\n",
      "‹#›\n",
      "Another layer\n",
      "Down projection dimension = embedding dim x 49 152\n",
      "\n",
      "\n",
      "12 288\n",
      "Feed forward layer : “Down projection”\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 56, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000055'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 57\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension per layer\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 57, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000056'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 58\n",
      "Content:\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "II.B.4. Feed forward layer\n",
      "GPT 3 dimension (96 layers):\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 58, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000057'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 59\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 59, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000058'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 60\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 60, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000059'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 61\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .\n",
      "\n",
      "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. \n",
      "\n",
      "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel .”\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 61, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000060'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 62\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "The input is he vector from the last word of the sentence \n",
      "The output is the probability distribution over all words in the dictionary (50k words for GPT 3-\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 62, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000061'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 63\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "Q. Why don’t we take all the previous representations of the other vectors for the inference ?\n",
      "For training, each word/token is used for next word prediction. The model is trained to predict next word from only its previous word.\n",
      "Of course, the last word context is learned with attention\n",
      "decoder\n",
      "encoder\n",
      "are\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 63, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000062'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 64\n",
      "Content:\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "‹#›\n",
      "II.B.5 II.B.5. Softmax Layer - Temperature\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 64, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000063'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 65\n",
      "Content:\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "II.B.5. Softmax Layer\n",
      "GPT 3 dimension (96 layers):\n",
      "175 181 291 520 trainable parameters\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 65, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000064'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 66\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 66, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000065'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 67\n",
      "Content:\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.  Transformers Architecture\n",
      "\n",
      "Introduction\n",
      "Self Attention / Cross Attention\n",
      "Multi-Head Attention\n",
      "Residual connection & Layer normalization\n",
      "Feed forward layer\n",
      "Softmax Layer\n",
      "Positional Embeddings\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 67, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000066'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 68\n",
      "Content:\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
      "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 68, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000067'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 69\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Computations are not done sequentially (unlike RNN and LSTM)\n",
      "How to compare “A B C” and “C A B” ?\n",
      "\n",
      "Beneficial to find a method that satisfy the following points:\n",
      "\n",
      "Unambiguous (each position have its own value)\n",
      "Deterministic \n",
      "Allows to estimate distance between tokens\n",
      "Works with longer sequence than seen during training\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 69, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000068'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 70\n",
      "Content:\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Same size embedding to represent position (computationnaly for efficient that concatenation and model size increase)\n",
      "Don’t want to allow the model to extract information about positional informations only. Have to be coupled with word meaning\n",
      "\n",
      "Where:\n",
      "pos is the position {1, …, context length}\n",
      "i is the dimension = {1, …, dmodel }\n",
      "dmodel is the embedding dimension (GPT 3 = 12 288)\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 70, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000069'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 71\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 71, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000070'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 72\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Properties\n",
      "\n",
      "\n",
      "\n",
      "The positional values are unique if at least one function has maximum size of this sequence (context window) 👌\n",
      "The positional values are not random, created using two equations 👌\n",
      "Looking at frequencies, we can estimate distance between positions. For positions near to each other, we can use high frequency functions. For long distance position, we can use function with larger periods.  👌\n",
      "Since sin and cosine are periodic functions, the model can generalize for longer sequences 👌\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 72, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000071'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 73\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "“We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.”\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 73, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000072'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 74\n",
      "Content:\n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "II.B.6. Positional Embeddings\n",
      "‹#›\n",
      "Kazemnejads, 2019, Transformer Architecture: The Positional Encoding [Blog]\n",
      "Figure: Positional encoding representation\n",
      "Each row represent a positional vector for a given token \n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 74, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000073'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 75\n",
      "Content:\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "So far …\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 75, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000074'}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Page Number: 76\n",
      "Content:\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II. Resources\n",
      "Blogs:\n",
      "\n",
      "The Illustrated Transformer\n",
      "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive\n",
      "\n",
      "Papers:\n",
      "Attention is All You Need\n",
      "\n",
      "Videos:\n",
      "Visual introduction to Transformers (part 1)\n",
      "Transformers visualized (part 2)\n",
      "How might LLMs store fact (part 3)\n",
      "Residual Network and skip connections\n",
      "Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:\n",
      "{'source': './downloaded_files\\\\2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734880175.6979382, 'page_number': 76, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000075'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "from pptx import Presentation\n",
    "\n",
    "# Supposons que Document est une classe avec `metadata` et `page_content` comme attributs.\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "# Fonction pour lire le contenu d'un fichier PowerPoint\n",
    "def extract_pptx_content(file_path: str) -> list[Document]:\n",
    "    presentation = Presentation(file_path)\n",
    "    documents = []\n",
    "    \n",
    "    for i, slide in enumerate(presentation.slides):\n",
    "        # Extraire le texte de chaque slide\n",
    "        slide_content = \"\\n\".join([shape.text for shape in slide.shapes if hasattr(shape, \"text\")])\n",
    "        \n",
    "        # Créer un document pour chaque slide\n",
    "        metadata = {\n",
    "            'source': file_path,\n",
    "            'category_depth': 1,\n",
    "            'file_directory': os.path.dirname(file_path),\n",
    "            'filename': os.path.basename(file_path),\n",
    "            'last_modified': os.path.getmtime(file_path),  # Récupère la date de dernière modification\n",
    "            'page_number': i + 1,  # Numéro de page basé sur l'index de la slide\n",
    "            'languages': ['eng'],  # Langues par défaut\n",
    "            'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
    "            'category': 'Title',  # Catégorie par défaut\n",
    "            'element_id': f\"{i:08d}\",  # ID élément unique pour chaque slide\n",
    "        }\n",
    "        \n",
    "        documents.append(Document(slide_content, metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Fonction pour fusionner les documents par numéro de page\n",
    "def merge_documents_by_page(documents: list[Document]) -> list[Document]:\n",
    "    merged_documents: list[Document] = []\n",
    "    page_dict = {}\n",
    "\n",
    "    # Grouper les documents par numéro de page\n",
    "    for doc in documents:\n",
    "        page_number = doc.metadata.get('page_number')\n",
    "        if page_number is not None:\n",
    "            if page_number not in page_dict:\n",
    "                page_dict[page_number] = [doc]\n",
    "            else:\n",
    "                page_dict[page_number].append(doc)\n",
    "\n",
    "    # Fusionner les documents pour chaque page\n",
    "    for page_number, docs in page_dict.items():\n",
    "        if docs:\n",
    "            # Utiliser les métadonnées du premier document du groupe\n",
    "            merged_metadata = docs[0].metadata\n",
    "            # Concaténer le contenu des pages de tous les documents du groupe\n",
    "            merged_content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            # Répéter le contenu fusionné pour obtenir plusieurs répétitions\n",
    "            merged_content = \"\\n\".join([merged_content] * 5)  # Répéter le contenu 5 fois\n",
    "            # Créer un nouveau Document avec le contenu et les métadonnées fusionnés\n",
    "            merged_documents.append(Document(merged_content, merged_metadata))\n",
    "\n",
    "    return merged_documents\n",
    "\n",
    "# Charger les documents depuis le répertoire `downloaded_files`\n",
    "def load_documents_from_directory(directory: str) -> list[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pptx\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            documents.extend(extract_pptx_content(file_path))  # Ajouter les documents extraits\n",
    "    return documents\n",
    "\n",
    "# Spécifier le répertoire contenant vos fichiers\n",
    "directory_path = './downloaded_files'\n",
    "\n",
    "# Charger les documents depuis le répertoire\n",
    "documents = load_documents_from_directory(directory_path)\n",
    "\n",
    "# Fusionner les documents par page\n",
    "merged_documents = merge_documents_by_page(documents)\n",
    "\n",
    "# Afficher les documents fusionnés dans le format souhaité\n",
    "for doc in merged_documents:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Page Number: {doc.metadata.get('page_number')}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\")\n",
    "    print(f\"Metadata:\\n{doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Ingesting in Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ingest each merged_document in Cloud SQL.\n",
    "\n",
    "ALREADY DONE by teacher: \n",
    "- Create a Cloud SQL instance\n",
    "- Create a database in the instance\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Create a table in CloudSQL with you initials\n",
    "- Create the schema of the table\n",
    "- Ingest the data in the table\n",
    "\n",
    "\n",
    "Follow this [documentation](https://python.langchain.com/docs/integrations/vectorstores/google_cloud_sql_pg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Understand how to connect to Cloud SQL \n",
    "\n",
    "\n",
    "First we need to connect to Cloud SQL \n",
    "- Follow this [link](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy) to understand how it works\n",
    "\n",
    "Then be familiar ith the following PostgreSQL commands:\n",
    "```bash \n",
    "`psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=postgres\"` # to connect to the user `postgres`\n",
    "# the user we use is `students`\n",
    "# a password provided by the teacher is required\n",
    "`\\l` # to list all databases\n",
    "`\\c gen_ai_db` # to connect to the database `gen_ai_db`\n",
    "`\\dt` # to list all tables\n",
    "`\\d+ table_name` # to describe a table\n",
    "`SELECT * FROM table_name` # to select all rows from a table\n",
    "`\\du` # to list all users\n",
    "`\\q` # to quit\n",
    "`CREATE DATABASE db_name;` # to create a database\n",
    "`CREATE USER user_name WITH PASSWORD 'password';` # to create a user\n",
    "`GRANT ALL PRIVILEGES ON DATABASE db_name TO user_name;` # to grant all privileges to a user on a database\n",
    "`GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO user_name;` # to grant all privileges to a user on all tables in a schema\n",
    "`ALTER USER user_name WITH SUPERUSER;` # to grant superuser privileges to a user\n",
    "`DROP DATABASE db_name;` # to drop a database\n",
    "`DROP USER user_name;` # to drop a user\n",
    "`DROP TABLE table_name;` # to drop a table\n",
    "`REVOKE ALL PRIVILEGES ON DATABASE db_name FROM user_name;` # to revoke all privileges from a user on a database\n",
    "```\n",
    "\n",
    "When Cloud SQL Proxy is downloaded and the tutorial is followed. You should be connected to the instance. \n",
    "You can connect to the dabase as a user `students` with the password provided by the teacher.\n",
    "  - `psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=students\"`\n",
    "  - Enter the password provided by the teacher\n",
    "Try to create a table `initial_tests_table` with the following schema:\n",
    "  - `CREATE TABLE initial_tests_table (id SERIAL PRIMARY KEY, document TEXT, page_number INT, title TEXT, author TEXT, date TEXT);`\n",
    "  - `\\dt` to check if the table has been created\n",
    "  - `\\d+ initial_tests_table` to check the schema of the table\n",
    "  - `DROP TABLE initial_tests_table;` to drop the table\n",
    "  - `\\q` to quit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-cloud-sql-pg langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env.template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import PROJECT_ID, REGION, INSTANCE, DATABASE, DB_USER\n",
    "DB_PASSWORD = os.environ[\"DB_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"sk_table\" # Table name in the database initials-table. Ex: fb_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "engine = PostgresEngine.from_instance(\n",
    "    project_id=PROJECT_ID,\n",
    "    instance=INSTANCE,\n",
    "    region=REGION,\n",
    "    database=DATABASE,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already created\n"
     ]
    }
   ],
   "source": [
    "# Create a table in the PostgreSQL database with the required columns\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "try:\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=TABLE_NAME, # Vector size for VertexAI model(textembedding-gecko@latest)\n",
    "        vector_size=768,\n",
    "    )\n",
    "except ProgrammingError:\n",
    "    print(\"Table already created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute \\d+ [YOUR_INITIALS]_table in the psql shell to check the schema of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Create an embedding to convert your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\",  # Specify the embedding model name\n",
    "    project=\"dauphine-437611\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresVectorStore\n",
    "\n",
    "vector_store = PostgresVectorStore.create_sync(  # Use .create() to initialize an async vector store\n",
    "    engine=engine,\n",
    "    table_name=TABLE_NAME,\n",
    "    embedding_service=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_store.add_documents(merged_documents)\n",
    "# Excute only once this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Perform a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to train a Large Language Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to train a Large Language Model?\"\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # Default search type for vector similarity\n",
    "    search_kwargs={\"k\": 5}  # Retrieve the top 5 most similar documents\n",
    ")\n",
    "\n",
    "# Use .invoke() instead of .get_relevant_documents()\n",
    "docs = retriever.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Content:  Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Training process\n",
      "‹#›\n",
      "I.A.7 Training Process\n",
      "Steps \n",
      "\n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "\n",
      "\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Embedding\n",
      "Transformers\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Query\t\t\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "WQ\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Are we talking about TV ? \n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "Am I a superstar ?\n",
      "…\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734629889.6125753, 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000013'}\n",
      "--------------------------------------------------\n",
      "Content:  ‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "I.A Pretraining Large Language Model\n",
      "\n",
      "Pre training phase\n",
      "\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "‹#›\n",
      "II. Transformers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734629889.6125753, 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000001'}\n",
      "--------------------------------------------------\n",
      "Content:  ‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "‹#›\n",
      "I.B.2 RLHF\n",
      "\n",
      "SFT Limitations: \n",
      "\n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "“decoder”\n",
      "II.B.2 Cross attention\n",
      "\n",
      "\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734629889.6125753, 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000024'}\n",
      "--------------------------------------------------\n",
      "Content:  ‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "‹#›\n",
      "I.B.A Supervised Fine Tuning\n",
      "\n",
      "Idea: take a LLM pre-trained (as explained in I.Building Large Language Models) and fine tune to respect human preferences with moderation\n",
      "\n",
      " \n",
      "Famous LLM follow user instructions with moderation\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "\n",
      "Long Short Term Memory (LSTM)\n",
      "Both long and short term memory are provided\n",
      "\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "RNN architecture\n",
      "LSTM architecture\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "II.B.1 Self Attention Mechanism\n",
      "Embedding\n",
      "Transformers\n",
      "   are\n",
      "encoder\n",
      "decoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "VALUES\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "Wv\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "‹#›\n",
      "Query rephrasing\n",
      "Query rephrasing can be used to rephrase the query from the conversation history \n",
      "\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734629889.6125753, 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000020'}\n",
      "--------------------------------------------------\n",
      "Content:  Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jordan Hoffmann & Al, 2023, Chinchilla, Training Compute-Optimal Large Language Models\n",
      "‹#›\n",
      "I.A.8 Cost & Optimizations\n",
      "Display all the models with same amount of compute (left figure)\n",
      "Select the best model for each compute in terms of training loss (middle & right figure)\n",
      "Extrapolate to get the best model & data size for your compute (1.4T tokens and 63B param)\n",
      "Optimal model and data size\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "Briefly describe the architecture of a RNN [Blog] \n",
      "Each word is given sequentially (xt)\n",
      "An intern memory is updated after each word  (ht)\n",
      "A context is provided with this memory\n",
      "\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "WK\n",
      "Might be a TV object  of a model\n",
      "…\n",
      "I am a noun, starting the sentence\n",
      "I am a verb\n",
      "Key: What do I have ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WK|: Query matrix (12 288, 128)\n",
      "Embedding\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "E1\n",
      "E2\n",
      "E3\n",
      "E4\n",
      "K1\n",
      "K2\n",
      "K3\n",
      "K4\n",
      "KEYS\t\t\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "III.2. Information retrieval \n",
      "‹#›\n",
      "Best retrieval methods\n",
      "Leaderboard for best Information Retrieval methods: https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metadata:  {'source': './downloaded_files\\\\1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 1, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': 1734629889.6125753, 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '00000014'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content: \", doc.page_content)\n",
    "    print(\"Metadata: \", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You have successfully ingested the data in Cloud SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
