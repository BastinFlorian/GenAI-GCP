{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we’ll walk through the process of interacting with a Google Cloud Storage (GCS) bucket named dauphine-bucket, specifically focusing on the data directory within the bucket. We’ll cover how to:\n",
    "\n",
    "- List all files in the bucket’s data directory.\n",
    "- Retrieve information about a specific file.\n",
    "- Read files using the Unstructured library.\n",
    "- Visualize the extracted documents with LangChain.\n",
    "\n",
    "This guide is intended for users who are familiar with Python and basic cloud storage concepts.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following:\n",
    "\n",
    "- Python 3.x installed on your system.\n",
    "- Access to the GCP bucket dauphine-bucket/data with the necessary permissions.\n",
    "- Google Cloud SDK installed and authenticated. You can authenticate by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=8HUXNz6oRlUK4rKBZJ09OHOhvoHMX1&access_type=offline&code_challenge=dUs-iUh03KRRVSbyghhzU7hwlCxYcYAN-3dvVOrKxyc&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [linathabet101@gmail.com].\n",
      "Your current project is [dauphine-437611].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python libraries installed:\n",
    "- google-cloud-storage\n",
    "- unstructured\n",
    "- langchain\n",
    "\n",
    "To know more about the libraries, you can visit the following links:\n",
    "- [google-cloud-storage](https://googleapis.dev/python/storage/latest/index.html)\n",
    "- [unstructured](https://docs.unstructured.io/examplecode/codesamples/oss/vector-database)\n",
    "- [langchain](https://langchain.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q google-cloud-storage unstructured langchain python-magic sqlalchemy langchain_google_cloud_sql_pg\n",
    "%pip install -q \"unstructured[pptx]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Listing and loading files from a GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Listing Files in the GCP Bucket\n",
    "\n",
    "Explanation:\n",
    "\n",
    "To interact with a GCS bucket, we’ll use the google-cloud-storage library. We’ll initialize a client, access the bucket, and list all the files within the data directory.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'dauphine-bucket/data':\n",
      "data/\n",
      "data/1 - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx\n",
      "data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Access the bucket\n",
    "bucket_name = 'dauphine-bucket'\n",
    "bucket = client.get_bucket(bucket_name)  # Access the bucket\n",
    "\n",
    "# List all files in the 'data' directory\n",
    "blobs = bucket.list_blobs(prefix='data/')  # List blobs with 'data/' prefix\n",
    "\n",
    "print(\"Files in 'dauphine-bucket/data':\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "Running this code will display all the file paths within the data directory of the bucket. The prefix='data/' parameter ensures we only get files from that specific directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Getting Information About One File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Sometimes, you may need detailed information about a specific file, such as its size, content type, or the last time it was updated. We’ll retrieve this metadata for a chosen file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for 'data/1 - Gen AI - Dauphine Tunis.pptx':\n",
      "Size: 6724048 bytes\n",
      "Content Type: application/vnd.openxmlformats-officedocument.presentationml.presentation\n",
      "Updated On: 2024-10-07 09:52:30.256000+00:00\n",
      "Blob Name: data/1 - Gen AI - Dauphine Tunis.pptx\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path (replace with an actual file from your bucket)\n",
    "file_path = 'data/1 - Gen AI - Dauphine Tunis.pptx'\n",
    "\n",
    "# Get the blob object\n",
    "blob = bucket.get_blob(file_path)\n",
    "\n",
    "if blob:\n",
    "    print(f\"Information for '{file_path}':\")\n",
    "    print(f\"Size: {blob.size} bytes\")\n",
    "    print(f\"Content Type: {blob.content_type}\")\n",
    "    print(f\"Updated On: {blob.updated}\")  # Retrieve the last updated timestamp\n",
    "    print(f\"Blob Name: {blob.name}\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found in the bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Explanation:\n",
    "\n",
    "This code will output metadata about the specified file. Make sure to replace 'data/your_file.ext' with the actual file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Reading Files with Unstructured\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Unstructured library allows us to parse and process unstructured data from various file formats. We’ll download a file from the bucket and use Unstructured to read and extract its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "\n",
    "DOWNLOADED_LOCAL_FIRECTORY = \"./downloaded_files\"\n",
    "os.makedirs(DOWNLOADED_LOCAL_FIRECTORY, exist_ok=True)\n",
    "\n",
    "\n",
    "# Function to download the file: file_path from the GCS Bucket\n",
    "def download_file_from_bucket(bucket: Bucket, file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a GCS Bucket to the local filesystem.\n",
    "\n",
    "    Args:\n",
    "        bucket (Bucket): The GCS bucket object.\n",
    "        file_path (str): The path of the file within the bucket.\n",
    "\n",
    "    Returns:\n",
    "        str: The local file path.\n",
    "    \"\"\"\n",
    "    # Download the file locally\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    # Extract the local file name\n",
    "    local_file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Define the local file path\n",
    "    local_filepath = os.path.join(DOWNLOADED_LOCAL_FIRECTORY, local_file_name)\n",
    "\n",
    "    # Perform the download\n",
    "    blob.download_to_filename(local_filepath)\n",
    "    print(f\"Downloaded '{file_path}' to '{local_file_name}'\")\n",
    "    return local_filepath\n",
    "\n",
    "\n",
    "def read_file_from_local(local_filepath: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Reads a file from the local filesystem using UnstructuredLoader\n",
    "    and returns a list of Document objects.\n",
    "\n",
    "    Args:\n",
    "        local_filepath (str): The path to the local file to be read.\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: A list of Document objects loaded from the specified file.\n",
    "    \"\"\"\n",
    "    # Initialize the loader with the file path\n",
    "    loader = UnstructuredFileLoader(local_filepath)\n",
    "\n",
    "    # Load the documents\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing 'data/': [Errno 2] No such file or directory: './downloaded_files\\\\'\n",
      "Downloaded 'data/1 - Gen AI - Dauphine Tunis.pptx' to '1 - Gen AI - Dauphine Tunis.pptx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lina\\AppData\\Local\\Temp\\ipykernel_1976\\1296602457.py:50: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  loader = UnstructuredFileLoader(local_filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing 'data/1 - Gen AI - Dauphine Tunis.pptx': failed to find libmagic.  Check your installation\n",
      "Downloaded 'data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx' to '2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "An error occurred while processing 'data/2.1 - Before Transformers - Gen AI - Dauphine Tunis.pptx': failed to find libmagic.  Check your installation\n",
      "Downloaded 'data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx' to '2.2  - Transformers - Gen AI - Dauphine Tunis.pptx'\n",
      "An error occurred while processing 'data/2.2  - Transformers - Gen AI - Dauphine Tunis.pptx': failed to find libmagic.  Check your installation\n",
      "Downloaded 'data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx' to '3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx'\n",
      "An error occurred while processing 'data/3 - Retrieval Augmented Generation - Gen AI - Dauphine Tunis.pptx': failed to find libmagic.  Check your installation\n"
     ]
    }
   ],
   "source": [
    "# Load all the\n",
    "blobs = list(bucket.list_blobs(prefix='data/'))\n",
    "documents: list[Document] = []\n",
    "if blobs:\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            local_filepath = download_file_from_bucket(bucket, blob.name)\n",
    "            documents.extend(read_file_from_local(local_filepath))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{blob.name}': {e}\")\n",
    "else:\n",
    "    print(\"No files found in the 'data' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "failed to find libmagic.  Check your installation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m local_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloaded_files/1 - Gen AI - Dauphine Tunis.pptx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m UnstructuredLoader(local_filepath)\n\u001b[1;32m----> 6\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doc\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:178\u001b[0m, in \u001b[0;36mUnstructuredLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Call _UnstructuredBaseLoader normally since file and file_path are not lists\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m load_file(f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile, f_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:212\u001b[0m, in \u001b[0;36m_SingleDocumentLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     elements_json \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elements_json)\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processors\n\u001b[1;32m--> 212\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_elements_json\u001b[49m\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements_json:\n\u001b[0;32m    215\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata()\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:231\u001b[0m, in \u001b[0;36m_SingleDocumentLoader._elements_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_via_api:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elements_via_api\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_elements_to_dicts(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_elements_via_local\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:249\u001b[0m, in \u001b[0;36m_SingleDocumentLoader._elements_via_local\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata_filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf partitioning a fileIO object, metadata_filename must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\partition\\auto.py:181\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, hi_res_model_name, model_name, starting_page_number, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m headers \u001b[38;5;241m!=\u001b[39m {}:\n\u001b[0;32m    177\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe headers kwarg is set but the url kwarg is not. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe headers kwarg will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    180\u001b[0m         )\n\u001b[1;32m--> 181\u001b[0m     file_type \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_filetype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     file\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:98\u001b[0m, in \u001b[0;36mdetect_filetype\u001b[1;34m(file_path, file, encoding, content_type, metadata_file_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine file-type of specified file using libmagic and/or fallback methods.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mOne of `file_path` or `file` must be specified. A `file_path` that does not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    - Neither `file_path` nor `file` were specified.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m ctx \u001b[38;5;241m=\u001b[39m _FileTypeDetectionContext\u001b[38;5;241m.\u001b[39mnew(\n\u001b[0;32m     92\u001b[0m     file_path\u001b[38;5;241m=\u001b[39mfile_path,\n\u001b[0;32m     93\u001b[0m     file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     metadata_file_path\u001b[38;5;241m=\u001b[39mmetadata_file_path,\n\u001b[0;32m     97\u001b[0m )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FileTypeDetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:131\u001b[0m, in \u001b[0;36m_FileTypeDetector.file_type\u001b[1;34m(cls, ctx)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfile_type\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ctx: _FileTypeDetectionContext) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FileType:\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Detect file-type of document-source described by `ctx`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_type\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:141\u001b[0m, in \u001b[0;36m_FileTypeDetector._file_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_type\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# -- strategy 2: guess MIME-type using libmagic and use that --\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_type \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_type_from_guessed_mime_type\u001b[49m:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_type\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# -- strategy 3: use filename-extension, like \".docx\" -> FileType.DOCX --\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:181\u001b[0m, in \u001b[0;36m_FileTypeDetector._file_type_from_guessed_mime_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_file_type_from_guessed_mime_type\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FileType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"FileType based on auto-detection of MIME-type by libmagic.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    In some cases refinements are necessary on the magic-derived MIME-types. This process\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    includes applying those rules, most of which are accumulated through practical experience.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     mime_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmime_type\u001b[49m\n\u001b[0;32m    182\u001b[0m     extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mextension\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# -- when libmagic is not installed, the `filetype` package is used instead.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# -- `filetype.guess()` returns `None` for file-types it does not support, which\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# -- unfortunately includes all the textual file-types like CSV, EML, HTML, MD, RST, RTF,\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# -- TSV, and TXT. When we have no guessed MIME-type, this strategy is not applicable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\utils.py:154\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    149\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# --- on first access, the __dict__ item will be absent. Evaluate fget()\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# --- and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# --- __dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, value)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:362\u001b[0m, in \u001b[0;36m_FileTypeDetectionContext.mime_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LIBMAGIC_AVAILABLE:\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmagic\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     mime_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    365\u001b[0m         magic\u001b[38;5;241m.\u001b[39mfrom_file(file_path, mime\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_path\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mfrom_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_head, mime\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mime_type\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m mime_type \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\magic\\__init__.py:209\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\u001b[38;5;241m.\u001b[39mfrom_descriptor(fd)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[1;32m--> 209\u001b[0m libmagic \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m magic_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorcheck_null\u001b[39m(result, func, args):\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\magic\\loader.py:49\u001b[0m, in \u001b[0;36mload_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;66;03m# It is better to raise an ImportError since we are importing magic module\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to find libmagic.  Check your installation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: failed to find libmagic.  Check your installation"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "local_filepath = \"downloaded_files/1 - Gen AI - Dauphine Tunis.pptx\"\n",
    "\n",
    "loader = UnstructuredLoader(local_filepath)\n",
    "documents = loader.load()\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the documents\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(\"Content:\", doc.page_content[:300])  # Display the first 300 characters\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Visualizing the First Documents Extracted with LangChain\n",
    "\n",
    "Explanation:\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. We’ll use it to load and visualize the documents extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents[:3]:\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Join extracted document by page\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- The text extraction block is uninformative because very small text blocks are extracted from the document.\n",
    "- We can join the extracted text by page to get a more meaningful output.\n",
    "- A metadata with the 'page_number' can be helpful\n",
    "- The other metadatas need to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Function to merge documents by page number\n",
    "def merge_documents_by_page(documents: list[Document]) -> list[Document]:\n",
    "    merged_documents: list[Document] = []\n",
    "    page_dict = defaultdict(list)\n",
    "\n",
    "    # Group documents by page number\n",
    "    for doc in documents:\n",
    "        page_number = doc.metadata.get('page_number')\n",
    "        if page_number is not None:\n",
    "            page_dict[page_number].append(doc)\n",
    "\n",
    "    # Merge documents for each page\n",
    "    for page_number, docs in page_dict.items():\n",
    "        if docs:\n",
    "            # Use the metadata of the first document in the group\n",
    "            merged_metadata = docs[0].metadata\n",
    "\n",
    "            # Concatenate the content of all documents in the group\n",
    "            merged_content = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "            # Create a new Document with merged content and metadata\n",
    "            merged_documents.append(Document(page_content=merged_content, metadata=merged_metadata))\n",
    "\n",
    "    return merged_documents\n",
    "\n",
    "# Merge the documents by page\n",
    "merged_documents = merge_documents_by_page(documents)\n",
    "\n",
    "# Print the merged documents\n",
    "for doc in merged_documents:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Page Number: {doc.metadata.get('page_number')}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\\nMetadata:\\n{doc.metadata}\\n\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Ingesting in Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ingest each merged_document in Cloud SQL.\n",
    "\n",
    "ALREADY DONE by teacher: \n",
    "- Create a Cloud SQL instance\n",
    "- Create a database in the instance\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Create a table in CloudSQL with you initials\n",
    "- Create the schema of the table\n",
    "- Ingest the data in the table\n",
    "\n",
    "\n",
    "Follow this [documentation](https://python.langchain.com/docs/integrations/vectorstores/google_cloud_sql_pg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Understand how to connect to Cloud SQL \n",
    "\n",
    "\n",
    "First we need to connect to Cloud SQL \n",
    "- Follow this [link](https://cloud.google.com/sql/docs/postgres/connect-instance-auth-proxy) to understand how it works\n",
    "\n",
    "Then be familiar ith the following PostgreSQL commands:\n",
    "```bash \n",
    "`psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=postgres\"` # to connect to the user `postgres`\n",
    "# the user we use is `students`\n",
    "# a password provided by the teacher is required\n",
    "`\\l` # to list all databases\n",
    "`\\c gen_ai_db` # to connect to the database `gen_ai_db`\n",
    "`\\dt` # to list all tables\n",
    "`\\d+ table_name` # to describe a table\n",
    "`SELECT * FROM table_name` # to select all rows from a table\n",
    "`\\du` # to list all users\n",
    "`\\q` # to quit\n",
    "`CREATE DATABASE db_name;` # to create a database\n",
    "`CREATE USER user_name WITH PASSWORD 'password';` # to create a user\n",
    "`GRANT ALL PRIVILEGES ON DATABASE db_name TO user_name;` # to grant all privileges to a user on a database\n",
    "`GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO user_name;` # to grant all privileges to a user on all tables in a schema\n",
    "`ALTER USER user_name WITH SUPERUSER;` # to grant superuser privileges to a user\n",
    "`DROP DATABASE db_name;` # to drop a database\n",
    "`DROP USER user_name;` # to drop a user\n",
    "`DROP TABLE table_name;` # to drop a table\n",
    "`REVOKE ALL PRIVILEGES ON DATABASE db_name FROM user_name;` # to revoke all privileges from a user on a database\n",
    "```\n",
    "\n",
    "When Cloud SQL Proxy is downloaded and the tutorial is followed. You should be connected to the instance. \n",
    "You can connect to the dabase as a user `students` with the password provided by the teacher.\n",
    "  - `psql \"host=127.0.0.1 port=5432 sslmode=disable dbname=gen_ai_db user=students\"`\n",
    "  - Enter the password provided by the teacher\n",
    "Try to create a table `initial_tests_table` with the following schema:\n",
    "  - `CREATE TABLE initial_tests_table (id SERIAL PRIMARY KEY, document TEXT, page_number INT, title TEXT, author TEXT, date TEXT);`\n",
    "  - `\\dt` to check if the table has been created\n",
    "  - `\\d+ initial_tests_table` to check the schema of the table\n",
    "  - `DROP TABLE initial_tests_table;` to drop the table\n",
    "  - `\\q` to quit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-cloud-sql-pg langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m----> 2\u001b[0m \u001b[43mload_dotenv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:356\u001b[0m, in \u001b[0;36mload_dotenv\u001b[1;34m(dotenv_path, stream, verbose, override, interpolate, encoding)\u001b[0m\n\u001b[0;32m    346\u001b[0m     dotenv_path \u001b[38;5;241m=\u001b[39m find_dotenv()\n\u001b[0;32m    348\u001b[0m dotenv \u001b[38;5;241m=\u001b[39m DotEnv(\n\u001b[0;32m    349\u001b[0m     dotenv_path\u001b[38;5;241m=\u001b[39mdotenv_path,\n\u001b[0;32m    350\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    355\u001b[0m )\n\u001b[1;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdotenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_as_environment_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:92\u001b[0m, in \u001b[0;36mDotEnv.set_as_environment_variables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_as_environment_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    Load the current dotenv as system environment variable.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:76\u001b[0m, in \u001b[0;36mDotEnv.dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m raw_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(\u001b[43mresolve_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m OrderedDict(raw_values)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:238\u001b[0m, in \u001b[0;36mresolve_variables\u001b[1;34m(values, override)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_variables\u001b[39m(\n\u001b[0;32m    233\u001b[0m     values: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]],\n\u001b[0;32m    234\u001b[0m     override: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    236\u001b[0m     new_values: Dict[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:84\u001b[0m, in \u001b[0;36mDotEnv.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 84\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwith_warn_for_invalid_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\main.py:26\u001b[0m, in \u001b[0;36mwith_warn_for_invalid_lines\u001b[1;34m(mappings)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_warn_for_invalid_lines\u001b[39m(mappings: Iterator[Binding]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m---> 26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmappings\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPython-dotenv could not parse statement starting at line \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\parser.py:173\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_stream\u001b[39m(stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Binding]:\n\u001b[1;32m--> 173\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m parse_binding(reader)\n",
      "File \u001b[1;32mc:\\Users\\Lina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dotenv\\parser.py:64\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: IO[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark \u001b[38;5;241m=\u001b[39m Position\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DB_PASSWORD'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from config import PROJECT_ID, REGION, INSTANCE, DATABASE, DB_USER\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m DB_PASSWORD \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDB_PASSWORD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(DB_PASSWORD)\n",
      "File \u001b[1;32m<frozen os>:714\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DB_PASSWORD'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from config import PROJECT_ID, REGION, INSTANCE, DATABASE, DB_USER\n",
    "DB_PASSWORD = os.environ[\"DB_PASSWORD\"]\n",
    "print(DB_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"fb_table\" # Table name in the database initials-table. Ex: fb_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DB_PASSWORD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_cloud_sql_pg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PostgresEngine\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Connect to the PostgreSQL database\u001b[39;00m\n\u001b[0;32m      4\u001b[0m engine \u001b[38;5;241m=\u001b[39m PostgresEngine\u001b[38;5;241m.\u001b[39mfrom_instance(\n\u001b[0;32m      5\u001b[0m     project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[0;32m      6\u001b[0m     instance\u001b[38;5;241m=\u001b[39mINSTANCE,\n\u001b[0;32m      7\u001b[0m     region\u001b[38;5;241m=\u001b[39mREGION,\n\u001b[0;32m      8\u001b[0m     database\u001b[38;5;241m=\u001b[39mDATABASE,\n\u001b[0;32m      9\u001b[0m     user\u001b[38;5;241m=\u001b[39mDB_USER,\n\u001b[1;32m---> 10\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[43mDB_PASSWORD\u001b[49m,\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DB_PASSWORD' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "engine = PostgresEngine.from_instance(\n",
    "    project_id=PROJECT_ID,\n",
    "    instance=INSTANCE,\n",
    "    region=REGION,\n",
    "    database=DATABASE,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already created\n"
     ]
    }
   ],
   "source": [
    "# Create a table in the PostgreSQL database with the required columns\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "try:\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=TABLE_NAME, # Vector size for VertexAI model(textembedding-gecko@latest)\n",
    "        vector_size=768,\n",
    "    )\n",
    "except ProgrammingError:\n",
    "    print(\"Table already created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute \\d+ [YOUR_INITIALS]_table in the psql shell to check the schema of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Create an embedding to convert your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=#TODO,\n",
    "    project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresVectorStore\n",
    "\n",
    "vector_store = PostgresVectorStore.create_sync(  # Use .create() to initialize an async vector store\n",
    "    engine=engine,\n",
    "    table_name=TABLE_NAME,\n",
    "    embedding_service=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(merged_documents)\n",
    "# Excute only once this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Perform a similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to train a Large Language Model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=#TODO\n",
    "    search_kwargs=#TODO\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Content:  I.A.7 Training Process\n",
      "Training process\n",
      "Steps \n",
      "Find scaling recipes (example: learning rate decrease if the size of the model increase)\n",
      "Tune hyper parameters on small models of differents size\n",
      "Choose the best models among the smallest ones\n",
      "Train the biggest model with the \n",
      "Q. Should I use Transformers or LSTM ? \n",
      "‹#›\n",
      "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs) [Youtube]\n",
      "II.A.3 RNN\n",
      "Recurrent Neural Networks (Seq2seq model)\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "Query: What am I looking for ? \n",
      "|E| : Embedding (1, 12 288)\n",
      "|WQ|: Query matrix (12 288, 128)\n",
      "WQ\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "..\n",
      "..\n",
      "3.32\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "1.12\n",
      "3.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "4.98\n",
      "Embedding\n",
      "3.23    -1.23    0.89    0.32\n",
      " -3.29     3.23    1.23   -2.34\n",
      "  1.83     1.92    0.1     1.28\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "..\n",
      "..\n",
      "5.93\n",
      "Query\t\t\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "Q4\n",
      "Am I a superstar ?\n",
      "Do I mean Allocation de Retour à l’Emploi ?\n",
      "…\n",
      "Are we talking about TV ? \n",
      "‹#›\n",
      "III.2. Information retrieval \n",
      "Deep Bidirectional Language-Knowledge Graph Pretraining (DRAGON)\n",
      "Dense retriever\n",
      "Progressive Data Augmentation strategy for training sampling very difficult negatives\n",
      "‹#›\n",
      "Lin & Al, 2023, How to Train Your DRAGON\n",
      "Yasunaga, 2023, DRAGON: Training a Foundation Model from Text and Knowledge Graph [Blog]\n",
      "Metadata:  {'source': './downloaded_files/1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-10-07T17:06:06', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '4edca6e6a813ce76686e4f90e2143a4e'}\n",
      "--------------------------------------------------\n",
      "Content:  I.A Pretraining Large Language Model\n",
      "A. Pretraining a Large Language Model\n",
      "Introduction\n",
      "Cross entropy loss\n",
      "Tokenization\n",
      "Evaluation\n",
      "Data preprocessing\n",
      "Scaling laws\n",
      "Training process\n",
      "Cost and optimization\n",
      "Pre training phase\n",
      "‹#›\n",
      "II. Transformers\n",
      "‹#›\n",
      "II.B. Transformers Architecture\n",
      "‹#›\n",
      "III. Retrieval Augmented Generation\n",
      "Basic Architecture\n",
      "Information retrieval \n",
      "Vectorstore & Search optimization\n",
      "RAG Techniques\n",
      "Evaluation\n",
      "Multimodal RAG\n",
      "SOTA RAG architectures\n",
      "‹#›\n",
      "Metadata:  {'source': './downloaded_files/1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-10-07T17:06:06', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': 'c580c8bf0896b0e7bd6f302dd576638b'}\n",
      "--------------------------------------------------\n",
      "Content:  I.B Fine tuning Large Language Model\n",
      "B. Fine tuning a Large Language Model\n",
      "Supervised Fine Tuning\n",
      "RLHF\n",
      "Reward model\n",
      "PPO & DPO\n",
      "Evaluation & Challenges\n",
      "Post training phase\n",
      "‹#›\n",
      "II.A.3 RNN\n",
      "RNN limitations: Exploding / Vanishing gradient problem\n",
      "“The”\n",
      "Feed forward + Softmax model\n",
      "“transformers”\n",
      "“are”\n",
      "“encoder”\n",
      "?\n",
      "“decoder”\n",
      "“decoder”\n",
      "‹#›\n",
      "II.B.1 Self Attention Mechanism\n",
      "   are\n",
      "Transformers\n",
      "decoder\n",
      "encoder\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "Embedding\n",
      "E4\n",
      "E2\n",
      "E3\n",
      "E1\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      " 5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "Query\t\t\n",
      "Q4\n",
      "Q1\n",
      "Q2\n",
      "Q3\n",
      "WQ\n",
      "-3.11\n",
      "2.422\n",
      " 7.93\n",
      "2.11\n",
      "-4.22\n",
      "5.93\n",
      "3.23    -1.23    0.89    0.32\n",
      " -3.29     3.23    1.23   -2.34\n",
      "  1.83     1.92    0.1     1.28\n",
      "4    70    0    85\n",
      " -4   -10    0     0\n",
      "  2     0    0     0\n",
      "  3    -3    4     5\n",
      "K1\n",
      "E1\n",
      "2.43\n",
      "-3.2\n",
      "3.32\n",
      "2.11\n",
      "-3.22\n",
      "5.93\n",
      "WK\n",
      "E2\n",
      "K2\n",
      "1.23      -1.23    0.89    1.12\n",
      "2..29      3.23   -3.23   -3.34\n",
      "2.83       0.92    1.1     4.28\n",
      "2.11\n",
      "-1.2\n",
      "5.93\n",
      "2.11\n",
      "-4.22\n",
      "1.12\n",
      "K3\n",
      "E3\n",
      "-21\n",
      "42.21.2\n",
      "1.23\n",
      "3.11\n",
      "-4.22\n",
      "4.98\n",
      "‹#›\n",
      "K4\n",
      "E4\n",
      "III.3. Vectorstore & Search optimization\n",
      "Efficient similarity search\n",
      "Scalable Nearest Neighbors (ScaNN) - Google\n",
      "Facebook AI Similarity Search (FAISS)\n",
      "Hierarchical Navigable Small Worlds (HNSW)\n",
      "Definition:\n",
      "ScaNN, FAISS and HNSW are methods for retrieving similar embeddings based on vector quantization and ANN search instead of full scan search.\n",
      "Announcing ScaNN: Efficient Vector Similarity Search\n",
      "‹#›\n",
      "Hierarchical Navigable Small Worlds (HNSW) [Blog]\n",
      "Metadata:  {'source': './downloaded_files/1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-10-07T17:06:06', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '8c07ba4c6ce989e2e94fd2b175952e1a'}\n",
      "--------------------------------------------------\n",
      "Content:  I.B.2 RLHF\n",
      "Reinforcement Learning from Human Feedback - RLHF\n",
      "SFT Limitations: \n",
      "Behavior cloning\n",
      "Human abilities to answer perfectly to a given question\n",
      "Hallucination if answer from human not in training data\n",
      "Data collection cost\n",
      "‹#›\n",
      "II.A.4 LSTM\n",
      "Long Short Term Memory (LSTM)\n",
      "“decoder”\n",
      "Colah, Understanding LSTM Networks [Blog] \n",
      "‹#›\n",
      "II.B.2 Cross attention\n",
      "‹#›\n",
      "III.4. RAG Techniques\n",
      "Context \n",
      "Definition: The context size refers to the maximum number of tokens (words or subword units) that the model can process in a single input sequence. It determines how much textual information the model can consider at once when generating responses or predictions.\n",
      "A larger context size allows the model to capture longer dependencies and understand more extensive context within the input, leading to more coherent and relevant outputs.\n",
      "A smaller context size limits the amount of information the model can utilize from the input text.\n",
      "‹#›\n",
      "Variable Sequence Length Training for Long-Context Large Language Models\n",
      "Metadata:  {'source': './downloaded_files/1 - Gen AI - Dauphine Tunis.pptx', 'category_depth': 0, 'file_directory': './downloaded_files', 'filename': '1 - Gen AI - Dauphine Tunis.pptx', 'last_modified': '2024-10-07T17:06:06', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title', 'element_id': '3f3edd5deab91e90566409af84a75f8e'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content: \", doc.page_content)\n",
    "    print(\"Metadata: \", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You have successfully ingested the data in Cloud SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
